{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SecondModelPolishDataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i61b34AJ1zPY",
        "colab_type": "text"
      },
      "source": [
        "# Second model - Polish Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4R8arNJb14eM",
        "colab_type": "text"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLg6EMKN2D0x",
        "colab_type": "code",
        "outputId": "4b2612a4-9781-4d56-a963-c9c3cae3d37d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        }
      },
      "source": [
        "!pip install transformers\n",
        "!pip install -q pyyaml h5py\n",
        "!pip install siuba\n",
        "import pandas as pd\n",
        "from transformers import *\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from siuba.dply.forcats import fct_lump, fct_reorder\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/38/c9527aa055241c66c4d785381eaf6f80a28c224cae97daa1f8b183b5fabb/transformers-2.9.0-py3-none-any.whl (635kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 4.9MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 21.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 31.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers==0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 48.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=9da3d1632cd5e39b730b9f591f411407eed30550603831e4a53bdd550829119c\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.86 tokenizers-0.7.0 transformers-2.9.0\n",
            "Collecting siuba\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/65/73d5c3c8f8814ec56926d529b576a64413513873d373206734b9b3fd1bc4/siuba-0.0.19-py3-none-any.whl (99kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 3.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from siuba) (1.18.3)\n",
            "Requirement already satisfied: PyYAML>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from siuba) (3.13)\n",
            "Requirement already satisfied: SQLAlchemy>=1.1.18 in /usr/local/lib/python3.6/dist-packages (from siuba) (1.3.16)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.6/dist-packages (from siuba) (1.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->siuba) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.0->siuba) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.24.0->siuba) (1.12.0)\n",
            "Installing collected packages: siuba\n",
            "Successfully installed siuba-0.0.19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr2zJmEf2Eh3",
        "colab_type": "text"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLXojV3K5Y1U",
        "colab_type": "text"
      },
      "source": [
        "*   Reading data\n",
        "*   Change columns names\n",
        "*   Drop NaN rows\n",
        "*   Fill others NaN values by special sign"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j--zuCZf2HvJ",
        "colab_type": "code",
        "outputId": "3f483e11-4372-42fa-d6e2-b5e303986b4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "df = pd.read_excel('polishOriginalDataset.xlsx')\n",
        "df.reset_index()\n",
        "df = df.iloc[:,[6,8,9,10,15,16,19,21,5]]\n",
        "df.columns = [\n",
        "              \"type_of_sentence\",\n",
        "              \"verb_main_semantic_class\",\n",
        "              \"verb_second_semantic_class\",\n",
        "              \"verb_third_semantic_class\",\n",
        "              \"verb_veridical_positive\",\n",
        "              \"verb_veridical_negative\",\n",
        "              \"verb_tense\",\n",
        "              \"t_negation\",\n",
        "              \"semantic_relation\"\n",
        "              ]\n",
        "df.dropna(inplace=True, axis = 0, how = 'all')\n",
        "df.fillna(axis = 0, inplace =True, value=\"none\")\n",
        "df.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type_of_sentence</th>\n",
              "      <th>verb_main_semantic_class</th>\n",
              "      <th>verb_second_semantic_class</th>\n",
              "      <th>verb_third_semantic_class</th>\n",
              "      <th>verb_veridical_positive</th>\n",
              "      <th>verb_veridical_negative</th>\n",
              "      <th>verb_tense</th>\n",
              "      <th>t_negation</th>\n",
              "      <th>semantic_relation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>eliptyczne</td>\n",
              "      <td>mówienia</td>\n",
              "      <td>none</td>\n",
              "      <td>none</td>\n",
              "      <td>o?</td>\n",
              "      <td>?</td>\n",
              "      <td>brak</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>eliptyczne</td>\n",
              "      <td>epistemiczny</td>\n",
              "      <td>none</td>\n",
              "      <td>none</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>past</td>\n",
              "      <td>0</td>\n",
              "      <td>?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>eliptyczne</td>\n",
              "      <td>mówienia</td>\n",
              "      <td>none</td>\n",
              "      <td>none</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>past</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>epistemiczny</td>\n",
              "      <td>percepcyjny</td>\n",
              "      <td>none</td>\n",
              "      <td>\"+\"</td>\n",
              "      <td>\"+\"</td>\n",
              "      <td>past</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>epistemiczny</td>\n",
              "      <td>percepcyjny</td>\n",
              "      <td>none</td>\n",
              "      <td>o</td>\n",
              "      <td>o</td>\n",
              "      <td>present</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  type_of_sentence verb_main_semantic_class  ... t_negation semantic_relation\n",
              "0       eliptyczne                 mówienia  ...          0                 N\n",
              "1       eliptyczne             epistemiczny  ...          0                 ?\n",
              "2       eliptyczne                 mówienia  ...          0                 N\n",
              "3                1             epistemiczny  ...          0                 N\n",
              "4                1             epistemiczny  ...          0                 N\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTXvttwo5v0D",
        "colab_type": "text"
      },
      "source": [
        "### Cleaning data by deleting uncertainty - simplification "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewr4Z-ZH8e6Q",
        "colab_type": "code",
        "outputId": "fe1318c1-0cba-47fa-f26f-b657a1fdc884",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        }
      },
      "source": [
        "# df.type_of_sentence.unique() cleaning \n",
        "df.verb_main_semantic_class = fct_lump(df.verb_main_semantic_class, n=3, other_level=\"?\")\n",
        "\n",
        "# df.verb_second_semantic_class.unique() cleaning \n",
        "df.verb_second_semantic_class = fct_lump(df.verb_second_semantic_class, n=3, other_level=\"?\")\n",
        "\n",
        "# df.verb_third_semantic_class.unique() cleaning \n",
        "df.verb_third_semantic_class = fct_lump(df.verb_third_semantic_class, n=3, other_level=\"?\")\n",
        "\n",
        "# verb veridical positive cleaning\n",
        "df.verb_veridical_positive = df.verb_veridical_positive.apply(lambda x: '+' if '+' in x else x)\n",
        "df.verb_veridical_positive = df.verb_veridical_positive.apply(lambda x: '-' if '-' in x else x)\n",
        "df.verb_veridical_positive = df.verb_veridical_positive.apply(lambda x: 'o' if 'o' in x else x)\n",
        "df.verb_veridical_positive = df.verb_veridical_positive.apply(lambda x: '?' if '?' in x else x)\n",
        "\n",
        "# verb veridical negative cleaning\n",
        "df.verb_veridical_negative = df.verb_veridical_negative.apply(lambda x: '+' if '+' in x else x)\n",
        "df.verb_veridical_negative = df.verb_veridical_negative.apply(lambda x: 'o' if 'o' in x else x)\n",
        "df.verb_veridical_negative = df.verb_veridical_negative.apply(lambda x: '-' if '-' in x else x)\n",
        "df.verb_veridical_negative = df.verb_veridical_negative.apply(lambda x: '?' if '?' in x else x)\n",
        "\n",
        "# df.verb_tense.unique() cleaning not needed\n",
        "# df.t_negation.unique() cleaning not needed\n",
        "# df.semantic_relation.unique() cleaning not needed \n",
        "\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type_of_sentence</th>\n",
              "      <th>verb_main_semantic_class</th>\n",
              "      <th>verb_second_semantic_class</th>\n",
              "      <th>verb_third_semantic_class</th>\n",
              "      <th>verb_veridical_positive</th>\n",
              "      <th>verb_veridical_negative</th>\n",
              "      <th>verb_tense</th>\n",
              "      <th>t_negation</th>\n",
              "      <th>semantic_relation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>eliptyczne</td>\n",
              "      <td>mówienia</td>\n",
              "      <td>none</td>\n",
              "      <td>none</td>\n",
              "      <td>o</td>\n",
              "      <td>?</td>\n",
              "      <td>brak</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>eliptyczne</td>\n",
              "      <td>epistemiczny</td>\n",
              "      <td>none</td>\n",
              "      <td>none</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>past</td>\n",
              "      <td>0</td>\n",
              "      <td>?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>eliptyczne</td>\n",
              "      <td>mówienia</td>\n",
              "      <td>none</td>\n",
              "      <td>none</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>past</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>epistemiczny</td>\n",
              "      <td>?</td>\n",
              "      <td>none</td>\n",
              "      <td>+</td>\n",
              "      <td>+</td>\n",
              "      <td>past</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>epistemiczny</td>\n",
              "      <td>?</td>\n",
              "      <td>none</td>\n",
              "      <td>o</td>\n",
              "      <td>o</td>\n",
              "      <td>present</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  type_of_sentence verb_main_semantic_class  ... t_negation semantic_relation\n",
              "0       eliptyczne                 mówienia  ...          0                 N\n",
              "1       eliptyczne             epistemiczny  ...          0                 ?\n",
              "2       eliptyczne                 mówienia  ...          0                 N\n",
              "3                1             epistemiczny  ...          0                 N\n",
              "4                1             epistemiczny  ...          0                 N\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq5Zke-TO3zd",
        "colab_type": "text"
      },
      "source": [
        "### Possible feature values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVbjgIUMOXXi",
        "colab_type": "code",
        "outputId": "7cd4b2ac-9e0a-407b-da7b-e086b6251849",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "print(df.type_of_sentence.unique())\n",
        "print(df.verb_main_semantic_class.unique())\n",
        "print(df.verb_second_semantic_class.unique())\n",
        "print(df.verb_third_semantic_class.unique())\n",
        "print(df.verb_veridical_positive.unique())\n",
        "print(df.verb_veridical_negative.unique())\n",
        "print(df.verb_tense.unique())\n",
        "print(df.t_negation.unique())\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['eliptyczne' 1 'generalne' 'pytajne' 'modalne'\n",
            " 'powtarzające się zdarzenie' 'warunkowe' 'powinnościowe'\n",
            " 'imperatyw; warunkowe' 'none' '?' 'imperatyw ' 'modal'\n",
            " 'warunkowe; pytajne' 'wolitywne' 'performatyw' 'kontrfaktyczne'\n",
            " 'imperatyw' 'prostujące' 'korygujące' 'performatyw; warunkowe'\n",
            " 'warunkowe; modalne' 'przypuszczające' 'wolitywne; performatyw'\n",
            " 'modalne; pytajne' 'imperatyw; generalne' 'sprostowanie' 'dyspozycyjne'\n",
            " 'modalne; warunkowe' 'pytajne; modalne' 'modalne; alternatywa'\n",
            " 'alternatywa' 'warunkowe; generalne']\n",
            "[mówienia, epistemiczny, ?, odkrycia]\n",
            "Categories (4, object): [mówienia, epistemiczny, ?, odkrycia]\n",
            "[none, ?, epistemiczny, wolicjonalny]\n",
            "Categories (4, object): [none, ?, epistemiczny, wolicjonalny]\n",
            "[none, mówienia, ?, epistemiczny]\n",
            "Categories (4, object): [none, mówienia, ?, epistemiczny]\n",
            "['o' '?' '+' '-']\n",
            "['?' '+' 'o' '-']\n",
            "['brak' 'past' 'present' 'future']\n",
            "[0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEzZDtqp9X1i",
        "colab_type": "code",
        "outputId": "fa392a11-9e8f-4fbb-b074-f1c43b979fd4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "df.verb_main_semantic_class.value_counts()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "epistemiczny    1079\n",
              "mówienia         973\n",
              "?                448\n",
              "odkrycia          96\n",
              "Name: verb_main_semantic_class, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0yf4jWrgB4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# choose columns \n",
        "df = df[[\n",
        "  \"verb_main_semantic_class\",\n",
        "  \"verb_second_semantic_class\",\n",
        " # \"verb_third_semantic_class\",\n",
        "  \"verb_veridical_positive\",\n",
        "  \"verb_veridical_negative\",\n",
        "  \"verb_tense\",\n",
        "  \"semantic_relation\"      \n",
        "]]\n",
        "df.to_csv(\"plData.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOi5V_fDAJBS",
        "colab_type": "text"
      },
      "source": [
        "### Vactorize data and split to features and target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNZdBxWgyu8t",
        "colab_type": "text"
      },
      "source": [
        "Ml classifier labels - target for RandomForest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSR0_iVM0ujy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ml_classifier_labels = df[\"semantic_relation\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YokPfvoJxWoR",
        "colab_type": "text"
      },
      "source": [
        "#### Vectorize (one =hot encoding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r7NRdcwkIX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.get_dummies(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9f1MRWJxhBA",
        "colab_type": "text"
      },
      "source": [
        "#### Split to features and target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp6eOyEdxlMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df.iloc[:,0:-4]\n",
        "y = df.iloc[:,-4:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjQvT5PtxsFz",
        "colab_type": "text"
      },
      "source": [
        "#### Features columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLqFLidq0YLV",
        "colab_type": "code",
        "outputId": "65044983-508e-428e-c163-8c97428c3f4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "X.columns"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['verb_main_semantic_class_?', 'verb_main_semantic_class_epistemiczny',\n",
              "       'verb_main_semantic_class_mówienia',\n",
              "       'verb_main_semantic_class_odkrycia', 'verb_second_semantic_class_?',\n",
              "       'verb_second_semantic_class_epistemiczny',\n",
              "       'verb_second_semantic_class_none',\n",
              "       'verb_second_semantic_class_wolicjonalny', 'verb_veridical_positive_+',\n",
              "       'verb_veridical_positive_-', 'verb_veridical_positive_?',\n",
              "       'verb_veridical_positive_o', 'verb_veridical_negative_+',\n",
              "       'verb_veridical_negative_-', 'verb_veridical_negative_?',\n",
              "       'verb_veridical_negative_o', 'verb_tense_brak', 'verb_tense_future',\n",
              "       'verb_tense_past', 'verb_tense_present'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4896Jynxyew",
        "colab_type": "text"
      },
      "source": [
        "#### Target columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxEJ6K-fxkTU",
        "colab_type": "code",
        "outputId": "cfcc70ab-ffc4-471d-b7a5-9b3427c1cd57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "y.columns"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['semantic_relation_?', 'semantic_relation_C', 'semantic_relation_E',\n",
              "       'semantic_relation_N'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jmoU__ZsXtC",
        "colab_type": "text"
      },
      "source": [
        "### k-fold crossvalidation preparing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUp3OdBNsdn5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k=7\n",
        "from sklearn.model_selection import KFold\n",
        "kfold = KFold(n_splits = k, shuffle=True)\n",
        "\n",
        "acc_per_fold = []\n",
        "loss_per_fold = [] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Xi8QfyAkbiN",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "# Keras model building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln4inIDkB_h5",
        "colab_type": "text"
      },
      "source": [
        "### Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amhvtjnZQwhz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf \n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.metrics import confusion_matrix \n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H15DIedMCgOC",
        "colab_type": "text"
      },
      "source": [
        "It takes only 1-2 minutes to train this model with 7-crossvalidation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7ZB5xOvAZbu",
        "colab_type": "code",
        "outputId": "d032f1b4-88c7-40c1-c783-fa9e96fb516d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "fold_no = 1 \n",
        "\n",
        "#get number of columns in training data\n",
        "n_cols = X.shape[1]\n",
        "print(n_cols)\n",
        "\n",
        "for train, test in kfold.split(X,y):\n",
        "  # FOLD PRINTOUT\n",
        "  print(100*'_')\n",
        "  print (f\"FOLD NO {fold_no} START\")  \n",
        "\n",
        "  # model architecture  \n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(Dense(16, activation='relu', input_shape=(n_cols,)))\n",
        "  model.add(tf.keras.layers.Dropout(rate=0.4))\n",
        "  model.add(Dense(32, activation='relu'))\n",
        "  model.add(tf.keras.layers.Dropout(rate=0.4))\n",
        "  model.add(Dense(16, activation='relu'))\n",
        "  model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "  # model compile \n",
        "  model.compile(optimizer = 'adam', loss=\"categorical_crossentropy\", metrics = ['accuracy'])\n",
        "  \n",
        "  # training\n",
        "  history = model.fit(X.iloc[train], y.iloc[train], validation_split=0.2, epochs=70)\n",
        "\n",
        "  # scores \n",
        "  scores = model.evaluate(X.iloc[test], y.iloc[test], verbose=0)\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "\n",
        "  # iterator up\n",
        "  fold_no = fold_no + 1"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20\n",
            "____________________________________________________________________________________________________\n",
            "FOLD NO 1 START\n",
            "Epoch 1/70\n",
            "56/56 [==============================] - 0s 5ms/step - loss: 1.2206 - accuracy: 0.4416 - val_loss: 1.0390 - val_accuracy: 0.5640\n",
            "Epoch 2/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.8893 - accuracy: 0.6522 - val_loss: 0.7619 - val_accuracy: 0.6719\n",
            "Epoch 3/70\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.7207 - accuracy: 0.7680 - val_loss: 0.5530 - val_accuracy: 0.8764\n",
            "Epoch 4/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6527 - accuracy: 0.8135 - val_loss: 0.4707 - val_accuracy: 0.8831\n",
            "Epoch 5/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6005 - accuracy: 0.8242 - val_loss: 0.4536 - val_accuracy: 0.8831\n",
            "Epoch 6/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5946 - accuracy: 0.8427 - val_loss: 0.4476 - val_accuracy: 0.8831\n",
            "Epoch 7/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5577 - accuracy: 0.8466 - val_loss: 0.4387 - val_accuracy: 0.8831\n",
            "Epoch 8/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5519 - accuracy: 0.8528 - val_loss: 0.4335 - val_accuracy: 0.8831\n",
            "Epoch 9/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5527 - accuracy: 0.8500 - val_loss: 0.4334 - val_accuracy: 0.8854\n",
            "Epoch 10/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5510 - accuracy: 0.8511 - val_loss: 0.4386 - val_accuracy: 0.8854\n",
            "Epoch 11/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5462 - accuracy: 0.8596 - val_loss: 0.4324 - val_accuracy: 0.8854\n",
            "Epoch 12/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5330 - accuracy: 0.8629 - val_loss: 0.4263 - val_accuracy: 0.8854\n",
            "Epoch 13/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5181 - accuracy: 0.8646 - val_loss: 0.4249 - val_accuracy: 0.8854\n",
            "Epoch 14/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5258 - accuracy: 0.8618 - val_loss: 0.4280 - val_accuracy: 0.8854\n",
            "Epoch 15/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5137 - accuracy: 0.8635 - val_loss: 0.4230 - val_accuracy: 0.8854\n",
            "Epoch 16/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5143 - accuracy: 0.8663 - val_loss: 0.4252 - val_accuracy: 0.8854\n",
            "Epoch 17/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4973 - accuracy: 0.8629 - val_loss: 0.4245 - val_accuracy: 0.8854\n",
            "Epoch 18/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5012 - accuracy: 0.8680 - val_loss: 0.4271 - val_accuracy: 0.8854\n",
            "Epoch 19/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5008 - accuracy: 0.8618 - val_loss: 0.4202 - val_accuracy: 0.8854\n",
            "Epoch 20/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4915 - accuracy: 0.8657 - val_loss: 0.4215 - val_accuracy: 0.8854\n",
            "Epoch 21/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4957 - accuracy: 0.8607 - val_loss: 0.4263 - val_accuracy: 0.8854\n",
            "Epoch 22/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4936 - accuracy: 0.8685 - val_loss: 0.4234 - val_accuracy: 0.8854\n",
            "Epoch 23/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4910 - accuracy: 0.8680 - val_loss: 0.4230 - val_accuracy: 0.8854\n",
            "Epoch 24/70\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4958 - accuracy: 0.8669 - val_loss: 0.4279 - val_accuracy: 0.8854\n",
            "Epoch 25/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4773 - accuracy: 0.8657 - val_loss: 0.4242 - val_accuracy: 0.8854\n",
            "Epoch 26/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4818 - accuracy: 0.8691 - val_loss: 0.4309 - val_accuracy: 0.8854\n",
            "Epoch 27/70\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4813 - accuracy: 0.8702 - val_loss: 0.4269 - val_accuracy: 0.8854\n",
            "Epoch 28/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4890 - accuracy: 0.8742 - val_loss: 0.4296 - val_accuracy: 0.8854\n",
            "Epoch 29/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4694 - accuracy: 0.8685 - val_loss: 0.4210 - val_accuracy: 0.8854\n",
            "Epoch 30/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4761 - accuracy: 0.8674 - val_loss: 0.4264 - val_accuracy: 0.8854\n",
            "Epoch 31/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4752 - accuracy: 0.8725 - val_loss: 0.4275 - val_accuracy: 0.8854\n",
            "Epoch 32/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4748 - accuracy: 0.8680 - val_loss: 0.4256 - val_accuracy: 0.8854\n",
            "Epoch 33/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4812 - accuracy: 0.8697 - val_loss: 0.4262 - val_accuracy: 0.8854\n",
            "Epoch 34/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4723 - accuracy: 0.8713 - val_loss: 0.4271 - val_accuracy: 0.8854\n",
            "Epoch 35/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4775 - accuracy: 0.8713 - val_loss: 0.4285 - val_accuracy: 0.8854\n",
            "Epoch 36/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4656 - accuracy: 0.8713 - val_loss: 0.4310 - val_accuracy: 0.8854\n",
            "Epoch 37/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4697 - accuracy: 0.8697 - val_loss: 0.4363 - val_accuracy: 0.8854\n",
            "Epoch 38/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4733 - accuracy: 0.8669 - val_loss: 0.4377 - val_accuracy: 0.8854\n",
            "Epoch 39/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4678 - accuracy: 0.8697 - val_loss: 0.4370 - val_accuracy: 0.8876\n",
            "Epoch 40/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4718 - accuracy: 0.8713 - val_loss: 0.4341 - val_accuracy: 0.8876\n",
            "Epoch 41/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4674 - accuracy: 0.8691 - val_loss: 0.4372 - val_accuracy: 0.8876\n",
            "Epoch 42/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4665 - accuracy: 0.8697 - val_loss: 0.4318 - val_accuracy: 0.8876\n",
            "Epoch 43/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4710 - accuracy: 0.8685 - val_loss: 0.4345 - val_accuracy: 0.8876\n",
            "Epoch 44/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4558 - accuracy: 0.8725 - val_loss: 0.4373 - val_accuracy: 0.8876\n",
            "Epoch 45/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4542 - accuracy: 0.8747 - val_loss: 0.4380 - val_accuracy: 0.8876\n",
            "Epoch 46/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4663 - accuracy: 0.8691 - val_loss: 0.4396 - val_accuracy: 0.8876\n",
            "Epoch 47/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4596 - accuracy: 0.8674 - val_loss: 0.4373 - val_accuracy: 0.8876\n",
            "Epoch 48/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4592 - accuracy: 0.8713 - val_loss: 0.4431 - val_accuracy: 0.8876\n",
            "Epoch 49/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4550 - accuracy: 0.8736 - val_loss: 0.4452 - val_accuracy: 0.8876\n",
            "Epoch 50/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4602 - accuracy: 0.8742 - val_loss: 0.4434 - val_accuracy: 0.8899\n",
            "Epoch 51/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4572 - accuracy: 0.8713 - val_loss: 0.4497 - val_accuracy: 0.8876\n",
            "Epoch 52/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4476 - accuracy: 0.8736 - val_loss: 0.4458 - val_accuracy: 0.8876\n",
            "Epoch 53/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4480 - accuracy: 0.8730 - val_loss: 0.4412 - val_accuracy: 0.8876\n",
            "Epoch 54/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4460 - accuracy: 0.8742 - val_loss: 0.4447 - val_accuracy: 0.8876\n",
            "Epoch 55/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4460 - accuracy: 0.8708 - val_loss: 0.4457 - val_accuracy: 0.8876\n",
            "Epoch 56/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4494 - accuracy: 0.8742 - val_loss: 0.4437 - val_accuracy: 0.8876\n",
            "Epoch 57/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4525 - accuracy: 0.8713 - val_loss: 0.4512 - val_accuracy: 0.8899\n",
            "Epoch 58/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4520 - accuracy: 0.8736 - val_loss: 0.4511 - val_accuracy: 0.8876\n",
            "Epoch 59/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4472 - accuracy: 0.8753 - val_loss: 0.4530 - val_accuracy: 0.8876\n",
            "Epoch 60/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4420 - accuracy: 0.8747 - val_loss: 0.4475 - val_accuracy: 0.8899\n",
            "Epoch 61/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4451 - accuracy: 0.8730 - val_loss: 0.4513 - val_accuracy: 0.8899\n",
            "Epoch 62/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4568 - accuracy: 0.8730 - val_loss: 0.4520 - val_accuracy: 0.8899\n",
            "Epoch 63/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4477 - accuracy: 0.8747 - val_loss: 0.4561 - val_accuracy: 0.8809\n",
            "Epoch 64/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4453 - accuracy: 0.8753 - val_loss: 0.4521 - val_accuracy: 0.8899\n",
            "Epoch 65/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4403 - accuracy: 0.8753 - val_loss: 0.4539 - val_accuracy: 0.8899\n",
            "Epoch 66/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4412 - accuracy: 0.8798 - val_loss: 0.4552 - val_accuracy: 0.8876\n",
            "Epoch 67/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4407 - accuracy: 0.8770 - val_loss: 0.4557 - val_accuracy: 0.8809\n",
            "Epoch 68/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4408 - accuracy: 0.8764 - val_loss: 0.4515 - val_accuracy: 0.8899\n",
            "Epoch 69/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4441 - accuracy: 0.8803 - val_loss: 0.4550 - val_accuracy: 0.8876\n",
            "Epoch 70/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4425 - accuracy: 0.8753 - val_loss: 0.4577 - val_accuracy: 0.8876\n",
            "____________________________________________________________________________________________________\n",
            "FOLD NO 2 START\n",
            "Epoch 1/70\n",
            "56/56 [==============================] - 0s 4ms/step - loss: 1.2110 - accuracy: 0.5388 - val_loss: 0.8953 - val_accuracy: 0.8629\n",
            "Epoch 2/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.7772 - accuracy: 0.7955 - val_loss: 0.4765 - val_accuracy: 0.8809\n",
            "Epoch 3/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6136 - accuracy: 0.8393 - val_loss: 0.4179 - val_accuracy: 0.8809\n",
            "Epoch 4/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5842 - accuracy: 0.8517 - val_loss: 0.4203 - val_accuracy: 0.8809\n",
            "Epoch 5/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5663 - accuracy: 0.8567 - val_loss: 0.4176 - val_accuracy: 0.8809\n",
            "Epoch 6/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5578 - accuracy: 0.8562 - val_loss: 0.4205 - val_accuracy: 0.8809\n",
            "Epoch 7/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5438 - accuracy: 0.8640 - val_loss: 0.4191 - val_accuracy: 0.8809\n",
            "Epoch 8/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5399 - accuracy: 0.8646 - val_loss: 0.4216 - val_accuracy: 0.8809\n",
            "Epoch 9/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5394 - accuracy: 0.8640 - val_loss: 0.4232 - val_accuracy: 0.8831\n",
            "Epoch 10/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5151 - accuracy: 0.8702 - val_loss: 0.4195 - val_accuracy: 0.8809\n",
            "Epoch 11/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5178 - accuracy: 0.8663 - val_loss: 0.4225 - val_accuracy: 0.8809\n",
            "Epoch 12/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4963 - accuracy: 0.8663 - val_loss: 0.4206 - val_accuracy: 0.8809\n",
            "Epoch 13/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5058 - accuracy: 0.8646 - val_loss: 0.4237 - val_accuracy: 0.8809\n",
            "Epoch 14/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5011 - accuracy: 0.8646 - val_loss: 0.4213 - val_accuracy: 0.8809\n",
            "Epoch 15/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4968 - accuracy: 0.8702 - val_loss: 0.4234 - val_accuracy: 0.8809\n",
            "Epoch 16/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4851 - accuracy: 0.8708 - val_loss: 0.4217 - val_accuracy: 0.8809\n",
            "Epoch 17/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4919 - accuracy: 0.8691 - val_loss: 0.4230 - val_accuracy: 0.8809\n",
            "Epoch 18/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4865 - accuracy: 0.8680 - val_loss: 0.4246 - val_accuracy: 0.8809\n",
            "Epoch 19/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4902 - accuracy: 0.8685 - val_loss: 0.4225 - val_accuracy: 0.8809\n",
            "Epoch 20/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4941 - accuracy: 0.8708 - val_loss: 0.4256 - val_accuracy: 0.8831\n",
            "Epoch 21/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4822 - accuracy: 0.8719 - val_loss: 0.4259 - val_accuracy: 0.8809\n",
            "Epoch 22/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4931 - accuracy: 0.8708 - val_loss: 0.4289 - val_accuracy: 0.8831\n",
            "Epoch 23/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4745 - accuracy: 0.8702 - val_loss: 0.4242 - val_accuracy: 0.8831\n",
            "Epoch 24/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4701 - accuracy: 0.8685 - val_loss: 0.4247 - val_accuracy: 0.8854\n",
            "Epoch 25/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4713 - accuracy: 0.8730 - val_loss: 0.4256 - val_accuracy: 0.8854\n",
            "Epoch 26/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4756 - accuracy: 0.8702 - val_loss: 0.4190 - val_accuracy: 0.8831\n",
            "Epoch 27/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4706 - accuracy: 0.8708 - val_loss: 0.4217 - val_accuracy: 0.8854\n",
            "Epoch 28/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4703 - accuracy: 0.8697 - val_loss: 0.4201 - val_accuracy: 0.8854\n",
            "Epoch 29/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4589 - accuracy: 0.8730 - val_loss: 0.4212 - val_accuracy: 0.8854\n",
            "Epoch 30/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4708 - accuracy: 0.8730 - val_loss: 0.4202 - val_accuracy: 0.8854\n",
            "Epoch 31/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4637 - accuracy: 0.8742 - val_loss: 0.4210 - val_accuracy: 0.8854\n",
            "Epoch 32/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4672 - accuracy: 0.8719 - val_loss: 0.4248 - val_accuracy: 0.8854\n",
            "Epoch 33/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4572 - accuracy: 0.8725 - val_loss: 0.4219 - val_accuracy: 0.8854\n",
            "Epoch 34/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4621 - accuracy: 0.8719 - val_loss: 0.4266 - val_accuracy: 0.8854\n",
            "Epoch 35/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4647 - accuracy: 0.8708 - val_loss: 0.4259 - val_accuracy: 0.8854\n",
            "Epoch 36/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4540 - accuracy: 0.8730 - val_loss: 0.4268 - val_accuracy: 0.8854\n",
            "Epoch 37/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4584 - accuracy: 0.8697 - val_loss: 0.4240 - val_accuracy: 0.8854\n",
            "Epoch 38/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4618 - accuracy: 0.8708 - val_loss: 0.4238 - val_accuracy: 0.8854\n",
            "Epoch 39/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4538 - accuracy: 0.8725 - val_loss: 0.4247 - val_accuracy: 0.8854\n",
            "Epoch 40/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4591 - accuracy: 0.8691 - val_loss: 0.4244 - val_accuracy: 0.8854\n",
            "Epoch 41/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4576 - accuracy: 0.8697 - val_loss: 0.4285 - val_accuracy: 0.8854\n",
            "Epoch 42/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4496 - accuracy: 0.8742 - val_loss: 0.4261 - val_accuracy: 0.8854\n",
            "Epoch 43/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4499 - accuracy: 0.8730 - val_loss: 0.4295 - val_accuracy: 0.8854\n",
            "Epoch 44/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4444 - accuracy: 0.8736 - val_loss: 0.4256 - val_accuracy: 0.8854\n",
            "Epoch 45/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4479 - accuracy: 0.8708 - val_loss: 0.4226 - val_accuracy: 0.8854\n",
            "Epoch 46/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4614 - accuracy: 0.8719 - val_loss: 0.4272 - val_accuracy: 0.8854\n",
            "Epoch 47/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4518 - accuracy: 0.8725 - val_loss: 0.4244 - val_accuracy: 0.8854\n",
            "Epoch 48/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4482 - accuracy: 0.8747 - val_loss: 0.4262 - val_accuracy: 0.8854\n",
            "Epoch 49/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4479 - accuracy: 0.8736 - val_loss: 0.4271 - val_accuracy: 0.8854\n",
            "Epoch 50/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4414 - accuracy: 0.8753 - val_loss: 0.4281 - val_accuracy: 0.8854\n",
            "Epoch 51/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4446 - accuracy: 0.8736 - val_loss: 0.4276 - val_accuracy: 0.8854\n",
            "Epoch 52/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4432 - accuracy: 0.8753 - val_loss: 0.4262 - val_accuracy: 0.8854\n",
            "Epoch 53/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4464 - accuracy: 0.8758 - val_loss: 0.4270 - val_accuracy: 0.8854\n",
            "Epoch 54/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4488 - accuracy: 0.8725 - val_loss: 0.4249 - val_accuracy: 0.8854\n",
            "Epoch 55/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4354 - accuracy: 0.8742 - val_loss: 0.4269 - val_accuracy: 0.8854\n",
            "Epoch 56/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4558 - accuracy: 0.8742 - val_loss: 0.4327 - val_accuracy: 0.8854\n",
            "Epoch 57/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4411 - accuracy: 0.8747 - val_loss: 0.4297 - val_accuracy: 0.8854\n",
            "Epoch 58/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4454 - accuracy: 0.8730 - val_loss: 0.4249 - val_accuracy: 0.8854\n",
            "Epoch 59/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4440 - accuracy: 0.8742 - val_loss: 0.4280 - val_accuracy: 0.8854\n",
            "Epoch 60/70\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4427 - accuracy: 0.8747 - val_loss: 0.4307 - val_accuracy: 0.8854\n",
            "Epoch 61/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4423 - accuracy: 0.8736 - val_loss: 0.4287 - val_accuracy: 0.8854\n",
            "Epoch 62/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4376 - accuracy: 0.8742 - val_loss: 0.4333 - val_accuracy: 0.8854\n",
            "Epoch 63/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4397 - accuracy: 0.8758 - val_loss: 0.4394 - val_accuracy: 0.8854\n",
            "Epoch 64/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4434 - accuracy: 0.8770 - val_loss: 0.4293 - val_accuracy: 0.8854\n",
            "Epoch 65/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4375 - accuracy: 0.8770 - val_loss: 0.4252 - val_accuracy: 0.8854\n",
            "Epoch 66/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4489 - accuracy: 0.8725 - val_loss: 0.4252 - val_accuracy: 0.8854\n",
            "Epoch 67/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4348 - accuracy: 0.8764 - val_loss: 0.4238 - val_accuracy: 0.8854\n",
            "Epoch 68/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4349 - accuracy: 0.8792 - val_loss: 0.4258 - val_accuracy: 0.8854\n",
            "Epoch 69/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4379 - accuracy: 0.8775 - val_loss: 0.4259 - val_accuracy: 0.8854\n",
            "Epoch 70/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4315 - accuracy: 0.8747 - val_loss: 0.4289 - val_accuracy: 0.8854\n",
            "____________________________________________________________________________________________________\n",
            "FOLD NO 3 START\n",
            "Epoch 1/70\n",
            "56/56 [==============================] - 0s 4ms/step - loss: 1.2282 - accuracy: 0.4747 - val_loss: 0.9653 - val_accuracy: 0.8472\n",
            "Epoch 2/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.8403 - accuracy: 0.7174 - val_loss: 0.6824 - val_accuracy: 0.8315\n",
            "Epoch 3/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.7285 - accuracy: 0.7624 - val_loss: 0.5706 - val_accuracy: 0.8539\n",
            "Epoch 4/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6496 - accuracy: 0.8051 - val_loss: 0.5012 - val_accuracy: 0.8584\n",
            "Epoch 5/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6041 - accuracy: 0.8298 - val_loss: 0.4839 - val_accuracy: 0.8787\n",
            "Epoch 6/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5947 - accuracy: 0.8337 - val_loss: 0.4753 - val_accuracy: 0.8764\n",
            "Epoch 7/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5904 - accuracy: 0.8326 - val_loss: 0.4709 - val_accuracy: 0.8764\n",
            "Epoch 8/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5604 - accuracy: 0.8416 - val_loss: 0.4651 - val_accuracy: 0.8764\n",
            "Epoch 9/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5493 - accuracy: 0.8483 - val_loss: 0.4654 - val_accuracy: 0.8764\n",
            "Epoch 10/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5491 - accuracy: 0.8455 - val_loss: 0.4626 - val_accuracy: 0.8764\n",
            "Epoch 11/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5411 - accuracy: 0.8494 - val_loss: 0.4666 - val_accuracy: 0.8764\n",
            "Epoch 12/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5378 - accuracy: 0.8562 - val_loss: 0.4644 - val_accuracy: 0.8764\n",
            "Epoch 13/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5268 - accuracy: 0.8584 - val_loss: 0.4638 - val_accuracy: 0.8764\n",
            "Epoch 14/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5309 - accuracy: 0.8545 - val_loss: 0.4636 - val_accuracy: 0.8764\n",
            "Epoch 15/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5229 - accuracy: 0.8624 - val_loss: 0.4669 - val_accuracy: 0.8764\n",
            "Epoch 16/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5100 - accuracy: 0.8601 - val_loss: 0.4649 - val_accuracy: 0.8764\n",
            "Epoch 17/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5074 - accuracy: 0.8657 - val_loss: 0.4645 - val_accuracy: 0.8764\n",
            "Epoch 18/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5101 - accuracy: 0.8646 - val_loss: 0.4666 - val_accuracy: 0.8764\n",
            "Epoch 19/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4956 - accuracy: 0.8674 - val_loss: 0.4670 - val_accuracy: 0.8764\n",
            "Epoch 20/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4848 - accuracy: 0.8669 - val_loss: 0.4684 - val_accuracy: 0.8764\n",
            "Epoch 21/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5026 - accuracy: 0.8652 - val_loss: 0.4661 - val_accuracy: 0.8764\n",
            "Epoch 22/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.8680 - val_loss: 0.4642 - val_accuracy: 0.8764\n",
            "Epoch 23/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4760 - accuracy: 0.8702 - val_loss: 0.4643 - val_accuracy: 0.8764\n",
            "Epoch 24/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4859 - accuracy: 0.8691 - val_loss: 0.4706 - val_accuracy: 0.8764\n",
            "Epoch 25/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4748 - accuracy: 0.8657 - val_loss: 0.4647 - val_accuracy: 0.8764\n",
            "Epoch 26/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4869 - accuracy: 0.8702 - val_loss: 0.4637 - val_accuracy: 0.8764\n",
            "Epoch 27/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4689 - accuracy: 0.8713 - val_loss: 0.4608 - val_accuracy: 0.8764\n",
            "Epoch 28/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4726 - accuracy: 0.8730 - val_loss: 0.4634 - val_accuracy: 0.8764\n",
            "Epoch 29/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4613 - accuracy: 0.8708 - val_loss: 0.4619 - val_accuracy: 0.8764\n",
            "Epoch 30/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4789 - accuracy: 0.8680 - val_loss: 0.4663 - val_accuracy: 0.8764\n",
            "Epoch 31/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4682 - accuracy: 0.8663 - val_loss: 0.4614 - val_accuracy: 0.8764\n",
            "Epoch 32/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4789 - accuracy: 0.8685 - val_loss: 0.4630 - val_accuracy: 0.8764\n",
            "Epoch 33/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4657 - accuracy: 0.8713 - val_loss: 0.4630 - val_accuracy: 0.8764\n",
            "Epoch 34/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4669 - accuracy: 0.8770 - val_loss: 0.4650 - val_accuracy: 0.8764\n",
            "Epoch 35/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4735 - accuracy: 0.8781 - val_loss: 0.4663 - val_accuracy: 0.8764\n",
            "Epoch 36/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4695 - accuracy: 0.8702 - val_loss: 0.4669 - val_accuracy: 0.8764\n",
            "Epoch 37/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4586 - accuracy: 0.8753 - val_loss: 0.4650 - val_accuracy: 0.8764\n",
            "Epoch 38/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4556 - accuracy: 0.8730 - val_loss: 0.4665 - val_accuracy: 0.8764\n",
            "Epoch 39/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4584 - accuracy: 0.8747 - val_loss: 0.4658 - val_accuracy: 0.8764\n",
            "Epoch 40/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4577 - accuracy: 0.8764 - val_loss: 0.4635 - val_accuracy: 0.8764\n",
            "Epoch 41/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4546 - accuracy: 0.8697 - val_loss: 0.4649 - val_accuracy: 0.8764\n",
            "Epoch 42/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4639 - accuracy: 0.8691 - val_loss: 0.4712 - val_accuracy: 0.8764\n",
            "Epoch 43/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4566 - accuracy: 0.8713 - val_loss: 0.4703 - val_accuracy: 0.8764\n",
            "Epoch 44/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4595 - accuracy: 0.8713 - val_loss: 0.4655 - val_accuracy: 0.8764\n",
            "Epoch 45/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4570 - accuracy: 0.8736 - val_loss: 0.4625 - val_accuracy: 0.8764\n",
            "Epoch 46/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4518 - accuracy: 0.8719 - val_loss: 0.4718 - val_accuracy: 0.8764\n",
            "Epoch 47/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4532 - accuracy: 0.8736 - val_loss: 0.4642 - val_accuracy: 0.8764\n",
            "Epoch 48/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4489 - accuracy: 0.8730 - val_loss: 0.4673 - val_accuracy: 0.8764\n",
            "Epoch 49/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4501 - accuracy: 0.8764 - val_loss: 0.4668 - val_accuracy: 0.8764\n",
            "Epoch 50/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4410 - accuracy: 0.8764 - val_loss: 0.4647 - val_accuracy: 0.8764\n",
            "Epoch 51/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4366 - accuracy: 0.8747 - val_loss: 0.4658 - val_accuracy: 0.8764\n",
            "Epoch 52/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4490 - accuracy: 0.8713 - val_loss: 0.4644 - val_accuracy: 0.8764\n",
            "Epoch 53/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4357 - accuracy: 0.8770 - val_loss: 0.4693 - val_accuracy: 0.8764\n",
            "Epoch 54/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4383 - accuracy: 0.8820 - val_loss: 0.4624 - val_accuracy: 0.8764\n",
            "Epoch 55/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4499 - accuracy: 0.8708 - val_loss: 0.4613 - val_accuracy: 0.8764\n",
            "Epoch 56/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4477 - accuracy: 0.8798 - val_loss: 0.4616 - val_accuracy: 0.8764\n",
            "Epoch 57/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4495 - accuracy: 0.8747 - val_loss: 0.4615 - val_accuracy: 0.8764\n",
            "Epoch 58/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4401 - accuracy: 0.8798 - val_loss: 0.4630 - val_accuracy: 0.8764\n",
            "Epoch 59/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4514 - accuracy: 0.8770 - val_loss: 0.4632 - val_accuracy: 0.8764\n",
            "Epoch 60/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4470 - accuracy: 0.8775 - val_loss: 0.4635 - val_accuracy: 0.8764\n",
            "Epoch 61/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4352 - accuracy: 0.8770 - val_loss: 0.4653 - val_accuracy: 0.8764\n",
            "Epoch 62/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4375 - accuracy: 0.8758 - val_loss: 0.4643 - val_accuracy: 0.8764\n",
            "Epoch 63/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4413 - accuracy: 0.8787 - val_loss: 0.4658 - val_accuracy: 0.8764\n",
            "Epoch 64/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4400 - accuracy: 0.8775 - val_loss: 0.4690 - val_accuracy: 0.8764\n",
            "Epoch 65/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4465 - accuracy: 0.8747 - val_loss: 0.4665 - val_accuracy: 0.8764\n",
            "Epoch 66/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4442 - accuracy: 0.8787 - val_loss: 0.4650 - val_accuracy: 0.8764\n",
            "Epoch 67/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4529 - accuracy: 0.8787 - val_loss: 0.4716 - val_accuracy: 0.8764\n",
            "Epoch 68/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4405 - accuracy: 0.8764 - val_loss: 0.4678 - val_accuracy: 0.8764\n",
            "Epoch 69/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4299 - accuracy: 0.8770 - val_loss: 0.4668 - val_accuracy: 0.8764\n",
            "Epoch 70/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4266 - accuracy: 0.8831 - val_loss: 0.4710 - val_accuracy: 0.8764\n",
            "____________________________________________________________________________________________________\n",
            "FOLD NO 4 START\n",
            "Epoch 1/70\n",
            "56/56 [==============================] - 0s 4ms/step - loss: 1.2530 - accuracy: 0.4247 - val_loss: 0.9701 - val_accuracy: 0.6270\n",
            "Epoch 2/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.8340 - accuracy: 0.7090 - val_loss: 0.6855 - val_accuracy: 0.8270\n",
            "Epoch 3/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6748 - accuracy: 0.7860 - val_loss: 0.5341 - val_accuracy: 0.8764\n",
            "Epoch 4/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6110 - accuracy: 0.8287 - val_loss: 0.4986 - val_accuracy: 0.8764\n",
            "Epoch 5/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6155 - accuracy: 0.8399 - val_loss: 0.4897 - val_accuracy: 0.8764\n",
            "Epoch 6/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5800 - accuracy: 0.8596 - val_loss: 0.4716 - val_accuracy: 0.8787\n",
            "Epoch 7/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5578 - accuracy: 0.8534 - val_loss: 0.4651 - val_accuracy: 0.8787\n",
            "Epoch 8/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5554 - accuracy: 0.8539 - val_loss: 0.4631 - val_accuracy: 0.8787\n",
            "Epoch 9/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5272 - accuracy: 0.8629 - val_loss: 0.4667 - val_accuracy: 0.8787\n",
            "Epoch 10/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5340 - accuracy: 0.8652 - val_loss: 0.4616 - val_accuracy: 0.8787\n",
            "Epoch 11/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5113 - accuracy: 0.8624 - val_loss: 0.4608 - val_accuracy: 0.8787\n",
            "Epoch 12/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5188 - accuracy: 0.8646 - val_loss: 0.4617 - val_accuracy: 0.8787\n",
            "Epoch 13/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5012 - accuracy: 0.8646 - val_loss: 0.4607 - val_accuracy: 0.8787\n",
            "Epoch 14/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4989 - accuracy: 0.8674 - val_loss: 0.4570 - val_accuracy: 0.8787\n",
            "Epoch 15/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5005 - accuracy: 0.8640 - val_loss: 0.4608 - val_accuracy: 0.8787\n",
            "Epoch 16/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4883 - accuracy: 0.8652 - val_loss: 0.4570 - val_accuracy: 0.8787\n",
            "Epoch 17/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4764 - accuracy: 0.8691 - val_loss: 0.4516 - val_accuracy: 0.8787\n",
            "Epoch 18/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4782 - accuracy: 0.8669 - val_loss: 0.4544 - val_accuracy: 0.8787\n",
            "Epoch 19/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4785 - accuracy: 0.8725 - val_loss: 0.4575 - val_accuracy: 0.8787\n",
            "Epoch 20/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4800 - accuracy: 0.8657 - val_loss: 0.4574 - val_accuracy: 0.8787\n",
            "Epoch 21/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4691 - accuracy: 0.8685 - val_loss: 0.4574 - val_accuracy: 0.8787\n",
            "Epoch 22/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4546 - accuracy: 0.8719 - val_loss: 0.4521 - val_accuracy: 0.8787\n",
            "Epoch 23/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4631 - accuracy: 0.8730 - val_loss: 0.4539 - val_accuracy: 0.8787\n",
            "Epoch 24/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4682 - accuracy: 0.8680 - val_loss: 0.4544 - val_accuracy: 0.8787\n",
            "Epoch 25/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4667 - accuracy: 0.8685 - val_loss: 0.4591 - val_accuracy: 0.8787\n",
            "Epoch 26/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4620 - accuracy: 0.8719 - val_loss: 0.4525 - val_accuracy: 0.8787\n",
            "Epoch 27/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4717 - accuracy: 0.8697 - val_loss: 0.4518 - val_accuracy: 0.8787\n",
            "Epoch 28/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4545 - accuracy: 0.8742 - val_loss: 0.4518 - val_accuracy: 0.8787\n",
            "Epoch 29/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4644 - accuracy: 0.8691 - val_loss: 0.4498 - val_accuracy: 0.8787\n",
            "Epoch 30/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4513 - accuracy: 0.8725 - val_loss: 0.4530 - val_accuracy: 0.8787\n",
            "Epoch 31/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4550 - accuracy: 0.8742 - val_loss: 0.4466 - val_accuracy: 0.8787\n",
            "Epoch 32/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4503 - accuracy: 0.8713 - val_loss: 0.4520 - val_accuracy: 0.8787\n",
            "Epoch 33/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4452 - accuracy: 0.8747 - val_loss: 0.4465 - val_accuracy: 0.8787\n",
            "Epoch 34/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4575 - accuracy: 0.8702 - val_loss: 0.4488 - val_accuracy: 0.8787\n",
            "Epoch 35/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4567 - accuracy: 0.8685 - val_loss: 0.4498 - val_accuracy: 0.8787\n",
            "Epoch 36/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4545 - accuracy: 0.8736 - val_loss: 0.4549 - val_accuracy: 0.8787\n",
            "Epoch 37/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4377 - accuracy: 0.8736 - val_loss: 0.4528 - val_accuracy: 0.8787\n",
            "Epoch 38/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4627 - accuracy: 0.8702 - val_loss: 0.4534 - val_accuracy: 0.8787\n",
            "Epoch 39/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4378 - accuracy: 0.8764 - val_loss: 0.4540 - val_accuracy: 0.8787\n",
            "Epoch 40/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4515 - accuracy: 0.8725 - val_loss: 0.4559 - val_accuracy: 0.8787\n",
            "Epoch 41/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4571 - accuracy: 0.8764 - val_loss: 0.4576 - val_accuracy: 0.8787\n",
            "Epoch 42/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4519 - accuracy: 0.8792 - val_loss: 0.4626 - val_accuracy: 0.8787\n",
            "Epoch 43/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4409 - accuracy: 0.8758 - val_loss: 0.4533 - val_accuracy: 0.8787\n",
            "Epoch 44/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4316 - accuracy: 0.8753 - val_loss: 0.4491 - val_accuracy: 0.8787\n",
            "Epoch 45/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4453 - accuracy: 0.8764 - val_loss: 0.4514 - val_accuracy: 0.8787\n",
            "Epoch 46/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4472 - accuracy: 0.8708 - val_loss: 0.4502 - val_accuracy: 0.8787\n",
            "Epoch 47/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4407 - accuracy: 0.8747 - val_loss: 0.4511 - val_accuracy: 0.8787\n",
            "Epoch 48/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4527 - accuracy: 0.8775 - val_loss: 0.4555 - val_accuracy: 0.8787\n",
            "Epoch 49/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4472 - accuracy: 0.8713 - val_loss: 0.4557 - val_accuracy: 0.8787\n",
            "Epoch 50/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4368 - accuracy: 0.8719 - val_loss: 0.4560 - val_accuracy: 0.8787\n",
            "Epoch 51/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4334 - accuracy: 0.8730 - val_loss: 0.4584 - val_accuracy: 0.8787\n",
            "Epoch 52/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4427 - accuracy: 0.8758 - val_loss: 0.4624 - val_accuracy: 0.8787\n",
            "Epoch 53/70\n",
            "56/56 [==============================] - 0s 6ms/step - loss: 0.4341 - accuracy: 0.8764 - val_loss: 0.4592 - val_accuracy: 0.8787\n",
            "Epoch 54/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4401 - accuracy: 0.8764 - val_loss: 0.4533 - val_accuracy: 0.8787\n",
            "Epoch 55/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4370 - accuracy: 0.8758 - val_loss: 0.4608 - val_accuracy: 0.8787\n",
            "Epoch 56/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4314 - accuracy: 0.8753 - val_loss: 0.4661 - val_accuracy: 0.8787\n",
            "Epoch 57/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4378 - accuracy: 0.8742 - val_loss: 0.4543 - val_accuracy: 0.8787\n",
            "Epoch 58/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4309 - accuracy: 0.8736 - val_loss: 0.4486 - val_accuracy: 0.8787\n",
            "Epoch 59/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4373 - accuracy: 0.8719 - val_loss: 0.4493 - val_accuracy: 0.8787\n",
            "Epoch 60/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4213 - accuracy: 0.8770 - val_loss: 0.4480 - val_accuracy: 0.8787\n",
            "Epoch 61/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4229 - accuracy: 0.8803 - val_loss: 0.4507 - val_accuracy: 0.8787\n",
            "Epoch 62/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4298 - accuracy: 0.8764 - val_loss: 0.4549 - val_accuracy: 0.8787\n",
            "Epoch 63/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4355 - accuracy: 0.8753 - val_loss: 0.4482 - val_accuracy: 0.8787\n",
            "Epoch 64/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4223 - accuracy: 0.8753 - val_loss: 0.4539 - val_accuracy: 0.8787\n",
            "Epoch 65/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4364 - accuracy: 0.8798 - val_loss: 0.4535 - val_accuracy: 0.8787\n",
            "Epoch 66/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4290 - accuracy: 0.8775 - val_loss: 0.4529 - val_accuracy: 0.8787\n",
            "Epoch 67/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4277 - accuracy: 0.8787 - val_loss: 0.4552 - val_accuracy: 0.8787\n",
            "Epoch 68/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4248 - accuracy: 0.8798 - val_loss: 0.4508 - val_accuracy: 0.8787\n",
            "Epoch 69/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4418 - accuracy: 0.8758 - val_loss: 0.4577 - val_accuracy: 0.8787\n",
            "Epoch 70/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4296 - accuracy: 0.8758 - val_loss: 0.4554 - val_accuracy: 0.8787\n",
            "____________________________________________________________________________________________________\n",
            "FOLD NO 5 START\n",
            "Epoch 1/70\n",
            "56/56 [==============================] - 0s 4ms/step - loss: 1.3303 - accuracy: 0.3921 - val_loss: 1.1049 - val_accuracy: 0.5416\n",
            "Epoch 2/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.9471 - accuracy: 0.6371 - val_loss: 0.8820 - val_accuracy: 0.5416\n",
            "Epoch 3/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.7892 - accuracy: 0.6961 - val_loss: 0.7313 - val_accuracy: 0.8494\n",
            "Epoch 4/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6968 - accuracy: 0.7961 - val_loss: 0.5924 - val_accuracy: 0.8719\n",
            "Epoch 5/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6411 - accuracy: 0.8213 - val_loss: 0.5088 - val_accuracy: 0.8742\n",
            "Epoch 6/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5943 - accuracy: 0.8354 - val_loss: 0.4839 - val_accuracy: 0.8742\n",
            "Epoch 7/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5690 - accuracy: 0.8489 - val_loss: 0.4739 - val_accuracy: 0.8742\n",
            "Epoch 8/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5523 - accuracy: 0.8528 - val_loss: 0.4701 - val_accuracy: 0.8742\n",
            "Epoch 9/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5571 - accuracy: 0.8500 - val_loss: 0.4678 - val_accuracy: 0.8742\n",
            "Epoch 10/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5524 - accuracy: 0.8551 - val_loss: 0.4722 - val_accuracy: 0.8742\n",
            "Epoch 11/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5546 - accuracy: 0.8579 - val_loss: 0.4632 - val_accuracy: 0.8742\n",
            "Epoch 12/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5420 - accuracy: 0.8612 - val_loss: 0.4660 - val_accuracy: 0.8742\n",
            "Epoch 13/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5321 - accuracy: 0.8579 - val_loss: 0.4631 - val_accuracy: 0.8742\n",
            "Epoch 14/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5318 - accuracy: 0.8612 - val_loss: 0.4614 - val_accuracy: 0.8742\n",
            "Epoch 15/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5246 - accuracy: 0.8624 - val_loss: 0.4631 - val_accuracy: 0.8742\n",
            "Epoch 16/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5110 - accuracy: 0.8646 - val_loss: 0.4541 - val_accuracy: 0.8742\n",
            "Epoch 17/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5252 - accuracy: 0.8624 - val_loss: 0.4491 - val_accuracy: 0.8742\n",
            "Epoch 18/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5095 - accuracy: 0.8646 - val_loss: 0.4544 - val_accuracy: 0.8742\n",
            "Epoch 19/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5206 - accuracy: 0.8669 - val_loss: 0.4569 - val_accuracy: 0.8742\n",
            "Epoch 20/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5029 - accuracy: 0.8652 - val_loss: 0.4516 - val_accuracy: 0.8742\n",
            "Epoch 21/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4950 - accuracy: 0.8646 - val_loss: 0.4510 - val_accuracy: 0.8742\n",
            "Epoch 22/70\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4997 - accuracy: 0.8674 - val_loss: 0.4516 - val_accuracy: 0.8742\n",
            "Epoch 23/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5043 - accuracy: 0.8702 - val_loss: 0.4550 - val_accuracy: 0.8742\n",
            "Epoch 24/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4921 - accuracy: 0.8713 - val_loss: 0.4513 - val_accuracy: 0.8742\n",
            "Epoch 25/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5024 - accuracy: 0.8697 - val_loss: 0.4517 - val_accuracy: 0.8742\n",
            "Epoch 26/70\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4953 - accuracy: 0.8663 - val_loss: 0.4545 - val_accuracy: 0.8742\n",
            "Epoch 27/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4988 - accuracy: 0.8713 - val_loss: 0.4547 - val_accuracy: 0.8742\n",
            "Epoch 28/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4891 - accuracy: 0.8669 - val_loss: 0.4562 - val_accuracy: 0.8742\n",
            "Epoch 29/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4798 - accuracy: 0.8685 - val_loss: 0.4562 - val_accuracy: 0.8742\n",
            "Epoch 30/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4857 - accuracy: 0.8680 - val_loss: 0.4540 - val_accuracy: 0.8742\n",
            "Epoch 31/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4857 - accuracy: 0.8702 - val_loss: 0.4566 - val_accuracy: 0.8742\n",
            "Epoch 32/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4811 - accuracy: 0.8742 - val_loss: 0.4543 - val_accuracy: 0.8742\n",
            "Epoch 33/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4741 - accuracy: 0.8742 - val_loss: 0.4524 - val_accuracy: 0.8742\n",
            "Epoch 34/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4754 - accuracy: 0.8753 - val_loss: 0.4504 - val_accuracy: 0.8742\n",
            "Epoch 35/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4638 - accuracy: 0.8758 - val_loss: 0.4530 - val_accuracy: 0.8742\n",
            "Epoch 36/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4736 - accuracy: 0.8730 - val_loss: 0.4543 - val_accuracy: 0.8742\n",
            "Epoch 37/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4766 - accuracy: 0.8697 - val_loss: 0.4529 - val_accuracy: 0.8742\n",
            "Epoch 38/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4810 - accuracy: 0.8713 - val_loss: 0.4527 - val_accuracy: 0.8742\n",
            "Epoch 39/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4695 - accuracy: 0.8730 - val_loss: 0.4540 - val_accuracy: 0.8742\n",
            "Epoch 40/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4596 - accuracy: 0.8736 - val_loss: 0.4512 - val_accuracy: 0.8742\n",
            "Epoch 41/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4683 - accuracy: 0.8725 - val_loss: 0.4536 - val_accuracy: 0.8764\n",
            "Epoch 42/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4648 - accuracy: 0.8742 - val_loss: 0.4518 - val_accuracy: 0.8764\n",
            "Epoch 43/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4656 - accuracy: 0.8781 - val_loss: 0.4521 - val_accuracy: 0.8764\n",
            "Epoch 44/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4602 - accuracy: 0.8753 - val_loss: 0.4526 - val_accuracy: 0.8742\n",
            "Epoch 45/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4631 - accuracy: 0.8753 - val_loss: 0.4537 - val_accuracy: 0.8764\n",
            "Epoch 46/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4672 - accuracy: 0.8730 - val_loss: 0.4559 - val_accuracy: 0.8764\n",
            "Epoch 47/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4609 - accuracy: 0.8747 - val_loss: 0.4562 - val_accuracy: 0.8764\n",
            "Epoch 48/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4638 - accuracy: 0.8770 - val_loss: 0.4561 - val_accuracy: 0.8764\n",
            "Epoch 49/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4597 - accuracy: 0.8736 - val_loss: 0.4527 - val_accuracy: 0.8764\n",
            "Epoch 50/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4607 - accuracy: 0.8742 - val_loss: 0.4568 - val_accuracy: 0.8764\n",
            "Epoch 51/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4652 - accuracy: 0.8736 - val_loss: 0.4559 - val_accuracy: 0.8764\n",
            "Epoch 52/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4648 - accuracy: 0.8725 - val_loss: 0.4543 - val_accuracy: 0.8764\n",
            "Epoch 53/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4441 - accuracy: 0.8781 - val_loss: 0.4560 - val_accuracy: 0.8764\n",
            "Epoch 54/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4463 - accuracy: 0.8775 - val_loss: 0.4591 - val_accuracy: 0.8764\n",
            "Epoch 55/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4553 - accuracy: 0.8747 - val_loss: 0.4552 - val_accuracy: 0.8764\n",
            "Epoch 56/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4569 - accuracy: 0.8770 - val_loss: 0.4612 - val_accuracy: 0.8697\n",
            "Epoch 57/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4517 - accuracy: 0.8747 - val_loss: 0.4540 - val_accuracy: 0.8764\n",
            "Epoch 58/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4517 - accuracy: 0.8719 - val_loss: 0.4505 - val_accuracy: 0.8764\n",
            "Epoch 59/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4590 - accuracy: 0.8770 - val_loss: 0.4489 - val_accuracy: 0.8764\n",
            "Epoch 60/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4488 - accuracy: 0.8775 - val_loss: 0.4514 - val_accuracy: 0.8764\n",
            "Epoch 61/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4522 - accuracy: 0.8747 - val_loss: 0.4543 - val_accuracy: 0.8764\n",
            "Epoch 62/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4559 - accuracy: 0.8736 - val_loss: 0.4525 - val_accuracy: 0.8719\n",
            "Epoch 63/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4506 - accuracy: 0.8787 - val_loss: 0.4561 - val_accuracy: 0.8719\n",
            "Epoch 64/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4438 - accuracy: 0.8775 - val_loss: 0.4528 - val_accuracy: 0.8674\n",
            "Epoch 65/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4463 - accuracy: 0.8809 - val_loss: 0.4522 - val_accuracy: 0.8719\n",
            "Epoch 66/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4513 - accuracy: 0.8758 - val_loss: 0.4567 - val_accuracy: 0.8652\n",
            "Epoch 67/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4447 - accuracy: 0.8775 - val_loss: 0.4580 - val_accuracy: 0.8652\n",
            "Epoch 68/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4405 - accuracy: 0.8815 - val_loss: 0.4583 - val_accuracy: 0.8629\n",
            "Epoch 69/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4442 - accuracy: 0.8792 - val_loss: 0.4504 - val_accuracy: 0.8697\n",
            "Epoch 70/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4524 - accuracy: 0.8753 - val_loss: 0.4558 - val_accuracy: 0.8652\n",
            "____________________________________________________________________________________________________\n",
            "FOLD NO 6 START\n",
            "Epoch 1/70\n",
            "56/56 [==============================] - 0s 4ms/step - loss: 1.2423 - accuracy: 0.4315 - val_loss: 1.0063 - val_accuracy: 0.7146\n",
            "Epoch 2/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.8537 - accuracy: 0.7152 - val_loss: 0.6552 - val_accuracy: 0.8112\n",
            "Epoch 3/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.7033 - accuracy: 0.7787 - val_loss: 0.5080 - val_accuracy: 0.8517\n",
            "Epoch 4/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6563 - accuracy: 0.8034 - val_loss: 0.4696 - val_accuracy: 0.8674\n",
            "Epoch 5/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6084 - accuracy: 0.8371 - val_loss: 0.4497 - val_accuracy: 0.8719\n",
            "Epoch 6/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6029 - accuracy: 0.8461 - val_loss: 0.4498 - val_accuracy: 0.8719\n",
            "Epoch 7/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5588 - accuracy: 0.8483 - val_loss: 0.4445 - val_accuracy: 0.8719\n",
            "Epoch 8/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5693 - accuracy: 0.8562 - val_loss: 0.4529 - val_accuracy: 0.8742\n",
            "Epoch 9/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5402 - accuracy: 0.8551 - val_loss: 0.4516 - val_accuracy: 0.8742\n",
            "Epoch 10/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5265 - accuracy: 0.8607 - val_loss: 0.4489 - val_accuracy: 0.8719\n",
            "Epoch 11/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5449 - accuracy: 0.8551 - val_loss: 0.4457 - val_accuracy: 0.8719\n",
            "Epoch 12/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5244 - accuracy: 0.8584 - val_loss: 0.4467 - val_accuracy: 0.8719\n",
            "Epoch 13/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5242 - accuracy: 0.8573 - val_loss: 0.4494 - val_accuracy: 0.8719\n",
            "Epoch 14/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5210 - accuracy: 0.8573 - val_loss: 0.4456 - val_accuracy: 0.8719\n",
            "Epoch 15/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5111 - accuracy: 0.8646 - val_loss: 0.4459 - val_accuracy: 0.8719\n",
            "Epoch 16/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5051 - accuracy: 0.8584 - val_loss: 0.4427 - val_accuracy: 0.8719\n",
            "Epoch 17/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4982 - accuracy: 0.8635 - val_loss: 0.4402 - val_accuracy: 0.8787\n",
            "Epoch 18/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5039 - accuracy: 0.8584 - val_loss: 0.4455 - val_accuracy: 0.8719\n",
            "Epoch 19/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4955 - accuracy: 0.8635 - val_loss: 0.4434 - val_accuracy: 0.8742\n",
            "Epoch 20/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5004 - accuracy: 0.8640 - val_loss: 0.4451 - val_accuracy: 0.8742\n",
            "Epoch 21/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4796 - accuracy: 0.8624 - val_loss: 0.4389 - val_accuracy: 0.8787\n",
            "Epoch 22/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4941 - accuracy: 0.8657 - val_loss: 0.4431 - val_accuracy: 0.8787\n",
            "Epoch 23/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4751 - accuracy: 0.8697 - val_loss: 0.4423 - val_accuracy: 0.8787\n",
            "Epoch 24/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4805 - accuracy: 0.8680 - val_loss: 0.4405 - val_accuracy: 0.8787\n",
            "Epoch 25/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4845 - accuracy: 0.8663 - val_loss: 0.4472 - val_accuracy: 0.8787\n",
            "Epoch 26/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4876 - accuracy: 0.8635 - val_loss: 0.4414 - val_accuracy: 0.8787\n",
            "Epoch 27/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4743 - accuracy: 0.8652 - val_loss: 0.4485 - val_accuracy: 0.8742\n",
            "Epoch 28/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4727 - accuracy: 0.8657 - val_loss: 0.4428 - val_accuracy: 0.8787\n",
            "Epoch 29/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4742 - accuracy: 0.8691 - val_loss: 0.4444 - val_accuracy: 0.8787\n",
            "Epoch 30/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4611 - accuracy: 0.8713 - val_loss: 0.4431 - val_accuracy: 0.8787\n",
            "Epoch 31/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4686 - accuracy: 0.8713 - val_loss: 0.4447 - val_accuracy: 0.8787\n",
            "Epoch 32/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4658 - accuracy: 0.8708 - val_loss: 0.4465 - val_accuracy: 0.8787\n",
            "Epoch 33/70\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4568 - accuracy: 0.8719 - val_loss: 0.4421 - val_accuracy: 0.8787\n",
            "Epoch 34/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4584 - accuracy: 0.8742 - val_loss: 0.4442 - val_accuracy: 0.8787\n",
            "Epoch 35/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4590 - accuracy: 0.8736 - val_loss: 0.4463 - val_accuracy: 0.8787\n",
            "Epoch 36/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4544 - accuracy: 0.8730 - val_loss: 0.4441 - val_accuracy: 0.8787\n",
            "Epoch 37/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4703 - accuracy: 0.8713 - val_loss: 0.4479 - val_accuracy: 0.8787\n",
            "Epoch 38/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4467 - accuracy: 0.8781 - val_loss: 0.4471 - val_accuracy: 0.8787\n",
            "Epoch 39/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4498 - accuracy: 0.8742 - val_loss: 0.4421 - val_accuracy: 0.8787\n",
            "Epoch 40/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4574 - accuracy: 0.8736 - val_loss: 0.4444 - val_accuracy: 0.8787\n",
            "Epoch 41/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4530 - accuracy: 0.8764 - val_loss: 0.4460 - val_accuracy: 0.8787\n",
            "Epoch 42/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4541 - accuracy: 0.8702 - val_loss: 0.4511 - val_accuracy: 0.8787\n",
            "Epoch 43/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4670 - accuracy: 0.8742 - val_loss: 0.4440 - val_accuracy: 0.8787\n",
            "Epoch 44/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4449 - accuracy: 0.8781 - val_loss: 0.4472 - val_accuracy: 0.8787\n",
            "Epoch 45/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4509 - accuracy: 0.8758 - val_loss: 0.4503 - val_accuracy: 0.8787\n",
            "Epoch 46/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4483 - accuracy: 0.8736 - val_loss: 0.4520 - val_accuracy: 0.8787\n",
            "Epoch 47/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4525 - accuracy: 0.8792 - val_loss: 0.4435 - val_accuracy: 0.8764\n",
            "Epoch 48/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4554 - accuracy: 0.8713 - val_loss: 0.4503 - val_accuracy: 0.8787\n",
            "Epoch 49/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4434 - accuracy: 0.8792 - val_loss: 0.4473 - val_accuracy: 0.8764\n",
            "Epoch 50/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4380 - accuracy: 0.8770 - val_loss: 0.4450 - val_accuracy: 0.8764\n",
            "Epoch 51/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4458 - accuracy: 0.8758 - val_loss: 0.4458 - val_accuracy: 0.8764\n",
            "Epoch 52/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4539 - accuracy: 0.8730 - val_loss: 0.4433 - val_accuracy: 0.8764\n",
            "Epoch 53/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4516 - accuracy: 0.8758 - val_loss: 0.4505 - val_accuracy: 0.8787\n",
            "Epoch 54/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4493 - accuracy: 0.8781 - val_loss: 0.4494 - val_accuracy: 0.8764\n",
            "Epoch 55/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4415 - accuracy: 0.8764 - val_loss: 0.4507 - val_accuracy: 0.8764\n",
            "Epoch 56/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4495 - accuracy: 0.8753 - val_loss: 0.4550 - val_accuracy: 0.8764\n",
            "Epoch 57/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4556 - accuracy: 0.8775 - val_loss: 0.4571 - val_accuracy: 0.8787\n",
            "Epoch 58/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4387 - accuracy: 0.8781 - val_loss: 0.4436 - val_accuracy: 0.8764\n",
            "Epoch 59/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4420 - accuracy: 0.8742 - val_loss: 0.4502 - val_accuracy: 0.8764\n",
            "Epoch 60/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4452 - accuracy: 0.8770 - val_loss: 0.4539 - val_accuracy: 0.8764\n",
            "Epoch 61/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4464 - accuracy: 0.8764 - val_loss: 0.4572 - val_accuracy: 0.8764\n",
            "Epoch 62/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4346 - accuracy: 0.8747 - val_loss: 0.4459 - val_accuracy: 0.8764\n",
            "Epoch 63/70\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4439 - accuracy: 0.8742 - val_loss: 0.4514 - val_accuracy: 0.8764\n",
            "Epoch 64/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4512 - accuracy: 0.8747 - val_loss: 0.4500 - val_accuracy: 0.8764\n",
            "Epoch 65/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4318 - accuracy: 0.8803 - val_loss: 0.4500 - val_accuracy: 0.8787\n",
            "Epoch 66/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4407 - accuracy: 0.8792 - val_loss: 0.4554 - val_accuracy: 0.8764\n",
            "Epoch 67/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4430 - accuracy: 0.8747 - val_loss: 0.4490 - val_accuracy: 0.8764\n",
            "Epoch 68/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4365 - accuracy: 0.8764 - val_loss: 0.4473 - val_accuracy: 0.8764\n",
            "Epoch 69/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4464 - accuracy: 0.8764 - val_loss: 0.4473 - val_accuracy: 0.8787\n",
            "Epoch 70/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4422 - accuracy: 0.8758 - val_loss: 0.4491 - val_accuracy: 0.8787\n",
            "____________________________________________________________________________________________________\n",
            "FOLD NO 7 START\n",
            "Epoch 1/70\n",
            "56/56 [==============================] - 0s 4ms/step - loss: 1.1081 - accuracy: 0.5938 - val_loss: 0.8869 - val_accuracy: 0.7399\n",
            "Epoch 2/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.8134 - accuracy: 0.7146 - val_loss: 0.6426 - val_accuracy: 0.8229\n",
            "Epoch 3/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.7136 - accuracy: 0.7893 - val_loss: 0.5403 - val_accuracy: 0.8543\n",
            "Epoch 4/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6600 - accuracy: 0.8084 - val_loss: 0.5006 - val_accuracy: 0.8744\n",
            "Epoch 5/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6154 - accuracy: 0.8247 - val_loss: 0.4823 - val_accuracy: 0.8812\n",
            "Epoch 6/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6052 - accuracy: 0.8320 - val_loss: 0.4749 - val_accuracy: 0.8812\n",
            "Epoch 7/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5864 - accuracy: 0.8388 - val_loss: 0.4642 - val_accuracy: 0.8767\n",
            "Epoch 8/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5634 - accuracy: 0.8449 - val_loss: 0.4619 - val_accuracy: 0.8767\n",
            "Epoch 9/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5506 - accuracy: 0.8522 - val_loss: 0.4642 - val_accuracy: 0.8767\n",
            "Epoch 10/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5546 - accuracy: 0.8506 - val_loss: 0.4624 - val_accuracy: 0.8767\n",
            "Epoch 11/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5541 - accuracy: 0.8472 - val_loss: 0.4627 - val_accuracy: 0.8767\n",
            "Epoch 12/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5427 - accuracy: 0.8545 - val_loss: 0.4647 - val_accuracy: 0.8767\n",
            "Epoch 13/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5276 - accuracy: 0.8556 - val_loss: 0.4653 - val_accuracy: 0.8767\n",
            "Epoch 14/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5331 - accuracy: 0.8506 - val_loss: 0.4670 - val_accuracy: 0.8767\n",
            "Epoch 15/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5171 - accuracy: 0.8545 - val_loss: 0.4639 - val_accuracy: 0.8767\n",
            "Epoch 16/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5271 - accuracy: 0.8573 - val_loss: 0.4607 - val_accuracy: 0.8767\n",
            "Epoch 17/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5250 - accuracy: 0.8556 - val_loss: 0.4680 - val_accuracy: 0.8767\n",
            "Epoch 18/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5095 - accuracy: 0.8612 - val_loss: 0.4621 - val_accuracy: 0.8744\n",
            "Epoch 19/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5054 - accuracy: 0.8618 - val_loss: 0.4647 - val_accuracy: 0.8744\n",
            "Epoch 20/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5226 - accuracy: 0.8522 - val_loss: 0.4666 - val_accuracy: 0.8744\n",
            "Epoch 21/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4905 - accuracy: 0.8590 - val_loss: 0.4701 - val_accuracy: 0.8744\n",
            "Epoch 22/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5027 - accuracy: 0.8596 - val_loss: 0.4660 - val_accuracy: 0.8744\n",
            "Epoch 23/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4880 - accuracy: 0.8624 - val_loss: 0.4672 - val_accuracy: 0.8744\n",
            "Epoch 24/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4967 - accuracy: 0.8556 - val_loss: 0.4660 - val_accuracy: 0.8744\n",
            "Epoch 25/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4957 - accuracy: 0.8618 - val_loss: 0.4664 - val_accuracy: 0.8744\n",
            "Epoch 26/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4855 - accuracy: 0.8646 - val_loss: 0.4660 - val_accuracy: 0.8744\n",
            "Epoch 27/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4896 - accuracy: 0.8596 - val_loss: 0.4628 - val_accuracy: 0.8767\n",
            "Epoch 28/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4912 - accuracy: 0.8646 - val_loss: 0.4659 - val_accuracy: 0.8767\n",
            "Epoch 29/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5040 - accuracy: 0.8652 - val_loss: 0.4690 - val_accuracy: 0.8767\n",
            "Epoch 30/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4815 - accuracy: 0.8607 - val_loss: 0.4652 - val_accuracy: 0.8767\n",
            "Epoch 31/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4767 - accuracy: 0.8669 - val_loss: 0.4633 - val_accuracy: 0.8767\n",
            "Epoch 32/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4924 - accuracy: 0.8629 - val_loss: 0.4650 - val_accuracy: 0.8767\n",
            "Epoch 33/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4815 - accuracy: 0.8640 - val_loss: 0.4667 - val_accuracy: 0.8767\n",
            "Epoch 34/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4844 - accuracy: 0.8646 - val_loss: 0.4637 - val_accuracy: 0.8767\n",
            "Epoch 35/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4830 - accuracy: 0.8624 - val_loss: 0.4651 - val_accuracy: 0.8767\n",
            "Epoch 36/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4739 - accuracy: 0.8657 - val_loss: 0.4660 - val_accuracy: 0.8767\n",
            "Epoch 37/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4782 - accuracy: 0.8635 - val_loss: 0.4650 - val_accuracy: 0.8767\n",
            "Epoch 38/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4735 - accuracy: 0.8640 - val_loss: 0.4675 - val_accuracy: 0.8767\n",
            "Epoch 39/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4707 - accuracy: 0.8702 - val_loss: 0.4649 - val_accuracy: 0.8767\n",
            "Epoch 40/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4933 - accuracy: 0.8680 - val_loss: 0.4684 - val_accuracy: 0.8789\n",
            "Epoch 41/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4770 - accuracy: 0.8635 - val_loss: 0.4672 - val_accuracy: 0.8767\n",
            "Epoch 42/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4829 - accuracy: 0.8640 - val_loss: 0.4659 - val_accuracy: 0.8767\n",
            "Epoch 43/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4816 - accuracy: 0.8646 - val_loss: 0.4651 - val_accuracy: 0.8767\n",
            "Epoch 44/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4676 - accuracy: 0.8708 - val_loss: 0.4664 - val_accuracy: 0.8789\n",
            "Epoch 45/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4808 - accuracy: 0.8612 - val_loss: 0.4639 - val_accuracy: 0.8767\n",
            "Epoch 46/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4902 - accuracy: 0.8624 - val_loss: 0.4637 - val_accuracy: 0.8789\n",
            "Epoch 47/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4799 - accuracy: 0.8646 - val_loss: 0.4672 - val_accuracy: 0.8767\n",
            "Epoch 48/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4834 - accuracy: 0.8657 - val_loss: 0.4671 - val_accuracy: 0.8767\n",
            "Epoch 49/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4732 - accuracy: 0.8669 - val_loss: 0.4634 - val_accuracy: 0.8767\n",
            "Epoch 50/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4536 - accuracy: 0.8697 - val_loss: 0.4580 - val_accuracy: 0.8767\n",
            "Epoch 51/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4703 - accuracy: 0.8680 - val_loss: 0.4576 - val_accuracy: 0.8789\n",
            "Epoch 52/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4707 - accuracy: 0.8652 - val_loss: 0.4599 - val_accuracy: 0.8789\n",
            "Epoch 53/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4694 - accuracy: 0.8635 - val_loss: 0.4630 - val_accuracy: 0.8789\n",
            "Epoch 54/70\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4576 - accuracy: 0.8652 - val_loss: 0.4633 - val_accuracy: 0.8789\n",
            "Epoch 55/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4828 - accuracy: 0.8646 - val_loss: 0.4604 - val_accuracy: 0.8789\n",
            "Epoch 56/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4741 - accuracy: 0.8663 - val_loss: 0.4573 - val_accuracy: 0.8812\n",
            "Epoch 57/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4800 - accuracy: 0.8674 - val_loss: 0.4630 - val_accuracy: 0.8789\n",
            "Epoch 58/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4690 - accuracy: 0.8663 - val_loss: 0.4609 - val_accuracy: 0.8789\n",
            "Epoch 59/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4774 - accuracy: 0.8640 - val_loss: 0.4618 - val_accuracy: 0.8789\n",
            "Epoch 60/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4550 - accuracy: 0.8669 - val_loss: 0.4601 - val_accuracy: 0.8789\n",
            "Epoch 61/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4671 - accuracy: 0.8669 - val_loss: 0.4621 - val_accuracy: 0.8789\n",
            "Epoch 62/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4718 - accuracy: 0.8691 - val_loss: 0.4584 - val_accuracy: 0.8789\n",
            "Epoch 63/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4672 - accuracy: 0.8635 - val_loss: 0.4580 - val_accuracy: 0.8789\n",
            "Epoch 64/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4595 - accuracy: 0.8674 - val_loss: 0.4589 - val_accuracy: 0.8789\n",
            "Epoch 65/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4768 - accuracy: 0.8691 - val_loss: 0.4602 - val_accuracy: 0.8789\n",
            "Epoch 66/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4699 - accuracy: 0.8652 - val_loss: 0.4593 - val_accuracy: 0.8789\n",
            "Epoch 67/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4597 - accuracy: 0.8736 - val_loss: 0.4611 - val_accuracy: 0.8789\n",
            "Epoch 68/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4624 - accuracy: 0.8652 - val_loss: 0.4624 - val_accuracy: 0.8789\n",
            "Epoch 69/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4660 - accuracy: 0.8685 - val_loss: 0.4612 - val_accuracy: 0.8812\n",
            "Epoch 70/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4674 - accuracy: 0.8663 - val_loss: 0.4618 - val_accuracy: 0.8812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZF1RHlg1v3X",
        "colab_type": "text"
      },
      "source": [
        "### Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh3V3QdWu0VG",
        "colab_type": "code",
        "outputId": "740ccc3d-dd4a-495e-98fc-49214c43eba6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# == Provide average scores ==\n",
        "print('Score per fold:')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score per fold:\n",
            "> Fold 1 - Loss: 0.5186271071434021 - Accuracy: 85.44474244117737%\n",
            "> Fold 2 - Loss: 0.5082710981369019 - Accuracy: 85.98382472991943%\n",
            "> Fold 3 - Loss: 0.41896623373031616 - Accuracy: 88.40970396995544%\n",
            "> Fold 4 - Loss: 0.5098980069160461 - Accuracy: 86.52291297912598%\n",
            "> Fold 5 - Loss: 0.4092913866043091 - Accuracy: 87.87062168121338%\n",
            "> Fold 6 - Loss: 0.39911746978759766 - Accuracy: 88.94878625869751%\n",
            "> Fold 7 - Loss: 0.3293522894382477 - Accuracy: 92.43243336677551%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 87.94471791812352 (+- 2.1871688884678027)\n",
            "> Loss: 0.4419319416795458\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQDKmJQ_kmdh",
        "colab_type": "text"
      },
      "source": [
        "## Plot with train and test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mi0f_JObgC4C",
        "colab_type": "code",
        "outputId": "66bdb7cb-dc6f-4e59-ceb8-01ca6cc41a4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylim(0,1)\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "96821b45-0e1e-48a0-b450-6fd5b5a62edc",
        "id": "YQosphMa9Zab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('training loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7jx7N_ZwHgm",
        "colab_type": "text"
      },
      "source": [
        "Confusion matrix "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWH11sjmwJ4P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "dd393974-7b9c-4ac5-debf-8402b7ee5891"
      },
      "source": [
        "X_conf_matrix = X.iloc[test]\n",
        "y_conf_matrix = y.iloc[test]\n",
        "predicted_conf_matrix = model.predict_classes(X_conf_matrix)\n",
        "\n",
        "conf_matrix = confusion_matrix(y_conf_matrix.values.argmax(axis=1), predicted_conf_matrix)\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fcfa67abc50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwVgSUc6uC6Y",
        "colab_type": "text"
      },
      "source": [
        "### Save model \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2XZ1N0JuIop",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('featureModelPl.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ey04c2ykpMf6",
        "colab_type": "text"
      },
      "source": [
        "## Check RANDOM FOREST Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkLf7YkgpiMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLVy_y00pm_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random_forest = RandomForestClassifier(\n",
        "    n_estimators = 2000, \n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psCPtOdbpPzU",
        "colab_type": "code",
        "outputId": "9051c682-bfc4-45fc-f605-b1eb2cae0cc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        " X_train, X_test, y_train, y_test = train_test_split(X, ml_classifier_labels, test_size=0.33, random_state=42)\n",
        "random_forest.fit(X_train, y_train)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=2000,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7eal1FbzzLu",
        "colab_type": "code",
        "outputId": "ede06140-8d3a-469c-8023-fc030bc9451c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "predict = random_forest.predict(X_test)\n",
        "print(predict.shape)\n",
        "print(np.unique(predict))\n",
        "print(y_test.shape)\n",
        "print(np.unique(y_test, return_counts = True))\n",
        "conf_matrix = confusion_matrix(y_test, predict)\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(857,)\n",
            "['C' 'E' 'N']\n",
            "(857,)\n",
            "(array(['?', 'C', 'E', 'N'], dtype=object), array([ 30,  21, 240, 566]))\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fcfae963f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LO2MtVpmqexD",
        "colab_type": "code",
        "outputId": "4a05da55-7915-480f-e2f5-b98e3ec37a88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "random_forest.score(X_test, y_test)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8833138856476079"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    }
  ]
}