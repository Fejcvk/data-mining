{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SecondModelPolishDataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i61b34AJ1zPY",
        "colab_type": "text"
      },
      "source": [
        "# Second model - Polish Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4R8arNJb14eM",
        "colab_type": "text"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLg6EMKN2D0x",
        "colab_type": "code",
        "outputId": "9e568798-41f2-4611-ce89-73c834756120",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        }
      },
      "source": [
        "!pip install transformers\n",
        "!pip install -q pyyaml h5py\n",
        "import pandas as pd\n",
        "from transformers import *\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers) (0.1.86)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.41)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.38.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.12.46)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.4.5.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.16.0,>=1.15.46 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.15.46)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.5)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.46->boto3->transformers) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.46->boto3->transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qr2zJmEf2Eh3",
        "colab_type": "text"
      },
      "source": [
        "## Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLXojV3K5Y1U",
        "colab_type": "text"
      },
      "source": [
        "*   Reading data\n",
        "*   Change columns names\n",
        "*   Drop NaN rows\n",
        "*   Fill others NaN values by special sign"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j--zuCZf2HvJ",
        "colab_type": "code",
        "outputId": "3d8f976d-7959-4f6b-94b5-bda1f78054a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "df = pd.read_excel('polishOriginalDataset.xlsx')\n",
        "df.reset_index()\n",
        "df = df.iloc[:,[6,8,9,10,15,16,19,21,5]]\n",
        "df.columns = [\n",
        "              \"type_of_sentence\",\n",
        "              \"verb_main_semantic_class\",\n",
        "              \"verb_second_semantic_class\",\n",
        "              \"verb_third_semantic_class\",\n",
        "              \"verb_veridical_positive\",\n",
        "              \"verb_veridical_negative\",\n",
        "              \"verb_tense\",\n",
        "              \"t_negation\",\n",
        "              \"semantic_relation\"\n",
        "              ]\n",
        "df.dropna(inplace=True, axis = 0, how = 'all')\n",
        "df.fillna(axis = 0, inplace =True, value=\"none\")\n",
        "df.head()"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type_of_sentence</th>\n",
              "      <th>verb_main_semantic_class</th>\n",
              "      <th>verb_second_semantic_class</th>\n",
              "      <th>verb_third_semantic_class</th>\n",
              "      <th>verb_veridical_positive</th>\n",
              "      <th>verb_veridical_negative</th>\n",
              "      <th>verb_tense</th>\n",
              "      <th>t_negation</th>\n",
              "      <th>semantic_relation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>eliptyczne</td>\n",
              "      <td>mówienia</td>\n",
              "      <td>none</td>\n",
              "      <td>none</td>\n",
              "      <td>o?</td>\n",
              "      <td>?</td>\n",
              "      <td>brak</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>eliptyczne</td>\n",
              "      <td>epistemiczny</td>\n",
              "      <td>none</td>\n",
              "      <td>none</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>past</td>\n",
              "      <td>0</td>\n",
              "      <td>?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>eliptyczne</td>\n",
              "      <td>mówienia</td>\n",
              "      <td>none</td>\n",
              "      <td>none</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>past</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>epistemiczny</td>\n",
              "      <td>percepcyjny</td>\n",
              "      <td>none</td>\n",
              "      <td>\"+\"</td>\n",
              "      <td>\"+\"</td>\n",
              "      <td>past</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>epistemiczny</td>\n",
              "      <td>percepcyjny</td>\n",
              "      <td>none</td>\n",
              "      <td>o</td>\n",
              "      <td>o</td>\n",
              "      <td>present</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  type_of_sentence verb_main_semantic_class  ... t_negation semantic_relation\n",
              "0       eliptyczne                 mówienia  ...          0                 N\n",
              "1       eliptyczne             epistemiczny  ...          0                 ?\n",
              "2       eliptyczne                 mówienia  ...          0                 N\n",
              "3                1             epistemiczny  ...          0                 N\n",
              "4                1             epistemiczny  ...          0                 N\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTXvttwo5v0D",
        "colab_type": "text"
      },
      "source": [
        "### Cleaning data by deleting uncertainty - simplification "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewr4Z-ZH8e6Q",
        "colab_type": "code",
        "outputId": "4fca48c4-e910-4cfc-f9b5-1e48ff2f11f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "# df.type_of_sentence.unique() cleaning not needed \n",
        "\n",
        "# df.verb_main_semantic_class - only (epistemiczny, mówienia, ?)\n",
        "main_semantic_class_unique = df.verb_main_semantic_class.unique()\n",
        "main_semantic_class_unique = main_semantic_class_unique[main_semantic_class_unique != \"epistemiczny\"]\n",
        "main_semantic_class_unique = main_semantic_class_unique[main_semantic_class_unique != \"mówienia\"]\n",
        "df.verb_main_semantic_class = df.verb_main_semantic_class.apply(lambda x: '?' if x in main_semantic_class_unique else x )\n",
        "\n",
        "# df.verb_second_semantic_class.unique() cleaning not needed \n",
        "# df.verb_third_semantic_class.unique() cleaning not needed\n",
        "\n",
        "# verb veridical positive cleaning\n",
        "df.verb_veridical_positive = df.verb_veridical_positive.apply(lambda x: '+' if '+' in x else x)\n",
        "df.verb_veridical_positive = df.verb_veridical_positive.apply(lambda x: '-' if '-' in x else x)\n",
        "df.verb_veridical_positive = df.verb_veridical_positive.apply(lambda x: 'o' if 'o' in x else x)\n",
        "df.verb_veridical_positive = df.verb_veridical_positive.apply(lambda x: '?' if '?' in x else x)\n",
        "\n",
        "# verb veridical negative cleaning\n",
        "df.verb_veridical_negative = df.verb_veridical_negative.apply(lambda x: '+' if '+' in x else x)\n",
        "df.verb_veridical_negative = df.verb_veridical_negative.apply(lambda x: 'o' if 'o' in x else x)\n",
        "df.verb_veridical_negative = df.verb_veridical_negative.apply(lambda x: '-' if '-' in x else x)\n",
        "df.verb_veridical_negative = df.verb_veridical_negative.apply(lambda x: '?' if '?' in x else x)\n",
        "\n",
        "# df.verb_tense.unique() cleaning not needed\n",
        "# df.t_negation.unique() cleaning not needed\n",
        "# df.semantic_relation.unique() cleaning not needed \n",
        "\n",
        "df.head()"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>type_of_sentence</th>\n",
              "      <th>verb_main_semantic_class</th>\n",
              "      <th>verb_second_semantic_class</th>\n",
              "      <th>verb_third_semantic_class</th>\n",
              "      <th>verb_veridical_positive</th>\n",
              "      <th>verb_veridical_negative</th>\n",
              "      <th>verb_tense</th>\n",
              "      <th>t_negation</th>\n",
              "      <th>semantic_relation</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>eliptyczne</td>\n",
              "      <td>mówienia</td>\n",
              "      <td>none</td>\n",
              "      <td>none</td>\n",
              "      <td>o</td>\n",
              "      <td>?</td>\n",
              "      <td>brak</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>eliptyczne</td>\n",
              "      <td>epistemiczny</td>\n",
              "      <td>none</td>\n",
              "      <td>none</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>past</td>\n",
              "      <td>0</td>\n",
              "      <td>?</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>eliptyczne</td>\n",
              "      <td>mówienia</td>\n",
              "      <td>none</td>\n",
              "      <td>none</td>\n",
              "      <td>?</td>\n",
              "      <td>?</td>\n",
              "      <td>past</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>epistemiczny</td>\n",
              "      <td>percepcyjny</td>\n",
              "      <td>none</td>\n",
              "      <td>+</td>\n",
              "      <td>+</td>\n",
              "      <td>past</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>epistemiczny</td>\n",
              "      <td>percepcyjny</td>\n",
              "      <td>none</td>\n",
              "      <td>o</td>\n",
              "      <td>o</td>\n",
              "      <td>present</td>\n",
              "      <td>0</td>\n",
              "      <td>N</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  type_of_sentence verb_main_semantic_class  ... t_negation semantic_relation\n",
              "0       eliptyczne                 mówienia  ...          0                 N\n",
              "1       eliptyczne             epistemiczny  ...          0                 ?\n",
              "2       eliptyczne                 mówienia  ...          0                 N\n",
              "3                1             epistemiczny  ...          0                 N\n",
              "4                1             epistemiczny  ...          0                 N\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq5Zke-TO3zd",
        "colab_type": "text"
      },
      "source": [
        "### Possible feature values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVbjgIUMOXXi",
        "colab_type": "code",
        "outputId": "97dac4e4-25f1-44d4-967c-ed9ca454e14d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 324
        }
      },
      "source": [
        "print(df.type_of_sentence.unique())\n",
        "print(df.verb_main_semantic_class.unique())\n",
        "print(df.verb_second_semantic_class.unique())\n",
        "print(df.verb_third_semantic_class.unique())\n",
        "print(df.verb_veridical_positive.unique())\n",
        "print(df.verb_veridical_negative.unique())\n",
        "print(df.verb_tense.unique())\n",
        "print(df.t_negation.unique())\n"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['eliptyczne' 1 'generalne' 'pytajne' 'modalne'\n",
            " 'powtarzające się zdarzenie' 'warunkowe' 'powinnościowe'\n",
            " 'imperatyw; warunkowe' 'none' '?' 'imperatyw ' 'modal'\n",
            " 'warunkowe; pytajne' 'wolitywne' 'performatyw' 'kontrfaktyczne'\n",
            " 'imperatyw' 'prostujące' 'korygujące' 'performatyw; warunkowe'\n",
            " 'warunkowe; modalne' 'przypuszczające' 'wolitywne; performatyw'\n",
            " 'modalne; pytajne' 'imperatyw; generalne' 'sprostowanie' 'dyspozycyjne'\n",
            " 'modalne; warunkowe' 'pytajne; modalne' 'modalne; alternatywa'\n",
            " 'alternatywa' 'warunkowe; generalne']\n",
            "['mówienia' 'epistemiczny' '?']\n",
            "['none' 'percepcyjny' 'epistemiczny' 'emotywny' 'wolicjonalny' 'mówienia'\n",
            " 'wnioskowania' 'nie-wiedzowy' 'performatyw' 'pamięciowy' 'zdarzeniowy'\n",
            " 'przyczynowy' 'wolitywny' 'czynnościowy']\n",
            "['none' 'mówienia' 'wolicjonalny' 'epistemiczny' 'percepcyjny']\n",
            "['o' '?' '+' '-']\n",
            "['?' '+' 'o' '-']\n",
            "['brak' 'past' 'present' 'future']\n",
            "[0 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEzZDtqp9X1i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "95490aa1-3407-4066-d394-f213a054aac5"
      },
      "source": [
        "df.verb_main_semantic_class.value_counts()"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "epistemiczny    1079\n",
              "mówienia         973\n",
              "?                544\n",
              "Name: verb_main_semantic_class, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0yf4jWrgB4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# choose columns \n",
        "df = df[[\n",
        "  \"verb_main_semantic_class\",\n",
        "  \"verb_veridical_positive\",\n",
        "  \"verb_veridical_negative\",\n",
        "  \"verb_tense\",\n",
        "  \"semantic_relation\"      \n",
        "]]\n",
        "df.to_csv(\"plData.csv\", index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOi5V_fDAJBS",
        "colab_type": "text"
      },
      "source": [
        "### Vactorize data and split to features and target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YokPfvoJxWoR",
        "colab_type": "text"
      },
      "source": [
        "#### Vectorize (one =hot encoding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r7NRdcwkIX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.get_dummies(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9f1MRWJxhBA",
        "colab_type": "text"
      },
      "source": [
        "#### Split to features and target"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp6eOyEdxlMO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = df.iloc[:,0:-4]\n",
        "y = df.iloc[:,-4:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjQvT5PtxsFz",
        "colab_type": "text"
      },
      "source": [
        "#### Features columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLqFLidq0YLV",
        "colab_type": "code",
        "outputId": "2ab36d9d-78e1-4b2a-8bf9-861409b2b093",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "X.columns"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['verb_main_semantic_class_?', 'verb_main_semantic_class_epistemiczny',\n",
              "       'verb_main_semantic_class_mówienia', 'verb_veridical_positive_+',\n",
              "       'verb_veridical_positive_-', 'verb_veridical_positive_?',\n",
              "       'verb_veridical_positive_o', 'verb_veridical_negative_+',\n",
              "       'verb_veridical_negative_-', 'verb_veridical_negative_?',\n",
              "       'verb_veridical_negative_o', 'verb_tense_brak', 'verb_tense_future',\n",
              "       'verb_tense_past', 'verb_tense_present'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4896Jynxyew",
        "colab_type": "text"
      },
      "source": [
        "#### Target columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxEJ6K-fxkTU",
        "colab_type": "code",
        "outputId": "93fcf256-63a4-48a7-a44c-ef3e03d41119",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "y.columns"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['semantic_relation_?', 'semantic_relation_C', 'semantic_relation_E',\n",
              "       'semantic_relation_N'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jmoU__ZsXtC",
        "colab_type": "text"
      },
      "source": [
        "### k-fold crossvalidation preparing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QUp3OdBNsdn5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k=7\n",
        "from sklearn.model_selection import KFold\n",
        "kfold = KFold(n_splits = k, shuffle=True)\n",
        "\n",
        "acc_per_fold = []\n",
        "loss_per_fold = [] "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Xi8QfyAkbiN",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "# Keras model building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ln4inIDkB_h5",
        "colab_type": "text"
      },
      "source": [
        "### Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amhvtjnZQwhz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf \n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H15DIedMCgOC",
        "colab_type": "text"
      },
      "source": [
        "It takes only 1-2 minutes to train this model with 7-crossvalidation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7ZB5xOvAZbu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5412adf5-ee88-440d-9355-2c4ff8b46161"
      },
      "source": [
        "fold_no = 1 \n",
        "\n",
        "#get number of columns in training data\n",
        "n_cols = X.shape[1]\n",
        "print(n_cols)\n",
        "\n",
        "for train, test in kfold.split(X,y):\n",
        "  # FOLD PRINTOUT\n",
        "  print(100*'_')\n",
        "  print (f\"FOLD NO {fold_no} START\")  \n",
        "\n",
        "  # model architecture  \n",
        "  model = tf.keras.Sequential()\n",
        "  model.add(Dense(16, activation='relu', input_shape=(n_cols,)))\n",
        "  model.add(Dense(32, activation='relu'))\n",
        "  model.add(Dense(16, activation='relu'))\n",
        "  model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "  # model compile \n",
        "  model.compile(optimizer = 'adam', loss=\"categorical_crossentropy\", metrics = ['accuracy'])\n",
        "  \n",
        "  # training\n",
        "  history = model.fit(X.iloc[train], y.iloc[train], validation_split=0.2, epochs=70)\n",
        "\n",
        "  # scores \n",
        "  scores = model.evaluate(X.iloc[test], y.iloc[test], verbose=0)\n",
        "  acc_per_fold.append(scores[1] * 100)\n",
        "  loss_per_fold.append(scores[0])\n",
        "\n",
        "  # iterator up\n",
        "  fold_no = fold_no + 1"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "15\n",
            "____________________________________________________________________________________________________\n",
            "FOLD NO 1 START\n",
            "Epoch 1/70\n",
            "56/56 [==============================] - 0s 4ms/step - loss: 1.2040 - accuracy: 0.6011 - val_loss: 0.9537 - val_accuracy: 0.8539\n",
            "Epoch 2/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6816 - accuracy: 0.8646 - val_loss: 0.5506 - val_accuracy: 0.8629\n",
            "Epoch 3/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5217 - accuracy: 0.8691 - val_loss: 0.4938 - val_accuracy: 0.8629\n",
            "Epoch 4/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.8691 - val_loss: 0.4805 - val_accuracy: 0.8629\n",
            "Epoch 5/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4665 - accuracy: 0.8713 - val_loss: 0.4716 - val_accuracy: 0.8629\n",
            "Epoch 6/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4532 - accuracy: 0.8736 - val_loss: 0.4647 - val_accuracy: 0.8607\n",
            "Epoch 7/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4411 - accuracy: 0.8747 - val_loss: 0.4588 - val_accuracy: 0.8607\n",
            "Epoch 8/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4336 - accuracy: 0.8792 - val_loss: 0.4574 - val_accuracy: 0.8652\n",
            "Epoch 9/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4249 - accuracy: 0.8820 - val_loss: 0.4527 - val_accuracy: 0.8629\n",
            "Epoch 10/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4195 - accuracy: 0.8831 - val_loss: 0.4481 - val_accuracy: 0.8629\n",
            "Epoch 11/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4186 - accuracy: 0.8815 - val_loss: 0.4507 - val_accuracy: 0.8629\n",
            "Epoch 12/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4115 - accuracy: 0.8837 - val_loss: 0.4522 - val_accuracy: 0.8607\n",
            "Epoch 13/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4127 - accuracy: 0.8843 - val_loss: 0.4562 - val_accuracy: 0.8607\n",
            "Epoch 14/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4079 - accuracy: 0.8843 - val_loss: 0.4492 - val_accuracy: 0.8629\n",
            "Epoch 15/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4104 - accuracy: 0.8837 - val_loss: 0.4486 - val_accuracy: 0.8607\n",
            "Epoch 16/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4074 - accuracy: 0.8843 - val_loss: 0.4531 - val_accuracy: 0.8607\n",
            "Epoch 17/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4059 - accuracy: 0.8837 - val_loss: 0.4557 - val_accuracy: 0.8607\n",
            "Epoch 18/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4032 - accuracy: 0.8843 - val_loss: 0.4502 - val_accuracy: 0.8629\n",
            "Epoch 19/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4044 - accuracy: 0.8843 - val_loss: 0.4530 - val_accuracy: 0.8652\n",
            "Epoch 20/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4040 - accuracy: 0.8843 - val_loss: 0.4539 - val_accuracy: 0.8629\n",
            "Epoch 21/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4008 - accuracy: 0.8843 - val_loss: 0.4541 - val_accuracy: 0.8629\n",
            "Epoch 22/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4039 - accuracy: 0.8837 - val_loss: 0.4538 - val_accuracy: 0.8629\n",
            "Epoch 23/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4001 - accuracy: 0.8843 - val_loss: 0.4540 - val_accuracy: 0.8629\n",
            "Epoch 24/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4011 - accuracy: 0.8848 - val_loss: 0.4568 - val_accuracy: 0.8629\n",
            "Epoch 25/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3978 - accuracy: 0.8848 - val_loss: 0.4675 - val_accuracy: 0.8607\n",
            "Epoch 26/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4016 - accuracy: 0.8837 - val_loss: 0.4553 - val_accuracy: 0.8629\n",
            "Epoch 27/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3996 - accuracy: 0.8843 - val_loss: 0.4554 - val_accuracy: 0.8629\n",
            "Epoch 28/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4014 - accuracy: 0.8848 - val_loss: 0.4556 - val_accuracy: 0.8629\n",
            "Epoch 29/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3986 - accuracy: 0.8843 - val_loss: 0.4674 - val_accuracy: 0.8607\n",
            "Epoch 30/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3973 - accuracy: 0.8843 - val_loss: 0.4766 - val_accuracy: 0.8629\n",
            "Epoch 31/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4056 - accuracy: 0.8837 - val_loss: 0.4557 - val_accuracy: 0.8629\n",
            "Epoch 32/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3989 - accuracy: 0.8854 - val_loss: 0.4599 - val_accuracy: 0.8629\n",
            "Epoch 33/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3992 - accuracy: 0.8848 - val_loss: 0.4579 - val_accuracy: 0.8629\n",
            "Epoch 34/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4003 - accuracy: 0.8854 - val_loss: 0.4566 - val_accuracy: 0.8629\n",
            "Epoch 35/70\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.3961 - accuracy: 0.8848 - val_loss: 0.4707 - val_accuracy: 0.8629\n",
            "Epoch 36/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4042 - accuracy: 0.8854 - val_loss: 0.4591 - val_accuracy: 0.8629\n",
            "Epoch 37/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3985 - accuracy: 0.8854 - val_loss: 0.4550 - val_accuracy: 0.8629\n",
            "Epoch 38/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3961 - accuracy: 0.8848 - val_loss: 0.4604 - val_accuracy: 0.8629\n",
            "Epoch 39/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3988 - accuracy: 0.8848 - val_loss: 0.4615 - val_accuracy: 0.8629\n",
            "Epoch 40/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3980 - accuracy: 0.8854 - val_loss: 0.4667 - val_accuracy: 0.8629\n",
            "Epoch 41/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3988 - accuracy: 0.8860 - val_loss: 0.4571 - val_accuracy: 0.8629\n",
            "Epoch 42/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3977 - accuracy: 0.8848 - val_loss: 0.4641 - val_accuracy: 0.8629\n",
            "Epoch 43/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3956 - accuracy: 0.8860 - val_loss: 0.4636 - val_accuracy: 0.8607\n",
            "Epoch 44/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3958 - accuracy: 0.8854 - val_loss: 0.4589 - val_accuracy: 0.8629\n",
            "Epoch 45/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3954 - accuracy: 0.8860 - val_loss: 0.4688 - val_accuracy: 0.8629\n",
            "Epoch 46/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3957 - accuracy: 0.8860 - val_loss: 0.4627 - val_accuracy: 0.8629\n",
            "Epoch 47/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3932 - accuracy: 0.8854 - val_loss: 0.4643 - val_accuracy: 0.8629\n",
            "Epoch 48/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3944 - accuracy: 0.8860 - val_loss: 0.4614 - val_accuracy: 0.8629\n",
            "Epoch 49/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3953 - accuracy: 0.8848 - val_loss: 0.4581 - val_accuracy: 0.8629\n",
            "Epoch 50/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3969 - accuracy: 0.8860 - val_loss: 0.4635 - val_accuracy: 0.8629\n",
            "Epoch 51/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3940 - accuracy: 0.8843 - val_loss: 0.4590 - val_accuracy: 0.8629\n",
            "Epoch 52/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3957 - accuracy: 0.8854 - val_loss: 0.4627 - val_accuracy: 0.8629\n",
            "Epoch 53/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3972 - accuracy: 0.8854 - val_loss: 0.4612 - val_accuracy: 0.8629\n",
            "Epoch 54/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3958 - accuracy: 0.8854 - val_loss: 0.4578 - val_accuracy: 0.8629\n",
            "Epoch 55/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3934 - accuracy: 0.8860 - val_loss: 0.4616 - val_accuracy: 0.8629\n",
            "Epoch 56/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3962 - accuracy: 0.8860 - val_loss: 0.4613 - val_accuracy: 0.8629\n",
            "Epoch 57/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3938 - accuracy: 0.8860 - val_loss: 0.4616 - val_accuracy: 0.8629\n",
            "Epoch 58/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3924 - accuracy: 0.8854 - val_loss: 0.4626 - val_accuracy: 0.8629\n",
            "Epoch 59/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3958 - accuracy: 0.8865 - val_loss: 0.4583 - val_accuracy: 0.8629\n",
            "Epoch 60/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3907 - accuracy: 0.8860 - val_loss: 0.4669 - val_accuracy: 0.8629\n",
            "Epoch 61/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3922 - accuracy: 0.8854 - val_loss: 0.4794 - val_accuracy: 0.8652\n",
            "Epoch 62/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3962 - accuracy: 0.8843 - val_loss: 0.4619 - val_accuracy: 0.8629\n",
            "Epoch 63/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3922 - accuracy: 0.8854 - val_loss: 0.4586 - val_accuracy: 0.8629\n",
            "Epoch 64/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3927 - accuracy: 0.8848 - val_loss: 0.4727 - val_accuracy: 0.8652\n",
            "Epoch 65/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3907 - accuracy: 0.8865 - val_loss: 0.4703 - val_accuracy: 0.8652\n",
            "Epoch 66/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3925 - accuracy: 0.8860 - val_loss: 0.4627 - val_accuracy: 0.8629\n",
            "Epoch 67/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3909 - accuracy: 0.8860 - val_loss: 0.4654 - val_accuracy: 0.8652\n",
            "Epoch 68/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3935 - accuracy: 0.8860 - val_loss: 0.4682 - val_accuracy: 0.8629\n",
            "Epoch 69/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3930 - accuracy: 0.8876 - val_loss: 0.4665 - val_accuracy: 0.8652\n",
            "Epoch 70/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3901 - accuracy: 0.8860 - val_loss: 0.4630 - val_accuracy: 0.8629\n",
            "____________________________________________________________________________________________________\n",
            "FOLD NO 2 START\n",
            "Epoch 1/70\n",
            "56/56 [==============================] - 0s 4ms/step - loss: 0.9805 - accuracy: 0.6612 - val_loss: 0.7561 - val_accuracy: 0.7303\n",
            "Epoch 2/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5811 - accuracy: 0.8376 - val_loss: 0.4935 - val_accuracy: 0.8764\n",
            "Epoch 3/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4982 - accuracy: 0.8663 - val_loss: 0.4737 - val_accuracy: 0.8764\n",
            "Epoch 4/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4837 - accuracy: 0.8652 - val_loss: 0.4749 - val_accuracy: 0.8674\n",
            "Epoch 5/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4743 - accuracy: 0.8691 - val_loss: 0.4662 - val_accuracy: 0.8742\n",
            "Epoch 6/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4676 - accuracy: 0.8725 - val_loss: 0.4765 - val_accuracy: 0.8674\n",
            "Epoch 7/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4560 - accuracy: 0.8719 - val_loss: 0.4595 - val_accuracy: 0.8742\n",
            "Epoch 8/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4474 - accuracy: 0.8742 - val_loss: 0.4551 - val_accuracy: 0.8742\n",
            "Epoch 9/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4451 - accuracy: 0.8742 - val_loss: 0.4611 - val_accuracy: 0.8674\n",
            "Epoch 10/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4406 - accuracy: 0.8725 - val_loss: 0.4564 - val_accuracy: 0.8674\n",
            "Epoch 11/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4319 - accuracy: 0.8736 - val_loss: 0.4557 - val_accuracy: 0.8674\n",
            "Epoch 12/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4320 - accuracy: 0.8736 - val_loss: 0.4622 - val_accuracy: 0.8697\n",
            "Epoch 13/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4271 - accuracy: 0.8753 - val_loss: 0.4662 - val_accuracy: 0.8697\n",
            "Epoch 14/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4257 - accuracy: 0.8747 - val_loss: 0.4469 - val_accuracy: 0.8674\n",
            "Epoch 15/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4215 - accuracy: 0.8742 - val_loss: 0.4464 - val_accuracy: 0.8674\n",
            "Epoch 16/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4182 - accuracy: 0.8747 - val_loss: 0.4472 - val_accuracy: 0.8674\n",
            "Epoch 17/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4212 - accuracy: 0.8742 - val_loss: 0.4749 - val_accuracy: 0.8697\n",
            "Epoch 18/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4224 - accuracy: 0.8742 - val_loss: 0.4449 - val_accuracy: 0.8697\n",
            "Epoch 19/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4158 - accuracy: 0.8758 - val_loss: 0.4539 - val_accuracy: 0.8697\n",
            "Epoch 20/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4117 - accuracy: 0.8820 - val_loss: 0.4483 - val_accuracy: 0.8697\n",
            "Epoch 21/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4096 - accuracy: 0.8815 - val_loss: 0.4486 - val_accuracy: 0.8697\n",
            "Epoch 22/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4052 - accuracy: 0.8820 - val_loss: 0.4539 - val_accuracy: 0.8697\n",
            "Epoch 23/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4032 - accuracy: 0.8826 - val_loss: 0.4501 - val_accuracy: 0.8697\n",
            "Epoch 24/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4064 - accuracy: 0.8815 - val_loss: 0.4490 - val_accuracy: 0.8697\n",
            "Epoch 25/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4045 - accuracy: 0.8820 - val_loss: 0.4547 - val_accuracy: 0.8697\n",
            "Epoch 26/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4012 - accuracy: 0.8826 - val_loss: 0.4479 - val_accuracy: 0.8697\n",
            "Epoch 27/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4006 - accuracy: 0.8826 - val_loss: 0.4507 - val_accuracy: 0.8697\n",
            "Epoch 28/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4016 - accuracy: 0.8831 - val_loss: 0.4543 - val_accuracy: 0.8697\n",
            "Epoch 29/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3986 - accuracy: 0.8831 - val_loss: 0.4481 - val_accuracy: 0.8697\n",
            "Epoch 30/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3999 - accuracy: 0.8826 - val_loss: 0.4548 - val_accuracy: 0.8697\n",
            "Epoch 31/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3995 - accuracy: 0.8837 - val_loss: 0.4601 - val_accuracy: 0.8697\n",
            "Epoch 32/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3997 - accuracy: 0.8837 - val_loss: 0.4558 - val_accuracy: 0.8697\n",
            "Epoch 33/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3986 - accuracy: 0.8831 - val_loss: 0.4562 - val_accuracy: 0.8697\n",
            "Epoch 34/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4021 - accuracy: 0.8837 - val_loss: 0.4545 - val_accuracy: 0.8697\n",
            "Epoch 35/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3986 - accuracy: 0.8837 - val_loss: 0.4505 - val_accuracy: 0.8697\n",
            "Epoch 36/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3982 - accuracy: 0.8843 - val_loss: 0.4559 - val_accuracy: 0.8697\n",
            "Epoch 37/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3986 - accuracy: 0.8831 - val_loss: 0.4581 - val_accuracy: 0.8697\n",
            "Epoch 38/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3985 - accuracy: 0.8837 - val_loss: 0.4608 - val_accuracy: 0.8697\n",
            "Epoch 39/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3975 - accuracy: 0.8843 - val_loss: 0.4579 - val_accuracy: 0.8697\n",
            "Epoch 40/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3983 - accuracy: 0.8843 - val_loss: 0.4667 - val_accuracy: 0.8697\n",
            "Epoch 41/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3967 - accuracy: 0.8843 - val_loss: 0.4543 - val_accuracy: 0.8697\n",
            "Epoch 42/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3975 - accuracy: 0.8843 - val_loss: 0.4579 - val_accuracy: 0.8697\n",
            "Epoch 43/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3963 - accuracy: 0.8837 - val_loss: 0.4559 - val_accuracy: 0.8697\n",
            "Epoch 44/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3972 - accuracy: 0.8831 - val_loss: 0.4721 - val_accuracy: 0.8697\n",
            "Epoch 45/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3946 - accuracy: 0.8837 - val_loss: 0.4641 - val_accuracy: 0.8697\n",
            "Epoch 46/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3958 - accuracy: 0.8854 - val_loss: 0.4576 - val_accuracy: 0.8697\n",
            "Epoch 47/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3931 - accuracy: 0.8848 - val_loss: 0.4622 - val_accuracy: 0.8697\n",
            "Epoch 48/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3970 - accuracy: 0.8837 - val_loss: 0.4716 - val_accuracy: 0.8697\n",
            "Epoch 49/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3977 - accuracy: 0.8860 - val_loss: 0.4801 - val_accuracy: 0.8697\n",
            "Epoch 50/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3947 - accuracy: 0.8843 - val_loss: 0.4636 - val_accuracy: 0.8697\n",
            "Epoch 51/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3967 - accuracy: 0.8854 - val_loss: 0.4631 - val_accuracy: 0.8697\n",
            "Epoch 52/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3954 - accuracy: 0.8860 - val_loss: 0.4607 - val_accuracy: 0.8697\n",
            "Epoch 53/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3943 - accuracy: 0.8854 - val_loss: 0.4705 - val_accuracy: 0.8697\n",
            "Epoch 54/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3942 - accuracy: 0.8843 - val_loss: 0.4685 - val_accuracy: 0.8697\n",
            "Epoch 55/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3914 - accuracy: 0.8854 - val_loss: 0.4604 - val_accuracy: 0.8697\n",
            "Epoch 56/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3918 - accuracy: 0.8848 - val_loss: 0.4670 - val_accuracy: 0.8697\n",
            "Epoch 57/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3916 - accuracy: 0.8848 - val_loss: 0.4844 - val_accuracy: 0.8697\n",
            "Epoch 58/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3965 - accuracy: 0.8848 - val_loss: 0.4656 - val_accuracy: 0.8697\n",
            "Epoch 59/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3928 - accuracy: 0.8843 - val_loss: 0.4730 - val_accuracy: 0.8697\n",
            "Epoch 60/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3927 - accuracy: 0.8848 - val_loss: 0.4665 - val_accuracy: 0.8697\n",
            "Epoch 61/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3975 - accuracy: 0.8837 - val_loss: 0.4616 - val_accuracy: 0.8697\n",
            "Epoch 62/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3935 - accuracy: 0.8860 - val_loss: 0.4614 - val_accuracy: 0.8697\n",
            "Epoch 63/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3933 - accuracy: 0.8854 - val_loss: 0.4659 - val_accuracy: 0.8697\n",
            "Epoch 64/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3894 - accuracy: 0.8854 - val_loss: 0.4680 - val_accuracy: 0.8697\n",
            "Epoch 65/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3967 - accuracy: 0.8837 - val_loss: 0.4757 - val_accuracy: 0.8697\n",
            "Epoch 66/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3918 - accuracy: 0.8848 - val_loss: 0.4595 - val_accuracy: 0.8697\n",
            "Epoch 67/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3897 - accuracy: 0.8848 - val_loss: 0.4763 - val_accuracy: 0.8697\n",
            "Epoch 68/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3913 - accuracy: 0.8860 - val_loss: 0.4762 - val_accuracy: 0.8697\n",
            "Epoch 69/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3955 - accuracy: 0.8854 - val_loss: 0.4871 - val_accuracy: 0.8697\n",
            "Epoch 70/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3900 - accuracy: 0.8848 - val_loss: 0.4749 - val_accuracy: 0.8697\n",
            "____________________________________________________________________________________________________\n",
            "FOLD NO 3 START\n",
            "Epoch 1/70\n",
            "56/56 [==============================] - 0s 4ms/step - loss: 1.2435 - accuracy: 0.2933 - val_loss: 0.9858 - val_accuracy: 0.7101\n",
            "Epoch 2/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.7938 - accuracy: 0.7972 - val_loss: 0.6372 - val_accuracy: 0.8742\n",
            "Epoch 3/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5315 - accuracy: 0.8629 - val_loss: 0.5026 - val_accuracy: 0.8809\n",
            "Epoch 4/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4780 - accuracy: 0.8635 - val_loss: 0.4821 - val_accuracy: 0.8809\n",
            "Epoch 5/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4620 - accuracy: 0.8646 - val_loss: 0.4759 - val_accuracy: 0.8809\n",
            "Epoch 6/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4550 - accuracy: 0.8697 - val_loss: 0.4710 - val_accuracy: 0.8809\n",
            "Epoch 7/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4472 - accuracy: 0.8719 - val_loss: 0.4738 - val_accuracy: 0.8809\n",
            "Epoch 8/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4368 - accuracy: 0.8725 - val_loss: 0.4732 - val_accuracy: 0.8809\n",
            "Epoch 9/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4305 - accuracy: 0.8753 - val_loss: 0.4711 - val_accuracy: 0.8809\n",
            "Epoch 10/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4255 - accuracy: 0.8815 - val_loss: 0.4810 - val_accuracy: 0.8697\n",
            "Epoch 11/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4191 - accuracy: 0.8820 - val_loss: 0.4759 - val_accuracy: 0.8742\n",
            "Epoch 12/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4172 - accuracy: 0.8815 - val_loss: 0.4792 - val_accuracy: 0.8809\n",
            "Epoch 13/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4168 - accuracy: 0.8820 - val_loss: 0.4758 - val_accuracy: 0.8742\n",
            "Epoch 14/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4144 - accuracy: 0.8826 - val_loss: 0.4779 - val_accuracy: 0.8742\n",
            "Epoch 15/70\n",
            "56/56 [==============================] - 1s 10ms/step - loss: 0.4128 - accuracy: 0.8815 - val_loss: 0.4797 - val_accuracy: 0.8742\n",
            "Epoch 16/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4091 - accuracy: 0.8820 - val_loss: 0.4815 - val_accuracy: 0.8697\n",
            "Epoch 17/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4094 - accuracy: 0.8820 - val_loss: 0.4814 - val_accuracy: 0.8697\n",
            "Epoch 18/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4115 - accuracy: 0.8831 - val_loss: 0.4834 - val_accuracy: 0.8697\n",
            "Epoch 19/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4085 - accuracy: 0.8820 - val_loss: 0.4874 - val_accuracy: 0.8697\n",
            "Epoch 20/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4080 - accuracy: 0.8831 - val_loss: 0.4813 - val_accuracy: 0.8697\n",
            "Epoch 21/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4083 - accuracy: 0.8826 - val_loss: 0.4830 - val_accuracy: 0.8697\n",
            "Epoch 22/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4034 - accuracy: 0.8820 - val_loss: 0.4803 - val_accuracy: 0.8742\n",
            "Epoch 23/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4038 - accuracy: 0.8820 - val_loss: 0.4848 - val_accuracy: 0.8697\n",
            "Epoch 24/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4097 - accuracy: 0.8831 - val_loss: 0.4824 - val_accuracy: 0.8697\n",
            "Epoch 25/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4066 - accuracy: 0.8826 - val_loss: 0.4906 - val_accuracy: 0.8697\n",
            "Epoch 26/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4038 - accuracy: 0.8831 - val_loss: 0.4846 - val_accuracy: 0.8697\n",
            "Epoch 27/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4022 - accuracy: 0.8820 - val_loss: 0.4788 - val_accuracy: 0.8742\n",
            "Epoch 28/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4034 - accuracy: 0.8815 - val_loss: 0.4816 - val_accuracy: 0.8697\n",
            "Epoch 29/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4028 - accuracy: 0.8837 - val_loss: 0.4788 - val_accuracy: 0.8697\n",
            "Epoch 30/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4007 - accuracy: 0.8815 - val_loss: 0.4841 - val_accuracy: 0.8697\n",
            "Epoch 31/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4043 - accuracy: 0.8831 - val_loss: 0.4905 - val_accuracy: 0.8697\n",
            "Epoch 32/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4023 - accuracy: 0.8831 - val_loss: 0.4817 - val_accuracy: 0.8742\n",
            "Epoch 33/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4036 - accuracy: 0.8831 - val_loss: 0.4791 - val_accuracy: 0.8742\n",
            "Epoch 34/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4026 - accuracy: 0.8837 - val_loss: 0.4805 - val_accuracy: 0.8742\n",
            "Epoch 35/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3982 - accuracy: 0.8837 - val_loss: 0.4907 - val_accuracy: 0.8697\n",
            "Epoch 36/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4016 - accuracy: 0.8837 - val_loss: 0.4841 - val_accuracy: 0.8697\n",
            "Epoch 37/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3998 - accuracy: 0.8831 - val_loss: 0.4844 - val_accuracy: 0.8742\n",
            "Epoch 38/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3996 - accuracy: 0.8831 - val_loss: 0.4946 - val_accuracy: 0.8697\n",
            "Epoch 39/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4007 - accuracy: 0.8831 - val_loss: 0.4864 - val_accuracy: 0.8697\n",
            "Epoch 40/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3982 - accuracy: 0.8831 - val_loss: 0.4944 - val_accuracy: 0.8697\n",
            "Epoch 41/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3984 - accuracy: 0.8826 - val_loss: 0.4838 - val_accuracy: 0.8742\n",
            "Epoch 42/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3986 - accuracy: 0.8831 - val_loss: 0.4845 - val_accuracy: 0.8697\n",
            "Epoch 43/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3994 - accuracy: 0.8848 - val_loss: 0.4982 - val_accuracy: 0.8719\n",
            "Epoch 44/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3984 - accuracy: 0.8815 - val_loss: 0.4974 - val_accuracy: 0.8697\n",
            "Epoch 45/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3971 - accuracy: 0.8826 - val_loss: 0.4853 - val_accuracy: 0.8742\n",
            "Epoch 46/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3975 - accuracy: 0.8837 - val_loss: 0.4817 - val_accuracy: 0.8742\n",
            "Epoch 47/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3954 - accuracy: 0.8831 - val_loss: 0.4896 - val_accuracy: 0.8697\n",
            "Epoch 48/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3963 - accuracy: 0.8831 - val_loss: 0.4893 - val_accuracy: 0.8697\n",
            "Epoch 49/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3981 - accuracy: 0.8826 - val_loss: 0.4958 - val_accuracy: 0.8719\n",
            "Epoch 50/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3958 - accuracy: 0.8826 - val_loss: 0.4960 - val_accuracy: 0.8742\n",
            "Epoch 51/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3967 - accuracy: 0.8843 - val_loss: 0.4899 - val_accuracy: 0.8742\n",
            "Epoch 52/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3971 - accuracy: 0.8831 - val_loss: 0.4967 - val_accuracy: 0.8719\n",
            "Epoch 53/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3966 - accuracy: 0.8826 - val_loss: 0.4915 - val_accuracy: 0.8697\n",
            "Epoch 54/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3932 - accuracy: 0.8843 - val_loss: 0.4879 - val_accuracy: 0.8697\n",
            "Epoch 55/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3959 - accuracy: 0.8837 - val_loss: 0.5015 - val_accuracy: 0.8719\n",
            "Epoch 56/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3958 - accuracy: 0.8854 - val_loss: 0.4939 - val_accuracy: 0.8719\n",
            "Epoch 57/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3921 - accuracy: 0.8848 - val_loss: 0.5038 - val_accuracy: 0.8719\n",
            "Epoch 58/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3935 - accuracy: 0.8837 - val_loss: 0.4929 - val_accuracy: 0.8742\n",
            "Epoch 59/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3942 - accuracy: 0.8848 - val_loss: 0.4952 - val_accuracy: 0.8742\n",
            "Epoch 60/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3952 - accuracy: 0.8860 - val_loss: 0.4935 - val_accuracy: 0.8697\n",
            "Epoch 61/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3967 - accuracy: 0.8854 - val_loss: 0.4946 - val_accuracy: 0.8697\n",
            "Epoch 62/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3936 - accuracy: 0.8865 - val_loss: 0.4977 - val_accuracy: 0.8697\n",
            "Epoch 63/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3953 - accuracy: 0.8843 - val_loss: 0.4936 - val_accuracy: 0.8719\n",
            "Epoch 64/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3908 - accuracy: 0.8848 - val_loss: 0.4956 - val_accuracy: 0.8719\n",
            "Epoch 65/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3925 - accuracy: 0.8854 - val_loss: 0.4985 - val_accuracy: 0.8719\n",
            "Epoch 66/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3954 - accuracy: 0.8837 - val_loss: 0.4935 - val_accuracy: 0.8742\n",
            "Epoch 67/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3925 - accuracy: 0.8865 - val_loss: 0.4992 - val_accuracy: 0.8719\n",
            "Epoch 68/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3893 - accuracy: 0.8854 - val_loss: 0.4961 - val_accuracy: 0.8697\n",
            "Epoch 69/70\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.3931 - accuracy: 0.8854 - val_loss: 0.4996 - val_accuracy: 0.8719\n",
            "Epoch 70/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3895 - accuracy: 0.8848 - val_loss: 0.5029 - val_accuracy: 0.8697\n",
            "____________________________________________________________________________________________________\n",
            "FOLD NO 4 START\n",
            "Epoch 1/70\n",
            "56/56 [==============================] - 0s 4ms/step - loss: 1.0507 - accuracy: 0.7966 - val_loss: 0.7318 - val_accuracy: 0.8787\n",
            "Epoch 2/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5681 - accuracy: 0.8640 - val_loss: 0.4705 - val_accuracy: 0.8809\n",
            "Epoch 3/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4686 - accuracy: 0.8770 - val_loss: 0.4532 - val_accuracy: 0.8809\n",
            "Epoch 4/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4515 - accuracy: 0.8781 - val_loss: 0.4520 - val_accuracy: 0.8809\n",
            "Epoch 5/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4425 - accuracy: 0.8787 - val_loss: 0.4468 - val_accuracy: 0.8809\n",
            "Epoch 6/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4307 - accuracy: 0.8854 - val_loss: 0.4432 - val_accuracy: 0.8809\n",
            "Epoch 7/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4205 - accuracy: 0.8876 - val_loss: 0.4404 - val_accuracy: 0.8809\n",
            "Epoch 8/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4138 - accuracy: 0.8876 - val_loss: 0.4412 - val_accuracy: 0.8764\n",
            "Epoch 9/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4126 - accuracy: 0.8899 - val_loss: 0.4388 - val_accuracy: 0.8764\n",
            "Epoch 10/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4076 - accuracy: 0.8899 - val_loss: 0.4427 - val_accuracy: 0.8764\n",
            "Epoch 11/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4039 - accuracy: 0.8899 - val_loss: 0.4328 - val_accuracy: 0.8809\n",
            "Epoch 12/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4018 - accuracy: 0.8888 - val_loss: 0.4376 - val_accuracy: 0.8764\n",
            "Epoch 13/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4004 - accuracy: 0.8904 - val_loss: 0.4355 - val_accuracy: 0.8764\n",
            "Epoch 14/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3981 - accuracy: 0.8904 - val_loss: 0.4429 - val_accuracy: 0.8764\n",
            "Epoch 15/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3982 - accuracy: 0.8904 - val_loss: 0.4323 - val_accuracy: 0.8764\n",
            "Epoch 16/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3959 - accuracy: 0.8904 - val_loss: 0.4338 - val_accuracy: 0.8764\n",
            "Epoch 17/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3963 - accuracy: 0.8904 - val_loss: 0.4351 - val_accuracy: 0.8764\n",
            "Epoch 18/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3938 - accuracy: 0.8899 - val_loss: 0.4419 - val_accuracy: 0.8764\n",
            "Epoch 19/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3948 - accuracy: 0.8910 - val_loss: 0.4387 - val_accuracy: 0.8764\n",
            "Epoch 20/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3910 - accuracy: 0.8910 - val_loss: 0.4383 - val_accuracy: 0.8764\n",
            "Epoch 21/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3933 - accuracy: 0.8904 - val_loss: 0.4386 - val_accuracy: 0.8764\n",
            "Epoch 22/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3919 - accuracy: 0.8899 - val_loss: 0.4452 - val_accuracy: 0.8787\n",
            "Epoch 23/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3912 - accuracy: 0.8910 - val_loss: 0.4399 - val_accuracy: 0.8764\n",
            "Epoch 24/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3930 - accuracy: 0.8910 - val_loss: 0.4454 - val_accuracy: 0.8764\n",
            "Epoch 25/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3895 - accuracy: 0.8910 - val_loss: 0.4394 - val_accuracy: 0.8764\n",
            "Epoch 26/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3894 - accuracy: 0.8921 - val_loss: 0.4365 - val_accuracy: 0.8764\n",
            "Epoch 27/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3923 - accuracy: 0.8916 - val_loss: 0.4443 - val_accuracy: 0.8764\n",
            "Epoch 28/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3876 - accuracy: 0.8904 - val_loss: 0.4400 - val_accuracy: 0.8764\n",
            "Epoch 29/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3890 - accuracy: 0.8916 - val_loss: 0.4442 - val_accuracy: 0.8787\n",
            "Epoch 30/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3856 - accuracy: 0.8927 - val_loss: 0.4402 - val_accuracy: 0.8764\n",
            "Epoch 31/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3867 - accuracy: 0.8910 - val_loss: 0.4454 - val_accuracy: 0.8787\n",
            "Epoch 32/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3867 - accuracy: 0.8921 - val_loss: 0.4399 - val_accuracy: 0.8764\n",
            "Epoch 33/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3886 - accuracy: 0.8921 - val_loss: 0.4443 - val_accuracy: 0.8787\n",
            "Epoch 34/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3881 - accuracy: 0.8910 - val_loss: 0.4440 - val_accuracy: 0.8787\n",
            "Epoch 35/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3869 - accuracy: 0.8904 - val_loss: 0.4531 - val_accuracy: 0.8787\n",
            "Epoch 36/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3863 - accuracy: 0.8904 - val_loss: 0.4495 - val_accuracy: 0.8787\n",
            "Epoch 37/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3838 - accuracy: 0.8916 - val_loss: 0.4544 - val_accuracy: 0.8764\n",
            "Epoch 38/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3861 - accuracy: 0.8916 - val_loss: 0.4440 - val_accuracy: 0.8787\n",
            "Epoch 39/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3876 - accuracy: 0.8927 - val_loss: 0.4438 - val_accuracy: 0.8742\n",
            "Epoch 40/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3839 - accuracy: 0.8910 - val_loss: 0.4529 - val_accuracy: 0.8742\n",
            "Epoch 41/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3839 - accuracy: 0.8927 - val_loss: 0.4466 - val_accuracy: 0.8764\n",
            "Epoch 42/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3838 - accuracy: 0.8916 - val_loss: 0.4433 - val_accuracy: 0.8764\n",
            "Epoch 43/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3849 - accuracy: 0.8916 - val_loss: 0.4439 - val_accuracy: 0.8764\n",
            "Epoch 44/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3859 - accuracy: 0.8927 - val_loss: 0.4563 - val_accuracy: 0.8742\n",
            "Epoch 45/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3833 - accuracy: 0.8938 - val_loss: 0.4478 - val_accuracy: 0.8764\n",
            "Epoch 46/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3844 - accuracy: 0.8927 - val_loss: 0.4479 - val_accuracy: 0.8742\n",
            "Epoch 47/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3836 - accuracy: 0.8921 - val_loss: 0.4478 - val_accuracy: 0.8764\n",
            "Epoch 48/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3828 - accuracy: 0.8921 - val_loss: 0.4585 - val_accuracy: 0.8742\n",
            "Epoch 49/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3822 - accuracy: 0.8916 - val_loss: 0.4547 - val_accuracy: 0.8764\n",
            "Epoch 50/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3802 - accuracy: 0.8927 - val_loss: 0.4546 - val_accuracy: 0.8787\n",
            "Epoch 51/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3854 - accuracy: 0.8910 - val_loss: 0.4513 - val_accuracy: 0.8742\n",
            "Epoch 52/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3835 - accuracy: 0.8933 - val_loss: 0.4461 - val_accuracy: 0.8764\n",
            "Epoch 53/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3841 - accuracy: 0.8921 - val_loss: 0.4544 - val_accuracy: 0.8742\n",
            "Epoch 54/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3811 - accuracy: 0.8927 - val_loss: 0.4533 - val_accuracy: 0.8764\n",
            "Epoch 55/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3831 - accuracy: 0.8921 - val_loss: 0.4592 - val_accuracy: 0.8742\n",
            "Epoch 56/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3828 - accuracy: 0.8933 - val_loss: 0.4484 - val_accuracy: 0.8764\n",
            "Epoch 57/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3805 - accuracy: 0.8933 - val_loss: 0.4543 - val_accuracy: 0.8787\n",
            "Epoch 58/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3830 - accuracy: 0.8916 - val_loss: 0.4615 - val_accuracy: 0.8742\n",
            "Epoch 59/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3856 - accuracy: 0.8938 - val_loss: 0.4563 - val_accuracy: 0.8742\n",
            "Epoch 60/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3794 - accuracy: 0.8938 - val_loss: 0.4487 - val_accuracy: 0.8764\n",
            "Epoch 61/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3796 - accuracy: 0.8927 - val_loss: 0.4574 - val_accuracy: 0.8742\n",
            "Epoch 62/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3853 - accuracy: 0.8921 - val_loss: 0.4543 - val_accuracy: 0.8764\n",
            "Epoch 63/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3814 - accuracy: 0.8933 - val_loss: 0.4566 - val_accuracy: 0.8764\n",
            "Epoch 64/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3806 - accuracy: 0.8921 - val_loss: 0.4530 - val_accuracy: 0.8787\n",
            "Epoch 65/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3798 - accuracy: 0.8933 - val_loss: 0.4507 - val_accuracy: 0.8764\n",
            "Epoch 66/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3803 - accuracy: 0.8921 - val_loss: 0.4507 - val_accuracy: 0.8764\n",
            "Epoch 67/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3816 - accuracy: 0.8921 - val_loss: 0.4574 - val_accuracy: 0.8764\n",
            "Epoch 68/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3796 - accuracy: 0.8933 - val_loss: 0.4568 - val_accuracy: 0.8742\n",
            "Epoch 69/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3811 - accuracy: 0.8927 - val_loss: 0.4582 - val_accuracy: 0.8764\n",
            "Epoch 70/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3790 - accuracy: 0.8927 - val_loss: 0.4575 - val_accuracy: 0.8742\n",
            "____________________________________________________________________________________________________\n",
            "FOLD NO 5 START\n",
            "Epoch 1/70\n",
            "56/56 [==============================] - 0s 4ms/step - loss: 1.1243 - accuracy: 0.6343 - val_loss: 0.7922 - val_accuracy: 0.7461\n",
            "Epoch 2/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5968 - accuracy: 0.8219 - val_loss: 0.4414 - val_accuracy: 0.8854\n",
            "Epoch 3/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4858 - accuracy: 0.8719 - val_loss: 0.4235 - val_accuracy: 0.8854\n",
            "Epoch 4/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4684 - accuracy: 0.8758 - val_loss: 0.4155 - val_accuracy: 0.8854\n",
            "Epoch 5/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4544 - accuracy: 0.8758 - val_loss: 0.4158 - val_accuracy: 0.8854\n",
            "Epoch 6/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4491 - accuracy: 0.8753 - val_loss: 0.4133 - val_accuracy: 0.8854\n",
            "Epoch 7/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4408 - accuracy: 0.8758 - val_loss: 0.4105 - val_accuracy: 0.8854\n",
            "Epoch 8/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4366 - accuracy: 0.8764 - val_loss: 0.4134 - val_accuracy: 0.8854\n",
            "Epoch 9/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4363 - accuracy: 0.8770 - val_loss: 0.4099 - val_accuracy: 0.8854\n",
            "Epoch 10/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4318 - accuracy: 0.8764 - val_loss: 0.4105 - val_accuracy: 0.8854\n",
            "Epoch 11/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4278 - accuracy: 0.8764 - val_loss: 0.4087 - val_accuracy: 0.8854\n",
            "Epoch 12/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4291 - accuracy: 0.8764 - val_loss: 0.4110 - val_accuracy: 0.8854\n",
            "Epoch 13/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4268 - accuracy: 0.8770 - val_loss: 0.4114 - val_accuracy: 0.8809\n",
            "Epoch 14/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4253 - accuracy: 0.8770 - val_loss: 0.4126 - val_accuracy: 0.8854\n",
            "Epoch 15/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4194 - accuracy: 0.8770 - val_loss: 0.4134 - val_accuracy: 0.8809\n",
            "Epoch 16/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4191 - accuracy: 0.8781 - val_loss: 0.4110 - val_accuracy: 0.8854\n",
            "Epoch 17/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4184 - accuracy: 0.8775 - val_loss: 0.4138 - val_accuracy: 0.8809\n",
            "Epoch 18/70\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4186 - accuracy: 0.8781 - val_loss: 0.4130 - val_accuracy: 0.8854\n",
            "Epoch 19/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4167 - accuracy: 0.8775 - val_loss: 0.4113 - val_accuracy: 0.8854\n",
            "Epoch 20/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4131 - accuracy: 0.8775 - val_loss: 0.4156 - val_accuracy: 0.8876\n",
            "Epoch 21/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4141 - accuracy: 0.8787 - val_loss: 0.4134 - val_accuracy: 0.8831\n",
            "Epoch 22/70\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4127 - accuracy: 0.8781 - val_loss: 0.4149 - val_accuracy: 0.8831\n",
            "Epoch 23/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4149 - accuracy: 0.8781 - val_loss: 0.4134 - val_accuracy: 0.8831\n",
            "Epoch 24/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4097 - accuracy: 0.8787 - val_loss: 0.4140 - val_accuracy: 0.8831\n",
            "Epoch 25/70\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4092 - accuracy: 0.8787 - val_loss: 0.4126 - val_accuracy: 0.8831\n",
            "Epoch 26/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4071 - accuracy: 0.8787 - val_loss: 0.4200 - val_accuracy: 0.8831\n",
            "Epoch 27/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4068 - accuracy: 0.8787 - val_loss: 0.4166 - val_accuracy: 0.8831\n",
            "Epoch 28/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4085 - accuracy: 0.8781 - val_loss: 0.4169 - val_accuracy: 0.8876\n",
            "Epoch 29/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4062 - accuracy: 0.8815 - val_loss: 0.4139 - val_accuracy: 0.8831\n",
            "Epoch 30/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4075 - accuracy: 0.8831 - val_loss: 0.4123 - val_accuracy: 0.8831\n",
            "Epoch 31/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4054 - accuracy: 0.8843 - val_loss: 0.4137 - val_accuracy: 0.8831\n",
            "Epoch 32/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4030 - accuracy: 0.8831 - val_loss: 0.4151 - val_accuracy: 0.8831\n",
            "Epoch 33/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4030 - accuracy: 0.8854 - val_loss: 0.4144 - val_accuracy: 0.8831\n",
            "Epoch 34/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4048 - accuracy: 0.8848 - val_loss: 0.4157 - val_accuracy: 0.8831\n",
            "Epoch 35/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4015 - accuracy: 0.8848 - val_loss: 0.4160 - val_accuracy: 0.8831\n",
            "Epoch 36/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4034 - accuracy: 0.8871 - val_loss: 0.4196 - val_accuracy: 0.8809\n",
            "Epoch 37/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4007 - accuracy: 0.8854 - val_loss: 0.4172 - val_accuracy: 0.8831\n",
            "Epoch 38/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4015 - accuracy: 0.8860 - val_loss: 0.4174 - val_accuracy: 0.8831\n",
            "Epoch 39/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4031 - accuracy: 0.8860 - val_loss: 0.4187 - val_accuracy: 0.8809\n",
            "Epoch 40/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4004 - accuracy: 0.8876 - val_loss: 0.4170 - val_accuracy: 0.8809\n",
            "Epoch 41/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4022 - accuracy: 0.8876 - val_loss: 0.4169 - val_accuracy: 0.8809\n",
            "Epoch 42/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3989 - accuracy: 0.8882 - val_loss: 0.4189 - val_accuracy: 0.8809\n",
            "Epoch 43/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3982 - accuracy: 0.8888 - val_loss: 0.4174 - val_accuracy: 0.8809\n",
            "Epoch 44/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3970 - accuracy: 0.8888 - val_loss: 0.4193 - val_accuracy: 0.8809\n",
            "Epoch 45/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3963 - accuracy: 0.8888 - val_loss: 0.4167 - val_accuracy: 0.8809\n",
            "Epoch 46/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3971 - accuracy: 0.8888 - val_loss: 0.4203 - val_accuracy: 0.8809\n",
            "Epoch 47/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3967 - accuracy: 0.8888 - val_loss: 0.4191 - val_accuracy: 0.8809\n",
            "Epoch 48/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3954 - accuracy: 0.8888 - val_loss: 0.4197 - val_accuracy: 0.8809\n",
            "Epoch 49/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3980 - accuracy: 0.8888 - val_loss: 0.4190 - val_accuracy: 0.8809\n",
            "Epoch 50/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3952 - accuracy: 0.8888 - val_loss: 0.4198 - val_accuracy: 0.8809\n",
            "Epoch 51/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3941 - accuracy: 0.8888 - val_loss: 0.4211 - val_accuracy: 0.8809\n",
            "Epoch 52/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3961 - accuracy: 0.8893 - val_loss: 0.4222 - val_accuracy: 0.8809\n",
            "Epoch 53/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3946 - accuracy: 0.8893 - val_loss: 0.4247 - val_accuracy: 0.8809\n",
            "Epoch 54/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3935 - accuracy: 0.8893 - val_loss: 0.4194 - val_accuracy: 0.8809\n",
            "Epoch 55/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3924 - accuracy: 0.8893 - val_loss: 0.4198 - val_accuracy: 0.8809\n",
            "Epoch 56/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3912 - accuracy: 0.8893 - val_loss: 0.4240 - val_accuracy: 0.8809\n",
            "Epoch 57/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3907 - accuracy: 0.8893 - val_loss: 0.4217 - val_accuracy: 0.8809\n",
            "Epoch 58/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3903 - accuracy: 0.8893 - val_loss: 0.4226 - val_accuracy: 0.8809\n",
            "Epoch 59/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3912 - accuracy: 0.8893 - val_loss: 0.4228 - val_accuracy: 0.8809\n",
            "Epoch 60/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3904 - accuracy: 0.8893 - val_loss: 0.4206 - val_accuracy: 0.8809\n",
            "Epoch 61/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3906 - accuracy: 0.8893 - val_loss: 0.4219 - val_accuracy: 0.8809\n",
            "Epoch 62/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3922 - accuracy: 0.8893 - val_loss: 0.4218 - val_accuracy: 0.8809\n",
            "Epoch 63/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3917 - accuracy: 0.8893 - val_loss: 0.4208 - val_accuracy: 0.8809\n",
            "Epoch 64/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3883 - accuracy: 0.8893 - val_loss: 0.4219 - val_accuracy: 0.8809\n",
            "Epoch 65/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3903 - accuracy: 0.8893 - val_loss: 0.4228 - val_accuracy: 0.8809\n",
            "Epoch 66/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3880 - accuracy: 0.8893 - val_loss: 0.4218 - val_accuracy: 0.8809\n",
            "Epoch 67/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3911 - accuracy: 0.8893 - val_loss: 0.4240 - val_accuracy: 0.8809\n",
            "Epoch 68/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3908 - accuracy: 0.8893 - val_loss: 0.4218 - val_accuracy: 0.8809\n",
            "Epoch 69/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3886 - accuracy: 0.8893 - val_loss: 0.4237 - val_accuracy: 0.8809\n",
            "Epoch 70/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3881 - accuracy: 0.8893 - val_loss: 0.4261 - val_accuracy: 0.8809\n",
            "____________________________________________________________________________________________________\n",
            "FOLD NO 6 START\n",
            "Epoch 1/70\n",
            "56/56 [==============================] - 0s 4ms/step - loss: 1.0593 - accuracy: 0.6725 - val_loss: 0.7640 - val_accuracy: 0.8045\n",
            "Epoch 2/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.6480 - accuracy: 0.8067 - val_loss: 0.5008 - val_accuracy: 0.8697\n",
            "Epoch 3/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5149 - accuracy: 0.8708 - val_loss: 0.4435 - val_accuracy: 0.8787\n",
            "Epoch 4/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4789 - accuracy: 0.8680 - val_loss: 0.4353 - val_accuracy: 0.8787\n",
            "Epoch 5/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4628 - accuracy: 0.8719 - val_loss: 0.4325 - val_accuracy: 0.8787\n",
            "Epoch 6/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4460 - accuracy: 0.8798 - val_loss: 0.4298 - val_accuracy: 0.8787\n",
            "Epoch 7/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4360 - accuracy: 0.8815 - val_loss: 0.4326 - val_accuracy: 0.8764\n",
            "Epoch 8/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4290 - accuracy: 0.8815 - val_loss: 0.4351 - val_accuracy: 0.8764\n",
            "Epoch 9/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4195 - accuracy: 0.8820 - val_loss: 0.4374 - val_accuracy: 0.8787\n",
            "Epoch 10/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4182 - accuracy: 0.8820 - val_loss: 0.4338 - val_accuracy: 0.8787\n",
            "Epoch 11/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4144 - accuracy: 0.8826 - val_loss: 0.4383 - val_accuracy: 0.8787\n",
            "Epoch 12/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4096 - accuracy: 0.8826 - val_loss: 0.4402 - val_accuracy: 0.8787\n",
            "Epoch 13/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4080 - accuracy: 0.8826 - val_loss: 0.4429 - val_accuracy: 0.8787\n",
            "Epoch 14/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4088 - accuracy: 0.8826 - val_loss: 0.4433 - val_accuracy: 0.8787\n",
            "Epoch 15/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4090 - accuracy: 0.8809 - val_loss: 0.4418 - val_accuracy: 0.8787\n",
            "Epoch 16/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4053 - accuracy: 0.8843 - val_loss: 0.4399 - val_accuracy: 0.8787\n",
            "Epoch 17/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4045 - accuracy: 0.8843 - val_loss: 0.4446 - val_accuracy: 0.8787\n",
            "Epoch 18/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4015 - accuracy: 0.8860 - val_loss: 0.4410 - val_accuracy: 0.8787\n",
            "Epoch 19/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4033 - accuracy: 0.8865 - val_loss: 0.4530 - val_accuracy: 0.8742\n",
            "Epoch 20/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3997 - accuracy: 0.8848 - val_loss: 0.4420 - val_accuracy: 0.8719\n",
            "Epoch 21/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4004 - accuracy: 0.8860 - val_loss: 0.4417 - val_accuracy: 0.8719\n",
            "Epoch 22/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3979 - accuracy: 0.8860 - val_loss: 0.4414 - val_accuracy: 0.8719\n",
            "Epoch 23/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3980 - accuracy: 0.8854 - val_loss: 0.4447 - val_accuracy: 0.8719\n",
            "Epoch 24/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3962 - accuracy: 0.8865 - val_loss: 0.4434 - val_accuracy: 0.8719\n",
            "Epoch 25/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3988 - accuracy: 0.8860 - val_loss: 0.4414 - val_accuracy: 0.8719\n",
            "Epoch 26/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3991 - accuracy: 0.8865 - val_loss: 0.4450 - val_accuracy: 0.8719\n",
            "Epoch 27/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3963 - accuracy: 0.8854 - val_loss: 0.4443 - val_accuracy: 0.8719\n",
            "Epoch 28/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3959 - accuracy: 0.8876 - val_loss: 0.4432 - val_accuracy: 0.8719\n",
            "Epoch 29/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3951 - accuracy: 0.8876 - val_loss: 0.4486 - val_accuracy: 0.8719\n",
            "Epoch 30/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3966 - accuracy: 0.8876 - val_loss: 0.4444 - val_accuracy: 0.8719\n",
            "Epoch 31/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3925 - accuracy: 0.8860 - val_loss: 0.4448 - val_accuracy: 0.8719\n",
            "Epoch 32/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3938 - accuracy: 0.8871 - val_loss: 0.4480 - val_accuracy: 0.8719\n",
            "Epoch 33/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3939 - accuracy: 0.8860 - val_loss: 0.4522 - val_accuracy: 0.8742\n",
            "Epoch 34/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3934 - accuracy: 0.8876 - val_loss: 0.4557 - val_accuracy: 0.8719\n",
            "Epoch 35/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3916 - accuracy: 0.8860 - val_loss: 0.4449 - val_accuracy: 0.8697\n",
            "Epoch 36/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3922 - accuracy: 0.8865 - val_loss: 0.4465 - val_accuracy: 0.8697\n",
            "Epoch 37/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3900 - accuracy: 0.8865 - val_loss: 0.4494 - val_accuracy: 0.8719\n",
            "Epoch 38/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3900 - accuracy: 0.8865 - val_loss: 0.4531 - val_accuracy: 0.8719\n",
            "Epoch 39/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3952 - accuracy: 0.8871 - val_loss: 0.4513 - val_accuracy: 0.8854\n",
            "Epoch 40/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3907 - accuracy: 0.8876 - val_loss: 0.4525 - val_accuracy: 0.8697\n",
            "Epoch 41/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3919 - accuracy: 0.8871 - val_loss: 0.4525 - val_accuracy: 0.8697\n",
            "Epoch 42/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3889 - accuracy: 0.8888 - val_loss: 0.4485 - val_accuracy: 0.8764\n",
            "Epoch 43/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3891 - accuracy: 0.8888 - val_loss: 0.4543 - val_accuracy: 0.8719\n",
            "Epoch 44/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3898 - accuracy: 0.8888 - val_loss: 0.4556 - val_accuracy: 0.8697\n",
            "Epoch 45/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3897 - accuracy: 0.8876 - val_loss: 0.4522 - val_accuracy: 0.8697\n",
            "Epoch 46/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3897 - accuracy: 0.8888 - val_loss: 0.4530 - val_accuracy: 0.8697\n",
            "Epoch 47/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3864 - accuracy: 0.8882 - val_loss: 0.4597 - val_accuracy: 0.8697\n",
            "Epoch 48/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3893 - accuracy: 0.8876 - val_loss: 0.4535 - val_accuracy: 0.8697\n",
            "Epoch 49/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3890 - accuracy: 0.8871 - val_loss: 0.4495 - val_accuracy: 0.8697\n",
            "Epoch 50/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3878 - accuracy: 0.8882 - val_loss: 0.4582 - val_accuracy: 0.8674\n",
            "Epoch 51/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3900 - accuracy: 0.8888 - val_loss: 0.4576 - val_accuracy: 0.8674\n",
            "Epoch 52/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3878 - accuracy: 0.8888 - val_loss: 0.4540 - val_accuracy: 0.8697\n",
            "Epoch 53/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3885 - accuracy: 0.8871 - val_loss: 0.4548 - val_accuracy: 0.8697\n",
            "Epoch 54/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3882 - accuracy: 0.8882 - val_loss: 0.4631 - val_accuracy: 0.8697\n",
            "Epoch 55/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3871 - accuracy: 0.8888 - val_loss: 0.4588 - val_accuracy: 0.8697\n",
            "Epoch 56/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3882 - accuracy: 0.8876 - val_loss: 0.4582 - val_accuracy: 0.8697\n",
            "Epoch 57/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3853 - accuracy: 0.8893 - val_loss: 0.4734 - val_accuracy: 0.8697\n",
            "Epoch 58/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3895 - accuracy: 0.8876 - val_loss: 0.4618 - val_accuracy: 0.8719\n",
            "Epoch 59/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3876 - accuracy: 0.8893 - val_loss: 0.4588 - val_accuracy: 0.8719\n",
            "Epoch 60/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3872 - accuracy: 0.8871 - val_loss: 0.4712 - val_accuracy: 0.8697\n",
            "Epoch 61/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3860 - accuracy: 0.8888 - val_loss: 0.4601 - val_accuracy: 0.8719\n",
            "Epoch 62/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3870 - accuracy: 0.8893 - val_loss: 0.4700 - val_accuracy: 0.8697\n",
            "Epoch 63/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3908 - accuracy: 0.8871 - val_loss: 0.4603 - val_accuracy: 0.8697\n",
            "Epoch 64/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3886 - accuracy: 0.8876 - val_loss: 0.4640 - val_accuracy: 0.8697\n",
            "Epoch 65/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3871 - accuracy: 0.8882 - val_loss: 0.4632 - val_accuracy: 0.8697\n",
            "Epoch 66/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3908 - accuracy: 0.8888 - val_loss: 0.4753 - val_accuracy: 0.8697\n",
            "Epoch 67/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3872 - accuracy: 0.8882 - val_loss: 0.4653 - val_accuracy: 0.8697\n",
            "Epoch 68/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3841 - accuracy: 0.8888 - val_loss: 0.4635 - val_accuracy: 0.8719\n",
            "Epoch 69/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3888 - accuracy: 0.8893 - val_loss: 0.4676 - val_accuracy: 0.8719\n",
            "Epoch 70/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3839 - accuracy: 0.8882 - val_loss: 0.4664 - val_accuracy: 0.8697\n",
            "____________________________________________________________________________________________________\n",
            "FOLD NO 7 START\n",
            "Epoch 1/70\n",
            "56/56 [==============================] - 0s 4ms/step - loss: 0.9508 - accuracy: 0.8135 - val_loss: 0.6033 - val_accuracy: 0.8543\n",
            "Epoch 2/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.5294 - accuracy: 0.8579 - val_loss: 0.4576 - val_accuracy: 0.8789\n",
            "Epoch 3/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4924 - accuracy: 0.8629 - val_loss: 0.4527 - val_accuracy: 0.8789\n",
            "Epoch 4/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4794 - accuracy: 0.8629 - val_loss: 0.4456 - val_accuracy: 0.8767\n",
            "Epoch 5/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4675 - accuracy: 0.8635 - val_loss: 0.4404 - val_accuracy: 0.8767\n",
            "Epoch 6/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4593 - accuracy: 0.8657 - val_loss: 0.4378 - val_accuracy: 0.8789\n",
            "Epoch 7/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4537 - accuracy: 0.8702 - val_loss: 0.4323 - val_accuracy: 0.8789\n",
            "Epoch 8/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4454 - accuracy: 0.8713 - val_loss: 0.4293 - val_accuracy: 0.8789\n",
            "Epoch 9/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4373 - accuracy: 0.8719 - val_loss: 0.4310 - val_accuracy: 0.8789\n",
            "Epoch 10/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4335 - accuracy: 0.8713 - val_loss: 0.4292 - val_accuracy: 0.8857\n",
            "Epoch 11/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4330 - accuracy: 0.8747 - val_loss: 0.4303 - val_accuracy: 0.8857\n",
            "Epoch 12/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4237 - accuracy: 0.8781 - val_loss: 0.4312 - val_accuracy: 0.8789\n",
            "Epoch 13/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4191 - accuracy: 0.8792 - val_loss: 0.4348 - val_accuracy: 0.8767\n",
            "Epoch 14/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4164 - accuracy: 0.8803 - val_loss: 0.4296 - val_accuracy: 0.8744\n",
            "Epoch 15/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4133 - accuracy: 0.8815 - val_loss: 0.4324 - val_accuracy: 0.8744\n",
            "Epoch 16/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4115 - accuracy: 0.8815 - val_loss: 0.4309 - val_accuracy: 0.8744\n",
            "Epoch 17/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4115 - accuracy: 0.8815 - val_loss: 0.4371 - val_accuracy: 0.8744\n",
            "Epoch 18/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4097 - accuracy: 0.8815 - val_loss: 0.4374 - val_accuracy: 0.8744\n",
            "Epoch 19/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4087 - accuracy: 0.8815 - val_loss: 0.4340 - val_accuracy: 0.8744\n",
            "Epoch 20/70\n",
            "56/56 [==============================] - 0s 3ms/step - loss: 0.4079 - accuracy: 0.8815 - val_loss: 0.4379 - val_accuracy: 0.8744\n",
            "Epoch 21/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4084 - accuracy: 0.8815 - val_loss: 0.4349 - val_accuracy: 0.8744\n",
            "Epoch 22/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4047 - accuracy: 0.8815 - val_loss: 0.4319 - val_accuracy: 0.8812\n",
            "Epoch 23/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4040 - accuracy: 0.8815 - val_loss: 0.4429 - val_accuracy: 0.8744\n",
            "Epoch 24/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4051 - accuracy: 0.8809 - val_loss: 0.4414 - val_accuracy: 0.8744\n",
            "Epoch 25/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4030 - accuracy: 0.8815 - val_loss: 0.4406 - val_accuracy: 0.8744\n",
            "Epoch 26/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4056 - accuracy: 0.8798 - val_loss: 0.4375 - val_accuracy: 0.8744\n",
            "Epoch 27/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4035 - accuracy: 0.8815 - val_loss: 0.4556 - val_accuracy: 0.8700\n",
            "Epoch 28/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4043 - accuracy: 0.8820 - val_loss: 0.4482 - val_accuracy: 0.8744\n",
            "Epoch 29/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4027 - accuracy: 0.8820 - val_loss: 0.4420 - val_accuracy: 0.8744\n",
            "Epoch 30/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4011 - accuracy: 0.8815 - val_loss: 0.4493 - val_accuracy: 0.8744\n",
            "Epoch 31/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4031 - accuracy: 0.8815 - val_loss: 0.4441 - val_accuracy: 0.8744\n",
            "Epoch 32/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4012 - accuracy: 0.8820 - val_loss: 0.4505 - val_accuracy: 0.8722\n",
            "Epoch 33/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3992 - accuracy: 0.8826 - val_loss: 0.4395 - val_accuracy: 0.8722\n",
            "Epoch 34/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4024 - accuracy: 0.8820 - val_loss: 0.4384 - val_accuracy: 0.8722\n",
            "Epoch 35/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4013 - accuracy: 0.8826 - val_loss: 0.4486 - val_accuracy: 0.8722\n",
            "Epoch 36/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4028 - accuracy: 0.8826 - val_loss: 0.4443 - val_accuracy: 0.8722\n",
            "Epoch 37/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4035 - accuracy: 0.8831 - val_loss: 0.4432 - val_accuracy: 0.8722\n",
            "Epoch 38/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.4022 - accuracy: 0.8815 - val_loss: 0.4435 - val_accuracy: 0.8722\n",
            "Epoch 39/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3969 - accuracy: 0.8826 - val_loss: 0.4441 - val_accuracy: 0.8722\n",
            "Epoch 40/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3967 - accuracy: 0.8831 - val_loss: 0.4455 - val_accuracy: 0.8700\n",
            "Epoch 41/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3966 - accuracy: 0.8837 - val_loss: 0.4540 - val_accuracy: 0.8700\n",
            "Epoch 42/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3967 - accuracy: 0.8837 - val_loss: 0.4447 - val_accuracy: 0.8722\n",
            "Epoch 43/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3989 - accuracy: 0.8831 - val_loss: 0.4460 - val_accuracy: 0.8700\n",
            "Epoch 44/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3963 - accuracy: 0.8826 - val_loss: 0.4610 - val_accuracy: 0.8386\n",
            "Epoch 45/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3958 - accuracy: 0.8843 - val_loss: 0.4435 - val_accuracy: 0.8722\n",
            "Epoch 46/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3944 - accuracy: 0.8843 - val_loss: 0.4474 - val_accuracy: 0.8722\n",
            "Epoch 47/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3995 - accuracy: 0.8831 - val_loss: 0.4553 - val_accuracy: 0.8700\n",
            "Epoch 48/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3977 - accuracy: 0.8831 - val_loss: 0.4449 - val_accuracy: 0.8700\n",
            "Epoch 49/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3966 - accuracy: 0.8848 - val_loss: 0.4462 - val_accuracy: 0.8700\n",
            "Epoch 50/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3975 - accuracy: 0.8826 - val_loss: 0.4511 - val_accuracy: 0.8700\n",
            "Epoch 51/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3970 - accuracy: 0.8848 - val_loss: 0.4491 - val_accuracy: 0.8700\n",
            "Epoch 52/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3947 - accuracy: 0.8837 - val_loss: 0.4474 - val_accuracy: 0.8700\n",
            "Epoch 53/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3933 - accuracy: 0.8848 - val_loss: 0.4462 - val_accuracy: 0.8700\n",
            "Epoch 54/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3956 - accuracy: 0.8848 - val_loss: 0.4485 - val_accuracy: 0.8700\n",
            "Epoch 55/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3916 - accuracy: 0.8848 - val_loss: 0.4460 - val_accuracy: 0.8700\n",
            "Epoch 56/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3930 - accuracy: 0.8848 - val_loss: 0.4428 - val_accuracy: 0.8700\n",
            "Epoch 57/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3933 - accuracy: 0.8848 - val_loss: 0.4559 - val_accuracy: 0.8700\n",
            "Epoch 58/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3936 - accuracy: 0.8848 - val_loss: 0.4517 - val_accuracy: 0.8700\n",
            "Epoch 59/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3936 - accuracy: 0.8848 - val_loss: 0.4641 - val_accuracy: 0.8408\n",
            "Epoch 60/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3947 - accuracy: 0.8848 - val_loss: 0.4456 - val_accuracy: 0.8677\n",
            "Epoch 61/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3910 - accuracy: 0.8843 - val_loss: 0.4482 - val_accuracy: 0.8677\n",
            "Epoch 62/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3951 - accuracy: 0.8843 - val_loss: 0.4545 - val_accuracy: 0.8700\n",
            "Epoch 63/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3918 - accuracy: 0.8854 - val_loss: 0.4494 - val_accuracy: 0.8677\n",
            "Epoch 64/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3927 - accuracy: 0.8848 - val_loss: 0.4492 - val_accuracy: 0.8700\n",
            "Epoch 65/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3918 - accuracy: 0.8854 - val_loss: 0.4487 - val_accuracy: 0.8677\n",
            "Epoch 66/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3944 - accuracy: 0.8854 - val_loss: 0.4475 - val_accuracy: 0.8677\n",
            "Epoch 67/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3893 - accuracy: 0.8860 - val_loss: 0.4468 - val_accuracy: 0.8677\n",
            "Epoch 68/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3894 - accuracy: 0.8865 - val_loss: 0.4610 - val_accuracy: 0.8677\n",
            "Epoch 69/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3928 - accuracy: 0.8848 - val_loss: 0.4505 - val_accuracy: 0.8677\n",
            "Epoch 70/70\n",
            "56/56 [==============================] - 0s 2ms/step - loss: 0.3886 - accuracy: 0.8865 - val_loss: 0.4531 - val_accuracy: 0.8677\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZF1RHlg1v3X",
        "colab_type": "text"
      },
      "source": [
        "### Scores"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dh3V3QdWu0VG",
        "colab_type": "code",
        "outputId": "6179dc4b-1946-49b2-b64d-8d8b05594d8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# == Provide average scores ==\n",
        "print('Score per fold:')\n",
        "for i in range(0, len(acc_per_fold)):\n",
        "  print(f'> Fold {i+1} - Loss: {loss_per_fold[i]} - Accuracy: {acc_per_fold[i]}%')\n",
        "print('------------------------------------------------------------------------')\n",
        "print('Average scores for all folds:')\n",
        "print(f'> Accuracy: {np.mean(acc_per_fold)} (+- {np.std(acc_per_fold)})')\n",
        "print(f'> Loss: {np.mean(loss_per_fold)}')\n",
        "print('------------------------------------------------------------------------')"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score per fold:\n",
            "> Fold 1 - Loss: 0.3937833309173584 - Accuracy: 89.75741267204285%\n",
            "> Fold 2 - Loss: 0.4209561049938202 - Accuracy: 88.94878625869751%\n",
            "> Fold 3 - Loss: 0.4010755717754364 - Accuracy: 88.94878625869751%\n",
            "> Fold 4 - Loss: 0.4849584102630615 - Accuracy: 85.17520427703857%\n",
            "> Fold 5 - Loss: 0.46992728114128113 - Accuracy: 86.79245114326477%\n",
            "> Fold 6 - Loss: 0.4490264356136322 - Accuracy: 87.87062168121338%\n",
            "> Fold 7 - Loss: 0.44478389620780945 - Accuracy: 88.3783757686615%\n",
            "------------------------------------------------------------------------\n",
            "Average scores for all folds:\n",
            "> Accuracy: 87.98166257994515 (+- 1.438455932686618)\n",
            "> Loss: 0.43778729013034273\n",
            "------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQDKmJQ_kmdh",
        "colab_type": "text"
      },
      "source": [
        "## Plot with train and test accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mi0f_JObgC4C",
        "colab_type": "code",
        "outputId": "a4b63713-1ad0-401a-ad6c-8fd772253ed9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylim(0,1)\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "ffa6a305-5b22-4ce7-baee-d420c2541048",
        "id": "YQosphMa9Zab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('training loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAgAElEQVR4nO3deZxcZZ3v8c+v1l7TnV6ydjYgQMIWIISwjCKIhkUQ4SIgXHXAjDM649xRRhgVleuMOIujcwdRVBQVYTCAIgQDyOZCSEKAkEBCVpJO0ulOJ73vVc/946nuru50J52lUt19vu/Xq15VdepU1a+qq8/3PM9zFnPOISIiwRXKdgEiIpJdCgIRkYBTEIiIBJyCQEQk4BQEIiIBpyAQEQk4BYEEmpl938y+cqTnPcgappuZM7PIkX5tkaEw7UcgI5WZbQFucc49m+1aDoeZTQc2A1HnXFd2q5EgUotARi2tYYsMjYJARiQz+zkwFfitmTWZ2T+mdbHcbGZbgedS8/7KzKrMrN7MXjKzk9Je56dm9o3U7QvMrNLMPm9m1Wa208w+eYjzlprZb82swcyWm9k3zOyPQ/xsk8zscTPbY2YbzOxTaY/NM7MVqdfdZWbfTk3PMbNfmFmtmdWl3nP8YX3JEhgKAhmRnHM3AVuBDznnCpxz/5r28HuBWcAHU/efAmYC44CVwAP7eekJQBEwGbgZuNvMxh7CvHcDzal5Pp66DNVDQCUwCbgG+BczuzD12HeB7zrnxgDHAg+npn88VcsUoBT4NNB6EO8pAaYgkNHoa865ZudcK4Bz7j7nXKNzrh34GnCamRUN8txO4E7nXKdzbjHQBJxwMPOaWRi4Gviqc67FOfcWcP9QCjezKcB5wBedc23OudeBHwH/O+09jzOzMudck3Nuadr0UuA451zCOfeqc65hKO8poiCQ0Whb9w0zC5vZXWa20cwagC2ph8oGeW5tvwHbFqDgIOctByLpdfS7vT+TgD3Ouca0ae/iWx3gWx7HA2tT3T+Xp6b/HFgCPGRmO8zsX80sOsT3lIBTEMhINtgmb+nTbwCuBN6P7zqZnppumSuLGqALqEibNmWIz90BlJhZYdq0qcB2AOfceufc9fhurm8Bi8wsP9Uq+bpzbjZwLnA5va0Ikf1SEMhItgs45gDzFALtQC2QB/xLpotyziWAR4GvmVmemZ3IEBfKzrltwJ+Bb6YGgE/FtwJ+AWBmN5pZuXMuCdSlnpY0s/eZ2SmpbqkGfFdR8sh+MhmtFAQykn0T+HJqK5kvDDLPz/BdK9uBt4Clg8x3pH0W3wKpwnfbPIgPpKG4Ht9y2QE8hh9r6N5XYgGwxsya8APH16XGQiYAi/Ah8DbwYup9RQ5IO5SJHAVm9i1ggnPuYLYeEjkq1CIQyQAzO9HMTjVvHr5757Fs1yUyEO15KZIZhfjuoEn4sYz/AH6T1YpEBqGuIRGRgFPXkIhIwI24rqGysjI3ffr0bJchIjKivPrqq7udc+UDPTbigmD69OmsWLEi22WIiIwoZvbuYI+pa0hEJOAUBCIiAacgEBEJuBE3RjCQzs5OKisraWtry3YpGZWTk0NFRQXRqA4qKSJHzqgIgsrKSgoLC5k+fTpmmTyoZPY456itraWyspIZM2ZkuxwRGUVGRddQW1sbpaWlozYEAMyM0tLSUd/qEZGjb1QEATCqQ6BbED6jiBx9oyYIDqS5vYuq+jZ0SA0Rkb4CEwQtHV1UN7aRzEAO1NXV8b3vfe+gn3fppZdSV1d34BlFRDIoMEHQ3a2SiRbBYEHQ1dU1wNy9Fi9eTHFx8RGvR0TkYIyKrYaGort7PRMtgttuu42NGzcyZ84cotEoOTk5jB07lrVr1/LOO+/w4Q9/mG3bttHW1sbnPvc5Fi5cCPQeLqOpqYlLLrmE888/nz//+c9MnjyZ3/zmN+Tm5h75YkVE+hl1QfD1367hrR0N+0zvSjraOxPkxcIHPeg6e9IYvvqhkwZ9/K677mL16tW8/vrrvPDCC1x22WWsXr26ZzPP++67j5KSElpbWznrrLO4+uqrKS0t7fMa69ev58EHH+SHP/wh1157LY888gg33njjQdUpInIoRl0QHIgDMr3tzbx58/ps6/9f//VfPPaYPznVtm3bWL9+/T5BMGPGDObMmQPAmWeeyZYtWzJcpYiIN+qCYLA194bWTrbUNnPcuALyYpn92Pn5+T23X3jhBZ599llefvll8vLyuOCCCwbcFyAej/fcDofDtLa2ZrRGEZFuARos9teZ2Hq0sLCQxsbGAR+rr69n7Nix5OXlsXbtWpYuXXrkCxAROQyjrkUwmExuNVRaWsp5553HySefTG5uLuPHj+95bMGCBXz/+99n1qxZnHDCCcyfP/+Iv7+IyOEYcecsnjt3rut/Ypq3336bWbNm7fd5Le1dbKhpYnpZPmNyRu5B24byWUVE+jOzV51zcwd6TF1DIiIBF6AgyFzXkIjISBaYIAhlcIcyEZGRLDBBoBaBiMjAghMEqWu1CERE+gpMEIS6WwQoCURE0mU0CMxsgZmtM7MNZnbbAI9PM7Pfm9kqM3vBzCoyV4u/zkTP0KEehhrgO9/5Di0tLUe4IhGRoctYEJhZGLgbuASYDVxvZrP7zfbvwM+cc6cCdwLfzGA9mNlRPQz1UCgIRCTbMrln8Txgg3NuE4CZPQRcCbyVNs9s4B9St58Hfp3BejAyfxjqiy++mHHjxvHwww/T3t7OVVddxde//nWam5u59tprqaysJJFI8JWvfIVdu3axY8cO3ve+91FWVsbzzz9/5IsTETmATAbBZGBb2v1K4Ox+87wBfAT4LnAVUGhmpc652vSZzGwhsBBg6tSp+3/Xp26DqjcHfGhGRxeRkEEkPPRPATDhFLjkrkEfTj8M9dNPP82iRYtYtmwZzjmuuOIKXnrpJWpqapg0aRJPPvkk4I9BVFRUxLe//W2ef/55ysrKDq4mEZEjJNuDxV8A3mtmrwHvBbYDif4zOefudc7Ndc7NLS8vP9o1HpSnn36ap59+mtNPP50zzjiDtWvXsn79ek455RSeeeYZvvjFL/KHP/yBoqKibJcqIgJktkWwHZiSdr8iNa2Hc24HvkWAmRUAVzvnDu8kvvtZc99W1UBeLMLUkrzDeov9cc5x++2381d/9Vf7PLZy5UoWL17Ml7/8ZS666CLuuOOOjNUhIjJUmWwRLAdmmtkMM4sB1wGPp89gZmVm1l3D7cB9GayHUIYGi9MPQ/3BD36Q++67j6amJgC2b99OdXU1O3bsIC8vjxtvvJFbb72VlStX7vNcEZFsyFiLwDnXZWafBZYAYeA+59waM7sTWOGcexy4APimmTngJeAzmaoH/GBxJjYfTT8M9SWXXMINN9zAOeecA0BBQQG/+MUv2LBhA7feeiuhUIhoNMo999wDwMKFC1mwYAGTJk3SYLGIZEVgDkMNsKG6iZDBMeUFmSov43QYahE5FDoMdUrIdBhqEZH+AhUEZkZSh5gQEelj1ATBULq4RnqLYKR144nIyDAqgiAnJ4fa2toDLigNG7FB4JyjtraWnJycbJciIqPMqDh5fUVFBZWVldTU1Ox3vr3NHbR3JUnuHZkL05ycHCoqMnZcPhEJqFERBNFolBkzZhxwvi899ia/W13Nq1+5+ChUJSIyMoyKrqGhikVCdHQls12GiMiwEqggiEfCtCsIRET6CFgQhOhIJLX1jYhImkAFQSziP65aBSIivQIVBHEFgYjIPoIVBFF/QhoNGIuI9ApWEIS7WwT7nPtGRCSwghUEUf9x1SIQEekVqCCIhTVGICLSX6CCoLtFoCAQEekVrCCIaLBYRKS/QAVB734EGiwWEekWqCDo2Y+gUy0CEZFuAQuCVNdQQkEgItItUEGgriERkX0FKgi6u4Y0WCwi0itQQaCDzomI7CtQQaDBYhGRfQUsCDRYLCLSX6CCIBo2ANo7NVgsItIto0FgZgvMbJ2ZbTCz2wZ4fKqZPW9mr5nZKjO7NMP1EI+ENEYgIpImY0FgZmHgbuASYDZwvZnN7jfbl4GHnXOnA9cB38tUPd1iCgIRkT4y2SKYB2xwzm1yznUADwFX9pvHAWNSt4uAHRmsB9AJ7EVE+stkEEwGtqXdr0xNS/c14EYzqwQWA3870AuZ2UIzW2FmK2pqag6rqHgkpP0IRETSZHuw+Hrgp865CuBS4Odmtk9Nzrl7nXNznXNzy8vLD+sN/RiBBotFRLplMgi2A1PS7lekpqW7GXgYwDn3MpADlGWwJo0RiIj0k8kgWA7MNLMZZhbDDwY/3m+ercBFAGY2Cx8Eh9f3cwDxaFhdQyIiaTIWBM65LuCzwBLgbfzWQWvM7E4zuyI12+eBT5nZG8CDwCeccy5TNYE/gb26hkREekUy+eLOucX4QeD0aXek3X4LOC+TNfQXj4Zobu86mm8pIjKsZXuw+KiLhTVGICKSLnBBEI8qCERE0gUvCCIaLBYRSRe4IIhpsFhEpI/ABYG6hkRE+gpeEOgQEyIifQQuCLRnsYhIX4ELgngkTCLp6NJZykREgAAGQfcJ7HW6ShERL3BBoBPYi4j0FcAg0AnsRUTSBS4IYmoRiIj0Ebgg6Oka0k5lIiJAoINALQIREQhgEMQUBCIifQQuCHoGixUEIiJAAIMgpjECEZE+AhcEGiMQEekrcEGQE03tWawgEBEBAhgEsbAfI1CLQETEC1wQxKMaIxARSRe4IIiF1TUkIpIucEHQ2yJQEIiIQACDQC0CEZG+AhcEkXCIcMg0RiAikhK4IAC/L4GOPioi4mU0CMxsgZmtM7MNZnbbAI//p5m9nrq8Y2Z1maynWzwS0vkIRERSIpl6YTMLA3cDFwOVwHIze9w591b3PM65/5M2/98Cp2eqnnQxtQhERHpkskUwD9jgnNvknOsAHgKu3M/81wMPZrCeHvFIWC0CEZGUTAbBZGBb2v3K1LR9mNk0YAbw3CCPLzSzFWa2oqam5rALi0VCGiwWEUkZLoPF1wGLnHMDLp2dc/c65+Y65+aWl5cf9ptpsFhEpFcmg2A7MCXtfkVq2kCu4yh1C4EGi0VE0mUyCJYDM81shpnF8Av7x/vPZGYnAmOBlzNYSx8aLBYR6ZWxIHDOdQGfBZYAbwMPO+fWmNmdZnZF2qzXAQ8551ymaukvHglrjEBEJGVIm4+a2eeAnwCNwI/wm3ne5px7en/Pc84tBhb3m3ZHv/tfO4h6j4h4JKRjDYmIpAy1RfCXzrkG4AP4bpybgLsyVlWGxSIhHWtIRCRlqEFgqetLgZ8759akTRtxfNeQgkBEBIYeBK+a2dP4IFhiZoXAiF2SxtQ1JCLSY6iHmLgZmANscs61mFkJ8MnMlZVZce1QJiLSY6gtgnOAdc65OjO7EfgyUJ+5sjIrHtUYgYhIt6EGwT1Ai5mdBnwe2Aj8LGNVZVg87LuGjuIWqyIiw9ZQg6ArtZ3/lcB/O+fuBgozV1ZmxaNhAO1dLCLC0IOg0cxux282+qSZhYBo5srKrHhEp6sUEek21CD4KNCO35+gCn/coH/LWFUZFovoBPYiIt2GFASphf8DQJGZXQ60OedG7hiBWgQiIj2GFARmdi2wDPhfwLXAK2Z2TSYLyyS1CEREeg11P4IvAWc556oBzKwceBZYlKnCMike8YPF2pdARGToYwSh7hBIqT2I5w476hoSEek11BbB78xsCb0nj/ko/Y4qOpKoa0hEpNeQgsA5d6uZXQ2cl5p0r3PuscyVlVndXUNqEYiIDL1FgHPuEeCRDNZy1PS2CDRGICKy3yAws0ZgoOMwGOCcc2MyUlWGdY8R6HSVIiIHCALn3Ig9jMT+9AwW6xATIiIjd8ufwxFTi0BEpEcgg0D7EYiI9ApmEES1+aiISLdABkEsrCAQEekWyCDQnsUiIr0CGQRmRiysE9iLiEBAgwB0AnsRkW7BDQKdwF5EBMhwEJjZAjNbZ2YbzOy2Qea51szeMrM1ZvbLTNaTTl1DIiLekI81dLDMLAzcDVwMVALLzexx59xbafPMBG4HznPO7TWzcZmqhz2boOpNmHUFmBGPhhUEIiJktkUwD9jgnNvknOsAHgKu7DfPp4C7nXN7Afqd8+DIevu38PD/ho4mwI8RdGiMQEQko0EwGdiWdr8yNS3d8cDxZvYnM1tqZgsyVk1+ub9urgH8YSbUIhARyf5gcQSYCVwAXA/80MyK+89kZgvNbIWZraipqTm0d+oJgt1Ad4tAQSAikskg2A5MSbtfkZqWrhJ43DnX6ZzbDLyDD4Y+nHP3OufmOufmlpeXH1o1+WX+Wi0CEZE+MhkEy4GZZjbDzGLAdcDj/eb5Nb41gJmV4buKNmWkmn5dQ/FIWPsRiIiQwSBwznUBnwWWAG8DDzvn1pjZnWZ2RWq2JUCtmb0FPA/c6pyrzUhBeX1bBOoaEhHxMrb5KIBzbjH9TnLvnLsj7bYD/iF1yaxoDsTH9IwRqGtIRMTL9mDx0ZVf1qdFoBPTiIgELgjK+4wR6FSVIiKBDIK0rqFODRaLiAQsCPp2DalFICISuCAoh5ZaSCaIRUJ0JhyJpMt2VSIiWRW8IHBJaN3bcwJ7bUIqIkEXsCDo3ZdAp6sUEfECFgS9exfHIt0nsNeAsYgEW8CCIHW6g7QWgXYqE5GgC1gQ9B6BNKYgEBEBghYEuWPBQqkWgR8sVteQiARdsIIgFPIHn2uuIR7VYLGICAQtCKBn7+J4WF1DIiIQyCDo2yJQEIhI0AUwCMr7jBGoa0hEgi6gQbBb+xGIiKQEMAjKoL2BOB2AWgQiIgEMAr8vQU7HHkBjBCIiAQ6CvQA6J4GIBF5ggyC3Yw/hkLGzoS3LBYmIZFcAg8AfgTTWVsu5x5byu9VVOKdzEohIcAUwCHqPQHrZKRN5t7aF1dsbsluTiEgWBS8IYvkQyYXmGhacPIFIyHjizR3ZrkpEJGuCFwRmPfsSFOfFOH9mGU+u2qnuIREJrOAFAfQ5if3lp06icm8rr2+ry3JRIiLZEdAgKO8JgotnjycWDvHEqp1ZLkpEJDsCHAS7ASjKjfKe4333UDKp7iERCZ6MBoGZLTCzdWa2wcxuG+DxT5hZjZm9nrrcksl6ehSkWgSpcYHLT51EVUMbK7fuPSpvLyIynGQsCMwsDNwNXALMBq43s9kDzPo/zrk5qcuPMlVPH/nlkOyEtnoA3j97PPGIuodEJJgy2SKYB2xwzm1yznUADwFXZvD9hi7t3MUABfEI7zthHE++uZOEuodEJGAyGQSTgW1p9ytT0/q72sxWmdkiM5sy0AuZ2UIzW2FmK2pqag6/stTexd0DxgCXnTqRmsZ2lm3ec/ivLyIygmR7sPi3wHTn3KnAM8D9A83knLvXOTfXOTe3vLz88N81be/ibhfNGkduNMyT2rlMRAImk0GwHUhfw69ITevhnKt1zrWn7v4IODOD9fQaIAjyYhEunj2eX62o5OWNtUelDBGR4SCTQbAcmGlmM8wsBlwHPJ4+g5lNTLt7BfB2BuvplVfqr1NjBN2++qHZTC3J4+b7l/Pqu+oiEpFgyFgQOOe6gM8CS/AL+Iedc2vM7E4zuyI129+Z2RozewP4O+ATmaqnj3AUcsf2aREAlBbEeeCWsxlXGOcT9y1nVaX2NhaR0c9G2jF25s6d61asWHH4L/TfZ8G42XDtvsMSO+paufYHL9PY1sWDn5rP7EljDv/9RESyyMxedc7NHeixbA8WZ0/a3sX9TSrO5cFPzScvFuamH7/C2iodplpERq8AB0HZPl1D6aaU5PHALWcTCRvX3btUB6UTkVErwEFQvt8gADimvIBFnz6XMTlRPvbDpdqaSERGpWAHQeseSHTtd7YpJXn86tPnMKk4l0/8ZBnPrd11lAoUETk6AhwEqb2LWw68lj9+TA7/81fncPz4Qhb+7FUeXrFNJ7IRkVEjwEGw705l+1OSH+OBT53NWdNL+MdFq7jl/hVU1bdlsEAROWIadkLV6mxXMWwpCIYYBABjcqL84pazuePy2fxp424u/vaLPLRsq1oHMnw1Vg26dVxg1LwD914AP7wQatZlu5qB1W6E1uxtkKIgOMh/knDI+MvzZ7Dk79/DSZPHcNujb/KxH73Css17FAgyvOzdAvec6xeAbQHdBLpmHdx/ObgkxPLgsU8fcFzwqHv3z/C9c+D7fwG71mSlhOAGQcE4f/3CN+Hlu6H54LYImlaazy9vmc83Pnwyb+9s4NofvMyH7/4TT6zaQVcimYGCpUf121D1ZrarGN7aGuCX10GiE+orYfEXDuK59bB7Q+ZqO9IadsCyH/rgS1e9Fn56uT8B1SeegMv+A3ashD99JytlDqhmHTx4PRRVQKIDfvwBeGfJUS8juHsWA6x6GF75AWxfAeEYnHgZnPUpmH7eQb1Ma0eCRSsrue+Pm9m8u5nJxbl84YPH8+E5kzGzI1OreA07/NpTWx2cfhO8/2u9A/+jWfVaeParcOxFMO9TsL/fVTLhFy4bnoWbHoWtr8AL/wJX3QunfXTw5zkHqx+B393uN6JYcNeB3yvbGnbCTy6BvZv9/UlnwMkfgfEnwyO3QCgMH38Cyo/3jz/8cVj7JCx8ASacPPjrVq+FF++CzS/55cLcm2HSnCNf+48v9gFw8zMQisCD18Gu1fCBb8D8v/HfffNu2L7SL6dOuPSQ69jfnsXBDoJuu9bAyp/Dqoegda//si/+v1B23EG9TCLpePbtXXzv+Q28UVnPhSeO45+vOpmJRblHtt6gcg5+cbVvSs+5AVbeD7F8uPArMPcv/T/9YBJd0NEEucUDP97VDm88COE4zLk+M/UfimTCt1if+wbg/ELjhMvgyv+GvJKBn/P0l+HP/8+vAZ91i//sP73M/84//QcombHvc/ZugSc/78Nj0um+63T90/57veRf/fG5BlO3FV57ADa9AMVToPwEKJ8F5SfC2Gn7f+7haN4NP7kUGrbDVT+APRth9aOw83X/eMEE3xIom5n2nFr43tlQOAFueQ4isb6vuXs9vPgteHOR/20dcwFsfA46W2DymT4QJpziF9qhiP/NhaMQzYNoLkRyITSEjpb2Rh9gtZvgk4t7F+4dzfDoQlj7BEw913+2unf9YxaCS/8dzrr5kL4uBcFQdbbC0nvgD9+GrlaYtxDe+4/+AHUHIZF0/PTPW/i3JWuJhkJ86bJZfPSsKWodHK5lP/RdHJf+u19TrV4LT93q19rGnwLv+yc44ZJ912DXPwu/+6IfkDv+g37heOxF/h820QmvPwAv/TvUp86jNP8zfo3sQP/QyaRfaKx9wi8wTr7aL0SP1N+5diP8+m9g21I48XK47Nt+jf2ZO6BgPFzzY5g6v+9zXvsF/OYz/rd76b/1Tq/bCvec79eMP/k7CEd63+PNRfDH//QLtQu/4r9bDJ6700+f/hdw7c/6Bk9Hiw+KlT/zC0qAyWdAU3Xv9whgYSiaDMXTfCgUT/O1F4yHwvH+SMB73/VrwVWroWoVJLtg4mkwcY7/Piec4vv307Xuhfs/5BfcNz4C08/v+72tf9r/rUuO2fd7XfskPHQDvPeLcPanfXfR9tegcjlseAYiOf77O/fvIL/UD+K+8RCsuA92D2GwOZLrW6kF433gFE7wnzNWAPECiBX639zml+CGh2Hm+/s+P5n0XdZrHoPxs30ATT7Tfx/xggO//yAUBAerqdqvgb32c8gpgjM/AWd+0v+QD8K7tc188ZFVLN20h/OPK+NfrjqFqaV5B36i7Gv3Bvj++TDtXP+P372wdQ7e+jU881W/5jThVP8PfuJlsGcTLPkSvPMUlB4Hxy/w3YHN1TB2Osy+Etb82j9v8ly44Ha/RvzKPTDrCvjIvX4tbyBtDX7N7Z2n/HtWv+3Pgz12Opx0FYw7yY9DFYyD/HF+7TLZ6YMn0enX7DtboL3Jt1Q6mvwWbPXb/VpgfaVfMIWicOm/wqkf7f3M21fCor/0C/eTrvJrkY07/RZCTbt8KH1sUe/Cvtubi+CRm/1abU4RrFsMNWv9Yydc5t+nqKLvc954CB7/W78wKzved2c0bPddcwBjKuD0G+H0j0HxVD+tvRF2v+ODeu9m39rY+67/npv2s0NmfnlqbTvq1+q757WwD4QZf+FDafzJ8ND1fpzo+gfhuPcP/pqDeezTvgWYrnQmnLAAzv0cFAxwAiznoHKFryvZlbokINEOnW1+5bGz1f8tm2qgqQoad/m/TdsAWwRd8d9wxk0HX/shUhAcqqrVPpnXLfY/gpkX+7XJ496//26INMmk45fLtnLXU2tJJB2f/8DxfPK8GYRDAWwdLL0HXv6eX8sqmuIvxVMgr8y3unKL/XXB+L5rPokuuO8Dfk3vb5bCmIn7vnai0y/kX/o3v/ApO94vgMIx36o7+699N0BXh1+DX/5jePePfi3rfV/yf9vuBe3L34Ml/wQVZ8H1D/l609Vu9H3wtRt6+9Hb6uDtJ2DNo7DpRXCJQ/uOLOS7NIom+y6WC/7J3+6vrQEW3+rXxtPXPMdO82uzOUUDv/5jfw1v/NIvXKef5wPghEv2v5KzbVlqsNlgzGT//Y+Z5Nfaj3nfkP8XAN8F17TLr2w1VkHLbh8mE07xLYR0DTthx2u+b3zLH2H7q37hC77+j/7cB/6haK3zK3tFk/24wqQ5g39nR0IyCZ3NPiTbmyASP+gVy8OlIDhc9ZXw6v2+T7ppF8TH+DWUyWdCxVyomDfwGkSaHXWtfOXXq/n92mpOqyjirqtPZdbEEXx468426GobvM89XTLh18xfuQemnuP7U+srfRdCZ8u+81vIL2Smneeb/JXL4Q//Adfc57tf9ifRBasXwbJ7fR/1RXf4BeRAWvb44BmoK+et3/g1/vxyX3PBOH87EvcrBxb2hzCf8Z59n9ve6Ae1m6p966Op2n/OUNQHUzjib8fyIV7or2P5PhALJ+67Jn8kdbbB5hdhyryD7vLMuvYm2LoUtr7sQ+zYC7Nd0YiiIDhSEp2w7ik/KLZ9hR98S3b5BdeJl/tR/qnzB+0jds7xxKqdfO3xNdS1dnLecWVcPHs8F88az4SinIOvxzm/kD3UBYdzfo3yT9/xg1aJdr/G1tXu+0mnnw/HXej700n2vuEAABHNSURBVEtm+IG29Ut8H2v3AFr5LJh2jh/YmnbOvl0Lna1+6421T+zb9+6c7+ttqfXXrXX+unY9bPmT/44THX7ek6/xfeJH09ZX4NmvQUOlb+p3tfrp406C63/pu4FERggFQaZ0tsLOVb7r6NWf+u6BSaf7QDjx8n0HuFL2Nnfw/Zc2smR1FVtq/RrxqRVFnDy5iNxomJxoiNxomLKCOB85o4JYJG3Qsq3BB9GGZ/wgaFsdnHLNwJu3OefXutsb/QK6u+nrnB9Me/Fbvrk9ZjLMeC9Ec/xWM5G4PyDfxhegfqt/zpjJvq/TJaFwku9OKJzo1862LYOORj9f0RQfhlPO9mv1S/7J96su+CbM/+uD/H7bfBhUrfZbCeVkuQXV3pTqypicuS1hRDJEQXA0dDT7gbWl9/g12lDE9z9Pne+7FooqUoNmm/0gZv12XOEEanOmsryxhKd2FrKlLkFB1x7GJPZSQj2l1DOzsJOLpkXITzT4wcRdq30rJF4Ex17gt0BY82hq87a5fuCurd53p1Qu7zs4Fy/yffLJLj9IWDwVzv8Hv5CNxPf9TM75/vCNv/ebbJbNTG3H3G/LmESXr2vry6mm+1I/UAa+ZfGRH8LsK/Z9fRE5ahQER1MyCVte8gOGW5f6Ne5Ee9958sv9YFvjrt4F5iCayKWOAgrHjqeoZDxMPBVmfsAPZHavlXZv3rb8Rz6EwG82V3GWv+SV9vbJ122D9ga/8D/1o5lZs3XOh17lChh/kt8ETkSySkGQTV3tfsuHpl2+T3nsjL5dHG0NfuuT2g2+Pzx/nB94zh8H+WVsa0jw2V+u5I3Kej5+zjRuv3QWOdFBttJwzq+ZF04Mxt62IjJkCoIRrqMrybd+t5Yf/3EzxXlRLj91IledXsEZU4u1k5qIDImCYJR4eWMtDy7bytNvVdHWmWR6aR6XnTqRM6eNZc6UsZTkxw78IiISSPsLggxusCxH2jnHlnLOsaU0tnXyu9VVPPbadu55YSPJVJZPLcljzpRizppRwvwZJRw3rkAtBhE5ILUIRrjm9i7e3F7PG9vqeH1bHa9traOqwZ85rawgxrwZJcyZUsz00nyml+UztSRv8DEGERm11CIYxfLjEeYfU8r8Y/xhEJxzbNvTytJNtT2XxW/2bplkBuUFcaLhvgdUK8mPMb0sn+mleUwrzWfK2FzKCuOU5ccZkxtRy0JkFFMQjDJmxtTSPKaW5nHtWVMAqGvpYEttC+/WNrN5dzM76lp7upMAks6xu6mDVZV1LH5zJ4lk31ZiNGyU5Mcoyo1SmBOlMCfCmJwokbDR3pmkrTNBa2eCpHPMnVbChbPGcVpFcTCPpyQyAmW0a8jMFgDfBcLAj5xzdw0y39XAIuAs59x++33UNZRZnYkk2/e2sm1vC3uaO9jd1EFtUzu7m9ppaO2isb3TX7d10plw5ERD5ETD5EbDdCaSvLm9nqSD0vwY7z2hnJMmFVEYj1CQE6EwJ0IkFGJvSwe7m9rZ3dTBnuZ2ygriHDeugJnjCplelkfIjHVVjayq9F1e71Q3MrEoh+PHF3LihEKOH19IYU6U+tYO6ls7qWvppK0zycTiHKaW5FGaHxs2LRjnHO1dScwgHlGXnGRPVrqGzCwM3A1cDFQCy83scefcW/3mKwQ+B7ySqVpk6KLhkO8iKss/pOfXtXTw4js1PL+2mufWVvPoyu2DzmsGY3KiNLR10r0+Eg4Z4ZDR0eVP91mcF+WE8YW8taOBp1ZXMZT1lrxYmClj85hSkkvF2DwqxuZSMTaXeDTMxuomNtY0saG6ia17WigriDOjLJ9jygs4piyfY8rzOba8gPx433+NnfWtvLyxllc27aEjkaSsIEZpQZzS/BjxaJiq+lZ21LWxva6VnfWt1Ld20tTWRVN7F50JX3RpfozxY3KYWJTD5LG5XHHaJM6cNnbYhJYEV8ZaBGZ2DvA159wHU/dvB3DOfbPffN8BngFuBb6gFsHokUw6Gtt966GpvYumti7au5KU5McoLYhRkhcjEg7R1pnoWThvrG6itTPBKRXFzKkoZkpJbs+CsqWjiw3VTayraqS1M0FRbpTivBjFuVFikRA76lrZtqeFrXta2bqnhcq9LVTubaWpve/JysfmRTluXAFTS/KpbW5nU00zlXtb+nSXTSrK4bjxhZQVxHhtax2bdzcDUJQbpSAeYXdTO+1dfc9NXRCPMLk4l4nFOYzNi1GQagkVxCMkko6qhjaq6v1lS20zLR0JZk8cw8fPncYVp00mN3boLYbdTe28U9XIu3tamFaaxymTiyjMOfi9xutbOnnstUrW7WqkYmweM8ryUxsa5JEXU0/ySJaV/QjM7BpggXPultT9m4CznXOfTZvnDOBLzrmrzewFBgkCM1sILASYOnXqme+++25GapbRxzlHQ2sX2/a20NaZ4JjyggH3t2jvSrC1tqUnkDZUN7G+uoldDe2cVlHUs+nurAljCIUM5xzNHQlqm9p7uqXGHMSCt6Wji1+/toOfvbyFtVWNFOVGOW1KMZGQETIjEjLCYX87ZBAy84d3cn5MJ5m63tvSwbqqRnY3dfR5fTM4piyf0yqKmVKSRzwaIicSJh4NkR+L9LRMJhTlEI+EWLm1jl++spUn39xBW2eS4rwodS2dfV6zMB7xGxAUxCgriBMKGXubO9jT3MHelg4a27qIR/wBE3NiYfJiYSYX53LSpCJmTxzD7EljmFiUM2gLqK0zwfpdTYRCMHvimKy3lLr/xgXx0RGAwzIIzCwEPAd8wjm3ZX9BkE4tAhlNnHMs27yHB17ZytY9LSSSjq6kI5l0dCaTfRb8iaQjFEqFAv66MCfC8eMLOWFCISdOGMO00jw21jSxqrKeVZV1vFFZT01j+35rKMyJ0NjWRX4szJWnT+aGeVM5eXIRTe1dbNndzJbaZt6tbaGmsZ2apnZ2N/oxI+f81mZj833rriAnQmciSUuH33igpb2Ld2tb2Fzb3NOlV5jjW00TinKYWJRLeUGMyr2trNnRwIaapp4NFSYX5/LBkyaw4OQJnDnNnzehprG9p1XV2NZJRyJJR1eS9i5/3ZVI0pl0/jrhcKnvzeGv2zoT1DZ1UNvczp6mDhrbuzhj6lgumjWOC08cR8XYPJxzrN7ewOLVO3nqzZ1sqW1h5rgCzjuujHOPLWX+saUUxPz3VZcao+pMJCnKjVKU6zeoiISMLbXNvFFZxxvb6nl9Wx2FORE+dvZU3j9rPJHwgc9pvKOulZrGdk6eXHTENrrIVhDst2vIzIqAjUBT6ikTgD3AFfsLAwWByMFJJh0dCb91V1tnkqb2LnY1tLGjrtV3VTW0cdKkIq6YMykja79N7V2sq2rwC/vqJnbUtVHV0MrOujZqmzsYPybe02I4aVIRTW1dLFlTxR/W76YjkSQ/FqatK7nP1mwDiYaNSChEZIDWVDwSprQgRml+jJL8OPFoiJc31vZ0+50wvpDmji4q97YSDhnnHlvK6VPH8trWvSzfsoe2zmSfM6Tur4bucaG8WJiTJxexfW8r2+taGT8mznVnTeWaMysYkxvtCauu1IYWf1i/mz+sr2Fjja+pvDDOgpMmcMkpE5g3vWRIITKYbAVBBHgHuAjYDiwHbnDOrRlk/hdQi0AkULoSyUEXbo1tnbywroZXNtdSnBtjfFEOE8b4S1FulHg0RCwc6rkOh+yQupM21TTx3Npqnl9XTSwc4pJTJnLxrPGMTetCbO9K8NrWOpZuqiWZdBTl+bX/4two0UiI+tZO6ls6qGvx42HHlhdw2pRijhtXQDhkJJKO59dW8/Ol7/LS+ppBgyQnGmLejFLeM7OMsoI4T79VxXNrq2nrTFKaH+OOD83myjkDnLp0CLJ2rCEzuxT4Dn7z0fucc/9sZncCK5xzj/eb9wUUBCIyym2tbeG5tbtIOHpaLCGDGWUFzJ0+dp89/1s6unhxXQ2LV1dx0/xpzJtRckjvq4POiYgE3P6C4NA7nEREZFRQEIiIBJyCQEQk4BQEIiIBpyAQEQk4BYGISMApCEREAk5BICIScCNuhzIzqwEO9fCjZcDuI1hOpo20emHk1ax6M0v1ZtbB1DvNOVc+0AMjLggOh5mtGGzPuuFopNULI69m1ZtZqjezjlS96hoSEQk4BYGISMAFLQjuzXYBB2mk1Qsjr2bVm1mqN7OOSL2BGiMQEZF9Ba1FICIi/SgIREQCLjBBYGYLzGydmW0ws9uyXU9/ZnafmVWb2eq0aSVm9oyZrU9dj81mjenMbIqZPW9mb5nZGjP7XGr6sKzZzHLMbJmZvZGq9+up6TPM7JXU7+J/zCx2oNc6mswsbGavmdkTqfvDtl4z22Jmb5rZ62a2IjVtWP4eAMys2MwWmdlaM3vbzM4Z5vWekPpuuy8NZvb3R6LmQASBmYWBu4FLgNnA9WY2O7tV7eOnwIJ+024Dfu+cmwn8PnV/uOgCPu+cmw3MBz6T+k6Ha83twIXOudOAOcACM5sPfAv4T+fcccBe4OYs1jiQzwFvp90f7vW+zzk3J23b9uH6ewD4LvA759yJwGn473nY1uucW5f6bucAZwItwGMciZqdc6P+ApwDLEm7fztwe7brGqDO6cDqtPvrgImp2xOBddmucT+1/wa4eCTUDOQBK4Gz8XtlRgb6nWT7AlSk/rEvBJ4AbJjXuwUo6zdtWP4egCJgM6kNZoZ7vQPU/wHgT0eq5kC0CIDJwLa0+5WpacPdeOfcztTtKmB8NosZjJlNB04HXmEY15zqZnkdqAaeATYCdc65rtQsw+138R3gH4Fk6n4pw7teBzxtZq+a2cLUtOH6e5gB1AA/SXW9/cjM8hm+9fZ3HfBg6vZh1xyUIBjxnI/7Ybetr5kVAI8Af++ca0h/bLjV7JxLON+srgDmASdmuaRBmdnlQLVz7tVs13IQznfOnYHvgv2Mmb0n/cFh9nuIAGcA9zjnTgea6delMszq7ZEaF7oC+FX/xw615qAEwXZgStr9itS04W6XmU0ESF1XZ7mePswsig+BB5xzj6YmD+uaAZxzdcDz+K6VYjOLpB4aTr+L84ArzGwL8BC+e+i7DN96cc5tT11X4/uu5zF8fw+VQKVz7pXU/UX4YBiu9aa7BFjpnNuVun/YNQclCJYDM1NbXMTwzarHs1zTUDwOfDx1++P4fvhhwcwM+DHwtnPu22kPDcuazazczIpTt3Px4xlv4wPhmtRsw6Ze59ztzrkK59x0/O/1Oefcxxim9ZpZvpkVdt/G92GvZpj+HpxzVcA2MzshNeki4C2Gab39XE9vtxAciZqzPehxFAdXLgXewfcLfynb9QxQ34PATqATv7ZyM75P+PfAeuBZoCTbdabVez6+CboKeD11uXS41gycCryWqnc1cEdq+jHAMmADvqkdz3atA9R+AfDEcK43Vdcbqcua7v+x4fp7SNU2B1iR+k38Ghg7nOtN1ZwP1AJFadMOu2YdYkJEJOCC0jUkIiKDUBCIiAScgkBEJOAUBCIiAacgEBEJOAWByFFkZhd0H0lUZLhQEIiIBJyCQGQAZnZj6vwFr5vZD1IHrGsys/9Mnc/g92ZWnpp3jpktNbNVZvZY9/Hgzew4M3s2dQ6ElWZ2bOrlC9KOg/9Aai9tkaxREIj0Y2azgI8C5zl/kLoE8DH8Xp0rnHMnAS8CX0095WfAF51zpwJvpk1/ALjb+XMgnIvfcxz8kVr/Hn9ujGPwxxUSyZrIgWcRCZyL8Cf+WJ5aWc/FH8grCfxPap5fAI+aWRFQ7Jx7MTX9fuBXqePuTHbOPQbgnGsDSL3eMudcZer+6/jzUPwx8x9LZGAKApF9GXC/c+72PhPNvtJvvkM9Pkt72u0E+j+ULFPXkMi+fg9cY2bjoOe8u9Pw/y/dR/68Afijc64e2Gtmf5GafhPwonOuEag0sw+nXiNuZnlH9VOIDJHWRET6cc69ZWZfxp9tK4Q/Iuxn8CcvmZd6rBo/jgD+0L/fTy3oNwGfTE2/CfiBmd2Zeo3/dRQ/hsiQ6eijIkNkZk3OuYJs1yFypKlrSEQk4NQiEBEJOLUIREQCTkEgIhJwCgIRkYBTEIiIBJyCQEQk4P4/j7K6OjH+iHkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RwVgSUc6uC6Y",
        "colab_type": "text"
      },
      "source": [
        "### Save model \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z2XZ1N0JuIop",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('featureModelPl.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}